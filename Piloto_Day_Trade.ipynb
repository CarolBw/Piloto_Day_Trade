{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MVP_Objetivo.md\n",
        "\n",
        "#@title **Objetivo do Projeto**\n",
        "\n",
        "### 1. Propósito do MVP\n",
        "\n",
        "Este projeto tem como objetivo principal a criação de um pipeline para extração, transformação, carga, análise e previsão da movimentação intradiária dos preços de um ativo financeiro em intervalos de 5 minutos. O modelo preditivo central será baseado em redes neurais recorrentes (LSTM), mas outras abordagens serão exploradas. O MVP visa garantir previsões para embasar decisões estratégicas de day trade.\n",
        "\n",
        "### 2. Problema a Ser Resolvido\n",
        "\n",
        "A alta volatilidade dos mercados financeiros exige ferramentas robustas para antecipação de movimentos de preço. A dificuldade está em capturar padrões de curto prazo e projetá-los com precisão. Traders e investidores necessitam de um modelo que consiga interpretar os padrões históricos e transformá-los em previsões úteis.\n",
        "\n",
        "### 3. Pipeline do Projeto\n",
        "\n",
        "O projeto será estruturado em sete etapas principais:\n",
        "\n",
        "1. **Extração e armazenamento dos dados brutos:** Coleta de dados históricos de ativos financeiros em intervalos de 15 minutos utilizando a API do Yahoo Finance (yfinance). Os dados serão armazenados em um repositório GitHub sincronizado com Google Colab, garantindo acesso remoto e backup na nuvem.\n",
        "\n",
        "2. **Limpeza e organização dos dados:** Padronização de colunas, tratamento de dados ausentes, eliminação de duplicatas e organização cronológica. Resultado salvo como `dados_limpos.csv`.\n",
        "\n",
        "3. **Transformação e engenharia de features:** Adição de indicadores técnicos (como médias móveis, RSI, MACD), criação de variáveis de lag e retornos. Resultado salvo como `dados_transformados.csv`.\n",
        "\n",
        "4. **Modelagem e estruturação do banco de dados:**\n",
        "\n",
        "   a) Organização em arquivos CSV:\n",
        "\n",
        "   - `dados_brutos.csv`: dados originais extraídos da API.\n",
        "   - `dados_limpos.csv`: após limpeza e padronização.\n",
        "   - `dados_transformados.csv`: após adição de features técnicas.\n",
        "   - `dados_final.csv`: versão padronizada e normalizada dos dados.\n",
        "\n",
        "   b) Banco de dados dimensional:\n",
        "\n",
        "   - **Fato**: `fato_precos`, contendo os valores de fechamento e chaves para dimensões.\n",
        "   - **Dimensões**:\n",
        "     - `dim_tempo`: atributos temporais.\n",
        "     - `dim_indicadores`: indicadores técnicos.\n",
        "     - `dim_lags`: variações e lags recentes.\n",
        "\n",
        "   Um **Catálogo de Dados** será elaborado com descrição, domínio, categorias e linhagem de cada variável.\n",
        "\n",
        "5. **Carga e Pipeline ETL:** Pipeline estruturado com as seguintes etapas:\n",
        "\n",
        "   - **Extração:** via API do Yahoo Finance.\n",
        "   - **Limpeza:** tratamento e estruturação básica.\n",
        "   - **Transformação:** geração de variáveis técnicas e derivadas.\n",
        "   - **Carga:** integração dos dados transformados no banco dimensional (fato e dimensões).\n",
        "\n",
        "6. **Treinamento e ajuste do modelo:** Implementação e ajuste de modelos preditivos (LSTM como baseline), com avaliação por métricas como MSE e R².\n",
        "\n",
        "7. **Interpretação dos resultados e resposta às perguntas:** Validação das previsões, análise de variáveis relevantes e contribuição dos resultados para decisões de trading.\n",
        "\n",
        "### 4. Perguntas a Serem Respondidas\n",
        "\n",
        "- É possível prever com precisão a movimentação intradiária de um ativo a cada 15 minutos?\n",
        "- Os dados do dia anterior fornecem informações suficientes para a previsão do dia seguinte?\n",
        "- A modelagem com LSTM captura corretamente as tendências de curto prazo?\n",
        "- A previsão da movimentação intradiária também permite derivar com precisão as targets globais do dia (abertura, mínima, máxima e fechamento)?\n",
        "- Quais indicadores técnicos e features são mais relevantes para melhorar a acurácia do modelo?\n",
        "- Como considerar corretamente as quebras de fim de semana (exemplo: prever a segunda-feira usando os dados de sexta-feira)?\n",
        "- A normalização e padronização das variáveis melhora a precisão do modelo?\n",
        "\n",
        "### 5. Critérios de Sucesso\n",
        "\n",
        "Para que o MVP seja considerado bem-sucedido o esperado é que:\n",
        "\n",
        "1. O pipeline de extração, transformação e previsão funcione de forma eficiente.\n",
        "2. O modelo consiga prever a movimentação dos preços com um erro médio aceitável (avaliado por MSE ou R2).\n",
        "3. A previsão de targets globais (abertura, máxima, mínima, fechamento) seja consistente com os valores reais.\n",
        "4. O modelo consiga lidar corretamente com fins de semana e feriados.\n",
        "5. As previsões sejam suficientes para auxiliar na tomada de decisão de trading.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vj0A_HHDW-dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Instalando dependências\n",
        "'''\n",
        "Usamos `-q` para ocultar a saída detalhada e mostrar apenas a barra de progresso\n",
        "\n",
        "'''\n",
        "!pip install -q tensorflow > /dev/null  # Framework para redes neurais e deep learning\n",
        "!pip install -q keras > /dev/null  # Biblioteca de alto nível para redes neurais\n",
        "!pip install -q pandas > /dev/null  # Manipulação e análise de dados\n",
        "!pip install -q numpy > /dev/null  # Computação numérica eficiente\n",
        "!pip install -q matplotlib > /dev/null  # Visualização de gráficos e análise exploratória\n",
        "!pip install -q scikit-learn > /dev/null  # Ferramentas para pré-processamento e métricas de avaliação\n",
        "!pip install -q gitpython > /dev/null  # Gerenciamento de repositórios Git via Python\n",
        "!pip install -q python-dotenv > /dev/null  # Manipulação de variáveis de ambiente\n",
        "!pip install -q seaborn > /dev/null  # Biblioteca de visualização estatística baseada no Matplotlib\n",
        "!pip install -q yfinance > /dev/null  # Coleta de dados financeiros diretamente do Yahoo Finance\n",
        "!pip install -q sqlalchemy > /dev/null  # ORM para interagir com bancos de dados relacionais\n",
        "!pip install -q dotenv > /dev/null # Manipulação de variáveis de ambiente\n",
        "\n",
        "# Importações das bibliotecas\n",
        "import pandas as pd  # Manipulação de DataFrames\n",
        "import numpy as np  # Cálculos numéricos e matrizes\n",
        "import matplotlib.pyplot as plt  # Geração de gráficos\n",
        "import sqlite3  # Integração com banco de dados SQLite\n",
        "\n",
        "# Pré-processamento dos dados\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # Normalização e padronização dos dados\n",
        "from sklearn.model_selection import train_test_split  # Divisão dos dados em treino e teste\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Avaliação do desempenho do modelo\n",
        "\n",
        "# Construção do modelo preditivo\n",
        "from keras.models import Sequential  # Modelo sequencial de rede neural\n",
        "from keras.layers import Dense  # Camada densa para aprendizado profundo\n",
        "\n",
        "# Controle de versão e variáveis de ambiente\n",
        "import git  # Gerenciamento do repositório Git\n",
        "import dotenv  # Carregamento de variáveis de ambiente\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")  # Oculta todos os warnings\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LWOFHzJ6IupA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Capturamdo todas as dependencias do ambiente nesta primeira etapa\n",
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "ujEHOrH9cTIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/configurar_git.py\n",
        "# Configurar Git e sincronizar com GitHub\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def git_config():\n",
        "    \"\"\"Configura o Git localmente e sincroniza com o repositório remoto no GitHub.\"\"\"\n",
        "\n",
        "    # Carregar variáveis do ambiente\n",
        "    load_dotenv(dotenv_path='/content/.env')\n",
        "\n",
        "    GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
        "    EMAIL = os.getenv('EMAIL')\n",
        "    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
        "    PROJECT_NAME = os.getenv('PROJECT_NAME')\n",
        "\n",
        "    if not all([GITHUB_USERNAME, EMAIL, GITHUB_TOKEN, PROJECT_NAME]):\n",
        "        raise ValueError(\"Variáveis de ambiente faltando. Verifique o arquivo .env.\")\n",
        "\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{PROJECT_NAME}.git\"\n",
        "\n",
        "    # Configurações globais do Git\n",
        "    os.system(f'git config --global user.name \"{GITHUB_USERNAME}\"')\n",
        "    os.system(f'git config --global user.email \"{EMAIL}\"')\n",
        "\n",
        "    if os.path.isdir(PROJECT_NAME):\n",
        "        print(f\"[INFO] Diretório '{PROJECT_NAME}' já existe. Sincronizando...\")\n",
        "\n",
        "        os.chdir(PROJECT_NAME)\n",
        "\n",
        "        os.system(\"git init\")\n",
        "        os.system(\"git remote remove origin || true\")\n",
        "        os.system(f\"git remote add origin {REPO_URL}\")\n",
        "        os.system(\"git fetch origin\")\n",
        "        os.system(\"git checkout -B main\")\n",
        "        os.system(\"git pull origin main --allow-unrelated-histories --no-rebase\")\n",
        "    else:\n",
        "        print(f\"[INFO] Clonando o repositório '{PROJECT_NAME}'...\")\n",
        "\n",
        "        os.system(f\"git clone {REPO_URL}\")\n",
        "        os.chdir(PROJECT_NAME)\n",
        "\n",
        "    print(f\"[SUCESSO] Git configurado e sincronizado com: {REPO_URL}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    git_config()\n"
      ],
      "metadata": {
        "id": "NmSdFzemI9yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sincronizando repositório\n",
        "!python /content/configurar_git.py\n"
      ],
      "metadata": {
        "id": "wLWe41oYxhcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Definindo estrutura de pastas inicial do projeto\n",
        "\n",
        "\"\"\"\n",
        "Estrutura inicial do repositório Piloto_Day_Trade:\n",
        "\n",
        "|- notebooks/         → Jupyter Notebooks para exploração e análises\n",
        "|- scripts/           → Funções reutilizáveis (pré-processamento, modelagem, avaliação, etc.)\n",
        "   |-modelagem_machine_learning\n",
        "   |-operacional\n",
        "   |-pipeline\n",
        "|- data/              → Dados organizados em 3 níveis:\n",
        "   |- raw/            → Dados brutos extraídos diretamente de fontes externas\n",
        "   |- cleaned/        → Dados limpos com tratamento básico (ex: datas, nulos, nomes de colunas)\n",
        "   |- transformed/    → Dados com features criadas e prontos para modelagem\n",
        "   |- prepared/          → Dados prontos para serem utilizados no modelo\n",
        "|- modelagem/         → Modelagem do banco de dados.\n",
        "   |- database/       → Banco de dados\n",
        "   |- catalog/        → Catálogo de dados\n",
        "   |- esquema/        → Esquema do banco de dados\n",
        "|- .github/\n",
        "   |-workflows\n",
        "|- models/            → Modelos treinados\n",
        "|- reports/           → Resultados, gráficos, relatórios de performance\n",
        "|- MVP_Objetivo.md    → Documento explicando o objetivo do projeto\n",
        "|- README.md          → Instruções gerais do projeto\n",
        "|-.env                → Variáveis de ambiente e configurações sensíveis\n",
        "|- requirements.txt   → Lista de dependências\n",
        "|- LICENSE            → Licença do projeto\n",
        "\"\"\"\n",
        "\n",
        "# Criar estrutura de diretórios\n",
        "%cd /content/\n",
        "\n",
        "!mkdir -p /content/Piloto_Day_Trade/notebooks\n",
        "!mkdir -p /content/Piloto_Day_Trade/scripts/modelagem_machine_learning\n",
        "!mkdir -p /content/Piloto_Day_Trade/scripts/operacional\n",
        "!mkdir -p /content/Piloto_Day_Trade/scripts/pipeline\n",
        "!mkdir -p /content/Piloto_Day_Trade/data/raw\n",
        "!mkdir -p /content/Piloto_Day_Trade/data/cleaned\n",
        "!mkdir -p /content/Piloto_Day_Trade/data/transformed\n",
        "!mkdir -p /content/Piloto_Day_Trade/data/prepared\n",
        "!mkdir -p /content/Piloto_Day_Trade/modelagem/catalog\n",
        "!mkdir -p /content/Piloto_Day_Trade/modelagem/esquema\n",
        "!mkdir -p /content/Piloto_Day_Trade/modelagem/database\n",
        "!mkdir -p /content/Piloto_Day_Trade/models\n",
        "!mkdir -p /content/Piloto_Day_Trade/reports\n",
        "!mkdir -p /content/Piloto_Day_Trade/.github/workflows\n",
        "\n",
        "# Criar arquivos principais\n",
        "!touch /content/Piloto_Day_Trade/.gitignore\n",
        "!touch /content/Piloto_Day_Trade/README.md\n",
        "\n",
        "# Mover arquivos existentes\n",
        "!mv /content/.env /content/Piloto_Day_Trade/.env\n",
        "!mv /content/configurar_git.py /content/Piloto_Day_Trade/scripts/operacional/configurar_git.py\n",
        "!mv /content/requirements.txt /content/Piloto_Day_Trade/requirements.txt\n",
        "!mv /content/MVP_Objetivo.md /content/Piloto_Day_Trade/MVP_Objetivo.md\n",
        "\n",
        "# Conferir estrutura de diretórios\n",
        "!apt-get install tree -y > /dev/null 2>&1 # Instala o tree e oculta a saida da instalação\n",
        "!tree /content/Piloto_Day_Trade -d\n",
        "\n"
      ],
      "metadata": {
        "id": "vVkhl3kbZKbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/operacional/atualizar_repo.py\n",
        "\n",
        "# Função operacional para atualizar o repositório remoto com segurança e validação\n",
        "from git import Repo, GitCommandError\n",
        "\n",
        "def atualizar_repo(mensagem_commit=\"Atualizações automáticas via Colab\"):\n",
        "    \"\"\"\n",
        "    Envia alterações locais para o repositório remoto GitHub.\n",
        "    - Detecta alterações\n",
        "    - Adiciona, comita e envia\n",
        "    - Valida o sucesso do push\n",
        "    \"\"\"\n",
        "    repo_path = \"/content/Piloto_Day_Trade\"\n",
        "    try:\n",
        "        repo = Repo(repo_path)\n",
        "\n",
        "        if repo.is_dirty(untracked_files=True):\n",
        "            print(\"[INFO] Alterações locais detectadas. Enviando para o GitHub...\")\n",
        "\n",
        "            repo.git.add(all=True)\n",
        "            repo.index.commit(mensagem_commit)\n",
        "            origin = repo.remote(name='origin')\n",
        "            result = origin.push(refspec='main')\n",
        "\n",
        "            for res in result:\n",
        "                if res.flags & res.ERROR:\n",
        "                    print(f\"[ERRO] Falha ao enviar: {res.summary}\")\n",
        "                else:\n",
        "                    print(f\"[SUCESSO] Push concluído: {res.summary}\")\n",
        "        else:\n",
        "            print(\"[INFO] Nenhuma alteração detectada. Nada a enviar.\")\n",
        "\n",
        "    except GitCommandError as ge:\n",
        "        print(f\"[ERRO] Comando Git falhou: {ge}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERRO] Falha inesperada ao enviar alterações: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    atualizar_repo()\n"
      ],
      "metadata": {
        "id": "oMWh1DGtsdZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile /content/scanner_git.py\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import git\n",
        "\n",
        "def imprimir(msg):\n",
        "    print(f\"[INFO] {msg}\")\n",
        "\n",
        "def erro(msg):\n",
        "    print(f\"[ERRO] {msg}\")\n",
        "\n",
        "def scanner_git():\n",
        "    imprimir(\"Carregando variáveis de ambiente...\")\n",
        "    load_dotenv('/content/.env')\n",
        "\n",
        "    GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
        "    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
        "    EMAIL = os.getenv('EMAIL')\n",
        "    PROJECT_NAME = os.getenv('PROJECT_NAME')\n",
        "\n",
        "    if not all([GITHUB_USERNAME, GITHUB_TOKEN, EMAIL, PROJECT_NAME]):\n",
        "        erro(\"Variáveis ausentes no .env. Verifique GITHUB_USERNAME, GITHUB_TOKEN, EMAIL, PROJECT_NAME.\")\n",
        "        return\n",
        "\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{PROJECT_NAME}.git\"\n",
        "\n",
        "    imprimir(f\"Usuário: {GITHUB_USERNAME}\")\n",
        "    imprimir(f\"Email: {EMAIL}\")\n",
        "    imprimir(f\"Repositório: {REPO_URL}\")\n",
        "\n",
        "    try:\n",
        "        repo_path = f\"/content/{PROJECT_NAME}\"\n",
        "        imprimir(f\"Tentando abrir repositório em {repo_path}...\")\n",
        "        repo = git.Repo(repo_path)\n",
        "    except Exception as e:\n",
        "        erro(f\"Não foi possível abrir o repositório local: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Mostrar status do repositório\n",
        "        imprimir(\"Verificando status do repositório local...\")\n",
        "        if repo.is_dirty(untracked_files=True):\n",
        "            imprimir(\"Alterações locais detectadas.\")\n",
        "        else:\n",
        "            imprimir(\"Sem alterações locais pendentes.\")\n",
        "\n",
        "        imprimir(\"Arquivos modificados:\")\n",
        "        print(repo.git.status())\n",
        "\n",
        "        # Verificar branch atual\n",
        "        branch = repo.active_branch.name\n",
        "        imprimir(f\"Branch atual: {branch}\")\n",
        "\n",
        "        # Remoto\n",
        "        origin = repo.remote(name='origin')\n",
        "        imprimir(f\"Remoto configurado: {origin.url}\")\n",
        "\n",
        "        # Verificar diferenças locais vs remoto\n",
        "        imprimir(\"Comparando local com remoto (git fetch)...\")\n",
        "        origin.fetch()\n",
        "        behind = repo.iter_commits(f'{branch}..origin/{branch}')\n",
        "        ahead = repo.iter_commits(f'origin/{branch}..{branch}')\n",
        "\n",
        "        num_behind = sum(1 for _ in behind)\n",
        "        num_ahead = sum(1 for _ in ahead)\n",
        "\n",
        "        imprimir(f\"Commits atrás do remoto: {num_behind}\")\n",
        "        imprimir(f\"Commits à frente do remoto: {num_ahead}\")\n",
        "\n",
        "        # Pull do remoto\n",
        "        imprimir(\"Tentando sincronizar com remoto (pull)...\")\n",
        "        origin.pull(branch)\n",
        "\n",
        "        # Push (se houver commits locais)\n",
        "        if num_ahead > 0 or repo.is_dirty(untracked_files=True):\n",
        "            imprimir(\"Fazendo commit e push de alterações...\")\n",
        "            repo.git.add(all=True)\n",
        "            repo.index.commit(\"Commit automático via scanner\")\n",
        "            push_result = origin.push(branch)\n",
        "\n",
        "            for res in push_result:\n",
        "                if res.flags & res.ERROR:\n",
        "                    erro(f\"Erro ao enviar: {res.summary}\")\n",
        "                else:\n",
        "                    imprimir(f\"Push realizado com sucesso: {res.summary}\")\n",
        "        else:\n",
        "            imprimir(\"Nada a enviar. Tudo sincronizado.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        erro(f\"Erro inesperado durante sincronização: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scanner_git()\n"
      ],
      "metadata": {
        "id": "UoXPaj0jzH26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Piloto_Day_Trade/scripts/operacional\n",
        "!python atualizar_repo.py"
      ],
      "metadata": {
        "id": "aXxOjTetuqAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/extracao_dadosvf.py\n",
        "\n",
        "#@title  Extração de dados\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import dotenv\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"yfinance\").setLevel(logging.CRITICAL)\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "\"\"\"\n",
        "Essa função tem como objetivo extrair dados históricos de um ativo financeiro do Yahoo Finance\n",
        "com controle de incremental, salvando tudo em um CSV que serve como base bruta do pipeline.\n",
        "Etapas:\n",
        "- Define intervalo de coleta:\n",
        "- Início: hoje menos dias\n",
        "- Fim: ontem às 18:10 (ajustado para o fechamento)\n",
        "- Verifica se já existem dados anteriores salvos (dados_brutos.csv):\n",
        "- Se sim, tenta encontrar a última data registrada válida e usa como novo início e(xtração incremental).\n",
        "- Faz a requisição ao Yahoo Finance (yf.download), no intervalo necessário.\n",
        "- Ajusta o fuso horário dos dados para \"America/Sao_Paulo\".\n",
        "- Mescla os dados novos aos antigos (se houver), remove duplicatas e linhas com muitos nulos.\n",
        "- Salva o conjunto atualizado no CSV.\n",
        "- Retorna o conjunto de dados atualizado.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def extrair_dados(ticker, dias, intervalo, dados_brutos):\n",
        "    \"\"\"Extrai e organiza dados do Yahoo Finance no intervalo correto.\"\"\"\n",
        "    df_total = pd.DataFrame()\n",
        "    data_inicio = data_inicio = (datetime.today() - timedelta(days=dias)).date()\n",
        "    data_fim = datetime.now().replace(hour=18, minute=10, second=0, microsecond=0) - timedelta(days=1)\n",
        "\n",
        "    primeira_extracao = True\n",
        "\n",
        "    if os.path.exists(dados_brutos) and os.path.getsize(dados_brutos) > 0:\n",
        "        df_temp = pd.read_csv(\n",
        "            dados_brutos,\n",
        "            index_col=0,\n",
        "            parse_dates=True,\n",
        "            date_format=\"%Y-%m-%d %H:%M:%S\"\n",
        "        )\n",
        "        if not df_temp.empty:\n",
        "            df_temp = df_temp.iloc[2:].copy()\n",
        "            df_temp.index.name = 'data'\n",
        "            df_temp = df_temp.reset_index()\n",
        "            df_temp['data'] = pd.to_datetime(df_temp['data']).dt.date\n",
        "            ultima_data = pd.to_datetime(df_temp['data'], errors='coerce').max()\n",
        "            print(f\"Última data encontrada: {ultima_data}\")\n",
        "\n",
        "            if pd.notnull(ultima_data):\n",
        "                data_inicio = ultima_data + timedelta(days=1)\n",
        "                primeira_extracao = False\n",
        "\n",
        "    if primeira_extracao:\n",
        "        print(f\"\\nPrimeira extração de dados.\")\n",
        "        print(f\"Data de início: {data_inicio.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"Data de fim: {data_fim.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    else:\n",
        "        print(f\"\\nExtração complementar a partir de {data_inicio.strftime('%Y-%m-%d %H:%M:%S')}.\")\n",
        "        print(f\"Data de fim: {data_fim.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "    df_novo = yf.download(\n",
        "        ticker,\n",
        "        start=data_inicio.strftime(\"%Y-%m-%d\"),\n",
        "        end=data_fim.strftime(\"%Y-%m-%d\"),\n",
        "        interval=intervalo,\n",
        "        progress=False\n",
        "    )\n",
        "\n",
        "\n",
        "    if not df_novo.empty:\n",
        "        df_novo.index = (\n",
        "            df_novo.index.tz_convert(\"America/Sao_Paulo\")\n",
        "            if df_novo.index.tz is not None\n",
        "            else df_novo.index.tz_localize(\"UTC\").tz_convert(\"America/Sao_Paulo\")\n",
        "        )\n",
        "\n",
        "        if os.path.exists(dados_brutos) and not primeira_extracao:\n",
        "            df_antigo = pd.read_csv(dados_brutos, index_col=0, parse_dates=True)\n",
        "            df_total = pd.concat([df_antigo, df_novo])\n",
        "        else:\n",
        "            df_total = df_novo\n",
        "\n",
        "        df_total = (\n",
        "            df_total.drop_duplicates()\n",
        "            .dropna(thresh=df_total.shape[1] * 0.5)\n",
        "\n",
        "        )\n",
        "\n",
        "        df_total.to_csv(dados_brutos)\n",
        "        print(\"Dados somados e salvos com sucesso.\")\n",
        "    else:\n",
        "        print(\"Nenhum dado complementar foi adicionado.\")\n",
        "\n",
        "    return df_total\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker = \"BBDC4.SA\"\n",
        "    intervalo = \"5m\"\n",
        "    dias = 45\n",
        "    dados_brutos = \"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\"\n",
        "    df = extrair_dados(ticker, dias, intervalo, dados_brutos)\n"
      ],
      "metadata": {
        "id": "GtzFbqU30zFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Executando extração de dados\n",
        "from Piloto_Day_Trade.scripts.pipeline.extracao_dadosvf import extrair_dados\n",
        "\n",
        "ticker = \"BBDC4.SA\"\n",
        "intervalo = \"5m\"\n",
        "dias = 45\n",
        "dados_brutos = \"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\"\n",
        "df = extrair_dados(ticker, dias, intervalo, dados_brutos)\n"
      ],
      "metadata": {
        "id": "OgEhS9Rwx_Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Piloto_Day_Trade/data/raw/dados_brutos.csv', index_col=0, parse_dates=True, dayfirst=True)"
      ],
      "metadata": {
        "id": "0SDrDXRzHygO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "GzXp5Ajixv6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Piloto_Day_Trade\n",
        "\n",
        "from scripts.operacional.atualizar_repo import atualizar_repo\n",
        "\n",
        "atualizar_repo(\"Extração de dados brutos\")"
      ],
      "metadata": {
        "id": "xoL6wIUHZQLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/limpeza_dados.py\n",
        "\n",
        "#@title ## Limpeza de dados\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def limpeza_dados(df, path_dados_limpos):\n",
        "    # Verificar se os dados estão corretos\n",
        "    print(\"Dados originais:\")\n",
        "    print(df.head())\n",
        "    print(df.info())\n",
        "\n",
        "    # Remover as primeiras duas linhas (com 'Ticker' e 'Datetime')\n",
        "    df = df.iloc[2:].copy()\n",
        "\n",
        "    # Verificar após a remoção\n",
        "    print(\"Após remoção das duas primeiras linhas:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Garantir que o índice esteja no formato de data e hora (timezone UTC)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "\n",
        "    # Definir o fuso horário como \"America/Sao_Paulo\"\n",
        "    df.index = df.index.tz_convert(\"America/Sao_Paulo\")\n",
        "\n",
        "    # Remover a referência de fuso horário\n",
        "    df.index = df.index.tz_localize(None)\n",
        "\n",
        "    # Criar a coluna 'hora' com base no índice\n",
        "    df['hora'] = df.index.strftime('%H:%M:%S')\n",
        "\n",
        "    # Renomear o índice para 'data'\n",
        "    df.index.name = 'data'\n",
        "\n",
        "    # Resetar o índice para transformar o Datetime em uma coluna normal\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # Verificar após a transformação do índice\n",
        "    print(\"\\nApós conversão de índice:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Remover o horário da coluna 'data', mantendo apenas a data\n",
        "    df['data'] = df['data'].dt.date\n",
        "\n",
        "    # Mapeamento das colunas para nomes padronizados\n",
        "    mapeamento_colunas = {\n",
        "        'Close': 'fechamento',\n",
        "        'High': 'maximo',\n",
        "        'Low': 'minimo',\n",
        "        'Open': 'abertura',\n",
        "        'Volume': 'volume'\n",
        "    }\n",
        "\n",
        "    # Renomear as colunas\n",
        "    df.rename(columns=mapeamento_colunas, inplace=True)\n",
        "\n",
        "    # Converte e arredonda as colunas numéricas\n",
        "    for col in ['abertura', 'minimo', 'maximo', 'fechamento']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').round(2)\n",
        "\n",
        "    # Converte a coluna 'volume' para número inteiro\n",
        "    df['volume'] = pd.to_numeric(df['volume'], errors='coerce', downcast='integer')\n",
        "\n",
        "    # Reorganiza as colunas na ordem desejada\n",
        "    df = df[['data', 'hora', 'abertura', 'minimo', 'maximo', 'fechamento', 'volume']]\n",
        "\n",
        "    # Verificar após reorganizar as colunas\n",
        "    print(\"\\nApós reorganizar as colunas:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Verificar e remover duplicatas mantendo a primeira ocorrência\n",
        "    df = df.drop_duplicates(keep='first')\n",
        "\n",
        "    # Remover as linhas com 50% ou mais de valores nulos\n",
        "    df = df.dropna(thresh=df.shape[1] * 0.5)\n",
        "\n",
        "    # Verificar após remoção de duplicatas e nulos\n",
        "    print(\"\\nApós remover duplicatas e nulos:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Garantir que 'data' e 'hora' estejam no formato datetime\n",
        "    df['data'] = pd.to_datetime(df['data'], format='%Y-%m-%d')\n",
        "    df['hora'] = pd.to_datetime(df['hora'], format='%H:%M:%S').dt.time\n",
        "\n",
        "    # Filtra apenas os dias úteis (segunda a sexta)\n",
        "    df = df[df['data'].dt.weekday < 5]\n",
        "\n",
        "    # Verificar após filtrar dias úteis\n",
        "    print(\"\\nApós filtrar apenas os dias úteis:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Filtra apenas horários entre 09:55 e 18:05\n",
        "    df = df[(df['hora'] >= pd.to_datetime('09:55:00').time()) &\n",
        "            (df['hora'] <= pd.to_datetime('18:05:00').time())]\n",
        "\n",
        "    # Verificar após filtrar o intervalo de horário\n",
        "    print(\"\\nApós filtrar o intervalo de horário (09:55-18:05):\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Caso o DataFrame fique vazio, informar o motivo\n",
        "    if df.empty:\n",
        "        print(\"O DataFrame ficou vazio após o filtro de horário. Verifique se os dados estão dentro do intervalo de 09:55-18:05.\")\n",
        "    else:\n",
        "        print(\"\\nLimpeza de dados concluída com sucesso.\")\n",
        "\n",
        "    # Ordenar os dados\n",
        "    df = df.sort_values([\"data\", \"hora\"], ascending=[False, True])\n",
        "    print(\"\\nDados limpos e ordenados:\")\n",
        "    print(df.head(10))\n",
        "\n",
        "    # Salva os dados limpos em CSV\n",
        "    df.to_csv(path_dados_limpos, index=False)\n",
        "    print(f\"\\nOs dados foram limpos e salvos em csv.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ler os dados brutos\n",
        "    dados_brutos = pd.read_csv(f\"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\", index_col=0, parse_dates=True, dayfirst=True)\n",
        "    path_dados_limpos = '/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv'\n",
        "    # Aplicar limpeza nos dados\n",
        "    df_limpo = limpeza_dados(dados_brutos, path_dados_limpos)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DIHkNWR1-QS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aplicando limpeza de dados\n",
        "from Piloto_Day_Trade.scripts.pipeline.limpeza_dados import limpeza_dados\n",
        "\n",
        "dados_brutos = pd.read_csv(f\"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\", index_col=0, parse_dates=True, dayfirst=True)\n",
        "path_dados_limpos = '/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv'\n",
        "df_limpo = limpeza_dados(dados_brutos, path_dados_limpos)\n",
        "\n"
      ],
      "metadata": {
        "id": "97mwKCsf1V3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_limpo = pd.read_csv('/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv')"
      ],
      "metadata": {
        "id": "0ZQuSPZcaVwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_limpo.info()"
      ],
      "metadata": {
        "id": "7KQP5yR2abzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_limpo.tail()"
      ],
      "metadata": {
        "id": "XTEGfZtqFcb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/transformacao_dados_versao_final.py\n",
        "\n",
        "#@title Transformação de dados\n",
        "\n",
        "\"\"\"\n",
        "Script de Transformação de Dados - Piloto Day Trade\n",
        "\n",
        "Este script realiza o processo de transformação dos dados financeiros, incluindo o cálculo de indicadores técnicos,\n",
        "adição de características temporais e diárias, além de filtrar novos dados a partir da última data registrada.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "def carregar_dados(arquivo):\n",
        "    \"\"\"\n",
        "    Carrega dados de um arquivo CSV ou retorna o DataFrame se já for fornecido como argumento.\n",
        "\n",
        "    Parâmetros:\n",
        "    Caminho do arquivo ou DataFrame a ser carregado.\n",
        "\n",
        "    Retorna:\n",
        "    Dados carregados a partir do arquivo ou argumento.\n",
        "    \"\"\"\n",
        "    if isinstance(arquivo, pd.DataFrame):\n",
        "        return arquivo\n",
        "    if not os.path.exists(arquivo):\n",
        "        print(f\"O arquivo {arquivo} não existe.\")\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_csv(arquivo, parse_dates=[\"data\"])\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar {arquivo}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def obter_ultima_data(df):\n",
        "    \"\"\"\n",
        "    Retorna a última data registrada no DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    df (pd.DataFrame): O DataFrame a ser analisado.\n",
        "\n",
        "    Retorna:\n",
        "    datetime: Última data registrada no DataFrame ou None se não houver dados.\n",
        "    \"\"\"\n",
        "    return df[\"data\"].max() if \"data\" in df.columns and not df.empty else None\n",
        "\n",
        "def filtrar_novos_dados(df, ultima_data):\n",
        "    \"\"\"\n",
        "    Filtra os dados para obter apenas os registros com data posterior à última data fornecida.\n",
        "\n",
        "    Parâmetros:\n",
        "    O DataFrame a ser filtrado.\n",
        "    A última data registrada nos dados transformados.\n",
        "\n",
        "    Retorna:\n",
        "    pd.DataFrame: Dados filtrados com data posterior à última data.\n",
        "    \"\"\"\n",
        "    return df[df[\"data\"] > ultima_data] if ultima_data else df\n",
        "\n",
        "def calcular_indicadores(df):\n",
        "    \"\"\"\n",
        "    Calcula indicadores técnicos financeiros e adiciona novas colunas ao DataFrame.\n",
        "\n",
        "    Parâmetros:\n",
        "    O DataFrame com dados financeiros que será transformado.\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame com as colunas dos indicadores calculados.\n",
        "    \"\"\"\n",
        "    # Verifica se o DataFrame possui as colunas necessárias para cálculo dos indicadores\n",
        "    if df.empty or not all(c in df.columns for c in ['data', 'hora', 'abertura', 'minimo', 'maximo', 'fechamento', 'volume']):\n",
        "        return df\n",
        "\n",
        "    # Ordena os dados por data e hora\n",
        "    df = df.sort_values(by=['data', 'hora'])\n",
        "\n",
        "    # Calcula o retorno percentual\n",
        "    df['retorno'] = df['fechamento'].pct_change()\n",
        "\n",
        "    # Calcula volatilidade, médias móveis e outros indicadores\n",
        "    df['volatilidade'] = df['retorno'].rolling(20).std()\n",
        "    # Substituir os primeiros NaN por 0\n",
        "    df['volatilidade'] = df['volatilidade'].fillna(0)\n",
        "    df['SMA_10'] = df['fechamento'].rolling(10).mean()\n",
        "    df['EMA_10'] = df['fechamento'].ewm(span=10).mean()\n",
        "    df['MACD'] = df['fechamento'].ewm(span=12).mean() - df['fechamento'].ewm(span=26).mean()\n",
        "    df['Signal_Line'] = df['MACD'].ewm(span=9).mean()\n",
        "\n",
        "    # Cálculo do índice de força relativa (RSI)\n",
        "    ganho = df['retorno'].clip(lower=0)\n",
        "    perda = -df['retorno'].clip(upper=0)\n",
        "    media_ganho = ganho.ewm(span=14).mean()\n",
        "    media_perda = perda.ewm(span=14).mean() + 1e-10\n",
        "    df['rsi'] = 100 - (100 / (1 + media_ganho / media_perda))\n",
        "\n",
        "    # Cálculo do On-Balance Volume (OBV)\n",
        "    df['OBV'] = (df['volume'] * np.sign(df['fechamento'].diff())).fillna(0).cumsum()\n",
        "\n",
        "    # Cálculo do ADX e outros indicadores de tendência\n",
        "    delta_high = df['maximo'].diff()\n",
        "    delta_low = -df['minimo'].diff()\n",
        "    plus_dm = np.where((delta_high > delta_low) & (delta_high > 0), delta_high, 0)\n",
        "    minus_dm = np.where((delta_low > delta_high) & (delta_low > 0), delta_low, 0)\n",
        "    tr1 = df['maximo'] - df['minimo']\n",
        "    tr2 = abs(df['maximo'] - df['fechamento'].shift())\n",
        "    tr3 = abs(df['minimo'] - df['fechamento'].shift())\n",
        "    tr = np.max([tr1, tr2, tr3], axis=0)\n",
        "    atr = pd.Series(tr).rolling(14).mean()\n",
        "    df['ADX'] = 100 * abs((pd.Series(plus_dm).rolling(14).mean() - pd.Series(minus_dm).rolling(14).mean()) /\n",
        "                          (pd.Series(plus_dm).rolling(14).mean() + pd.Series(minus_dm).rolling(14).mean() + 1e-10)).rolling(14).mean()\n",
        "\n",
        "    # Calcula as Bandas de Bollinger\n",
        "    df['BB_MA20'] = df['fechamento'].rolling(20).mean()\n",
        "    df['BB_STD20'] = df['fechamento'].rolling(20).std()\n",
        "    df['BB_upper'] = df['BB_MA20'] + (2 * df['BB_STD20'])\n",
        "    df['BB_lower'] = df['BB_MA20'] - (2 * df['BB_STD20'])\n",
        "\n",
        "    # Calcula o Stochastic Oscillator (%K e %D)\n",
        "    low14 = df['minimo'].rolling(14).min()\n",
        "    high14 = df['maximo'].rolling(14).max()\n",
        "    df['%K'] = 100 * ((df['fechamento'] - low14) / (high14 - low14 + 1e-10))\n",
        "    df['%D'] = df['%K'].rolling(3).mean()\n",
        "\n",
        "    # Calcula o Commodity Channel Index (CCI)\n",
        "    tp = (df['maximo'] + df['minimo'] + df['fechamento']) / 3\n",
        "    cci_ma = tp.rolling(20).mean()\n",
        "    cci_std = tp.rolling(20).std()\n",
        "    df['CCI'] = (tp - cci_ma) / (0.015 * cci_std + 1e-10)\n",
        "\n",
        "    # Inclui a volatilidade (ATR)\n",
        "    df['ATR'] = atr\n",
        "\n",
        "    # Cria colunas com lags de fechamento, retorno e volume\n",
        "    for lag in range(1, 4):\n",
        "        df[f'fechamento_lag{lag}'] = df['fechamento'].shift(lag)\n",
        "        df[f'retorno_lag{lag}'] = df['retorno'].shift(lag)\n",
        "        df[f'volume_lag{lag}'] = df['volume'].shift(lag)\n",
        "\n",
        "    return df\n",
        "\n",
        "def adicionar_features_temporais(df):\n",
        "    \"\"\"\n",
        "    Adiciona características temporais ao DataFrame, como o dia da semana e hora numérica.\n",
        "\n",
        "    Parâmetros:\n",
        "    df (pd.DataFrame): O DataFrame com dados financeiros.\n",
        "\n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame com as características temporais adicionadas.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Adiciona o dia da semana e a previsão do dia da semana\n",
        "    df['data'] = pd.to_datetime(df['data'], errors='coerce')\n",
        "    df['dia_da_semana_entrada'] = df['data'].dt.weekday\n",
        "    df.loc[df['dia_da_semana_entrada'] == 4, 'dia_da_semana_previsao'] = (df['data'] + pd.DateOffset(days=3)).dt.weekday\n",
        "\n",
        "    # Adiciona hora e minuto\n",
        "    if 'hora' in df.columns:\n",
        "        df['hora'] = pd.to_datetime(df['hora'].astype(str), format='%H:%M:%S', errors='coerce').dt.time\n",
        "        df['hora_num'] = df['hora'].apply(lambda x: x.hour if pd.notnull(x) else np.nan)\n",
        "        df['minuto'] = df['hora'].apply(lambda x: x.minute if pd.notnull(x) else np.nan)\n",
        "    else:\n",
        "        df['hora_num'] = np.nan\n",
        "        df['minuto'] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "def adicionar_features_diarias(df):\n",
        "    \"\"\"\n",
        "    Adiciona características diárias, como fechamento, volume e máximos/mínimos diários.\n",
        "\n",
        "    Parâmetros:\n",
        "    df (pd.DataFrame): O DataFrame com dados financeiros.\n",
        "\n",
        "    Retorna:\n",
        "    pd.DataFrame: DataFrame com as características diárias adicionadas.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Adiciona características diárias, agrupadas por data\n",
        "    df['fechamento_dia'] = df.groupby('data')['fechamento'].transform('last')\n",
        "    df['volume_dia'] = df.groupby('data')['volume'].transform('sum')\n",
        "    df['maximo_dia'] = df.groupby('data')['maximo'].transform('max')\n",
        "    df['minimo_dia'] = df.groupby('data')['minimo'].transform('min')\n",
        "\n",
        "    # Adiciona as características do dia anterior\n",
        "    df['fechamento_dia_anterior'] = df['fechamento_dia'].shift(1)\n",
        "    df['volume_dia_anterior'] = df['volume_dia'].shift(1).fillna(0).astype(int)\n",
        "    df['maximo_dia_anterior'] = df['maximo_dia'].shift(1)\n",
        "    df['minimo_dia_anterior'] = df['minimo_dia'].shift(1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def transformar_dados(dados_limpos, dados_transformados):\n",
        "    \"\"\"\n",
        "    Função principal para carregar, filtrar, calcular indicadores e transformar os dados financeiros.\n",
        "\n",
        "    Parâmetros:\n",
        "    dados_limpos: Caminho para os dados limpos a serem processados.\n",
        "    dados_transformados: Caminho onde os dados transformados serão salvos.\n",
        "\n",
        "    Retorna:\n",
        "    DataFrame com os dados transformados.\n",
        "    \"\"\"\n",
        "    # Carrega os dados limpos e transformados\n",
        "    df_transformado = carregar_dados(dados_transformados)\n",
        "    df_limpo = carregar_dados(dados_limpos)\n",
        "\n",
        "    # Obtem a última data dos dados transformados\n",
        "    ultima_data = obter_ultima_data(df_transformado)\n",
        "\n",
        "    # Filtra os dados limpos para obter apenas os novos dados\n",
        "    novos_dados = filtrar_novos_dados(df_limpo, ultima_data)\n",
        "\n",
        "    if not novos_dados.empty:\n",
        "        # Calcula os indicadores e adiciona as features temporais e diárias\n",
        "        novos_dados = calcular_indicadores(novos_dados)\n",
        "        novos_dados = adicionar_features_temporais(novos_dados)\n",
        "        novos_dados = adicionar_features_diarias(novos_dados)\n",
        "\n",
        "        # Concatena os novos dados com os dados transformados existentes\n",
        "        df_final = pd.concat([df_transformado, novos_dados], ignore_index=True) if not df_transformado.empty else novos_dados\n",
        "\n",
        "        # Remove registros com valores ausentes\n",
        "        linhas_antes = len(df_final)\n",
        "        df_final = df_final.dropna(subset=['fechamento', 'retorno', 'SMA_10', 'EMA_10', 'MACD', 'rsi'])\n",
        "        linhas_depois = len(df_final)\n",
        "        print(f\"Linhas perdidas no dropna: {linhas_antes - linhas_depois}\")\n",
        "\n",
        "        # Salva os dados transformados\n",
        "        os.makedirs(os.path.dirname(dados_transformados), exist_ok=True)\n",
        "        df_final.to_csv(dados_transformados, index=False)\n",
        "        print(f\"Dados transformados salvos em {dados_transformados} ({len(df_final)} registros)\")\n",
        "        return df_final\n",
        "\n",
        "    else:\n",
        "        print(\"Nenhum novo dado para processar.\")\n",
        "        return df_transformado\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define os caminhos dos arquivos de dados\n",
        "    path_dados_limpos = '/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv'\n",
        "    path_dados_transformados = '/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv'\n",
        "\n",
        "    # Executa a transformação de dados\n",
        "    df_transformado = transformar_dados(path_dados_limpos, path_dados_transformados)\n"
      ],
      "metadata": {
        "id": "ya58qSxHIXIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Piloto_Day_Trade.scripts.pipeline.transformacao_dados_versao_final import transformar_dados\n",
        "\n",
        "#@title Aplicando transformação de dados\n",
        "path_dados_limpos = '/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv'\n",
        "path_dados_transformados = '/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv'\n",
        "df_transformado = transformar_dados(path_dados_limpos, path_dados_transformados)\n",
        "print(df_transformado.head(5))"
      ],
      "metadata": {
        "id": "rPlOj1oYePmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformado = pd.read_csv('/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv')"
      ],
      "metadata": {
        "id": "-nvaDDrQducd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformado.info()"
      ],
      "metadata": {
        "id": "VWkjyIESd06k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando os tipos de dados\n",
        "df_transformado.dtypes"
      ],
      "metadata": {
        "id": "4G3hB-sknn-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/data/transformed/dados_transformados_doc.md\n",
        "\n",
        "## Escolha das Features na Transformação dos Dados\n",
        "\n",
        "O conjunto de dados transformado para o modelo de previsão intradiária de preços do ativo BBDC4 foi projetado para capturar diferentes aspectos do comportamento do mercado financeiro. Abaixo estão listadas as features selecionadas e as respectivas razões para sua inclusão:\n",
        "\n",
        "### 1. **Retorno e Volatilidade**\n",
        "- **retorno**: Variação percentual do preço de fechamento entre candles consecutivos.\n",
        "- **volatilidade**: Desvio padrão do retorno em uma janela móvel, mede a instabilidade e o risco.\n",
        "\n",
        "### 2. **Médias Móveis**\n",
        "- **SMA_10**: Média móvel simples de 10 períodos do fechamento.\n",
        "- **EMA_10**: Média móvel exponencial de 10 períodos, mais sensível a variações recentes.\n",
        "\n",
        "### 3. **Indicadores de Tendência**\n",
        "- **MACD**: Diferença entre a média móvel exponencial de 12 e 26 períodos.\n",
        "- **Signal_Line**: Média móvel exponencial de 9 períodos do MACD.\n",
        "- **ADX**: Índice direcional médio que quantifica a força da tendência.\n",
        "\n",
        "### 4. **Indicadores de Momento**\n",
        "- **RSI (Relative Strength Index)**: Índice de força relativa, indica sobrecompra ou sobrevenda.\n",
        "- **CCI (Commodity Channel Index)**: Mede variações do preço em relação à média, útil para detectar desvios extremos.\n",
        "\n",
        "### 5. **Indicadores de Volume**\n",
        "- **volume**: Volume negociado no candle.\n",
        "- **OBV (On-Balance Volume)**: Indicador cumulativo que relaciona variação de preço com volume.\n",
        "\n",
        "### 6. **Bandas de Bollinger**\n",
        "- **BB_upper**: Banda superior (média + 2 desvios).\n",
        "- **BB_lower**: Banda inferior (média - 2 desvios).\n",
        "- **BB_MA20**: Média móvel central de 20 períodos.\n",
        "- **BB_STD20**: Desvio padrão usado no cálculo das bandas.\n",
        "\n",
        "### 7. **Estocástico**\n",
        "- **%K**: Oscilador estocástico baseado em máximas e mínimas.\n",
        "- **%D**: Média móvel de 3 períodos de %K.\n",
        "\n",
        "### 8. **ATR (Average True Range)**\n",
        "- **ATR**: Média da faixa verdadeira, mede a amplitude de oscilação de preços.\n",
        "\n",
        "### 9. **Lags Temporais**\n",
        "- **fechamento_lag1, fechamento_lag2, fechamento_lag3**: Preços de fechamento anteriores.\n",
        "- **retorno_lag1, retorno_lag2, retorno_lag3**: Retornos anteriores.\n",
        "- **volume_lag1, volume_lag2, volume_lag3**: Volumes negociados anteriores.\n",
        "\n",
        "### 10. **Variáveis Temporais**\n",
        "- **dia_da_semana_entrada**: Dia da semana correspondente ao candle atual.\n",
        "- **dia_da_semana_previsao**: Dia da semana do candle de previsão.\n",
        "- **hora_num**: Representação numérica da hora do candle.\n",
        "- **minuto**: Minuto do candle.\n",
        "\n",
        "### 11. **Resumo Diário do Dia Anterior**\n",
        "- **fechamento_dia_anterior**: Preço de fechamento do último candle do dia anterior.\n",
        "- **volume_dia_anterior**: Volume total negociado no dia anterior.\n",
        "- **maximo_dia_anterior**: Maior preço registrado no dia anterior.\n",
        "- **minimo_dia_anterior**: Menor preço registrado no dia anterior.\n"
      ],
      "metadata": {
        "id": "lD3bvRfxzlG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/operacional/diagnostico_qualidade_dados.py\n",
        "\n",
        "# diagnostico_qualidade_dados.py\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def diagnosticar_qualidade_dados(df, path_output):\n",
        "    \"\"\"\n",
        "    Gera diagnóstico de qualidade dos dados:\n",
        "    duplicação, nulos, valores únicos, gaps e outliers.\n",
        "\n",
        "    Parâmetros:\n",
        "    df (pd.DataFrame): Conjunto de dados analisado.\n",
        "    path_output (str): Caminho para salvar o relatório CSV.\n",
        "\n",
        "    Retorna:\n",
        "    None\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(path_output), exist_ok=True)\n",
        "\n",
        "    relatorio = []\n",
        "\n",
        "    # Linhas duplicadas\n",
        "    relatorio.append([\"Duplicatas (linhas completas)\", df.duplicated().sum()])\n",
        "\n",
        "    # Gaps de tempo (se houver colunas data e hora)\n",
        "    if {'data', 'hora'}.issubset(df.columns):\n",
        "        try:\n",
        "            df_ordenado = df.sort_values(['data', 'hora'])\n",
        "            df_ordenado['timestamp'] = pd.to_datetime(df_ordenado['data'] + ' ' + df_ordenado['hora'])\n",
        "            gaps = df_ordenado['timestamp'].diff().gt(pd.Timedelta(minutes=5)).sum()\n",
        "            relatorio.append([\"Gaps (>5min entre registros)\", int(gaps)])\n",
        "        except Exception as e:\n",
        "            relatorio.append([\"Erro ao verificar gaps\", str(e)])\n",
        "    else:\n",
        "        relatorio.append([\"Gaps\", \"Colunas 'data' e 'hora' não encontradas\"])\n",
        "\n",
        "    # Valores nulos\n",
        "    for col in df.columns:\n",
        "        pct_null = df[col].isnull().mean() * 100\n",
        "        relatorio.append([f\"Nulos (%) - {col}\", round(pct_null, 2)])\n",
        "\n",
        "    # Valores únicos\n",
        "    for col in df.columns:\n",
        "        unicos = df[col].nunique()\n",
        "        relatorio.append([f\"Valores únicos - {col}\", unicos])\n",
        "\n",
        "    # Outliers (IQR)\n",
        "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]\n",
        "        relatorio.append([f\"Outliers (IQR) - {col}\", len(outliers)])\n",
        "\n",
        "    # Converter para DataFrame e salvar\n",
        "    relatorio_df = pd.DataFrame(relatorio, columns=[\"Verificação\", \"Resultado\"])\n",
        "    relatorio_df.to_csv(path_output, index=False)\n",
        "    print(f\"Relatório salvo em: {path_output}\")\n"
      ],
      "metadata": {
        "id": "b-78UyfWtXq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aplicar dignoticar dados\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/Piloto_Day_Trade\")\n",
        "\n",
        "from Piloto_Day_Trade.scripts.operacional.diagnostico_qualidade_dados import diagnosticar_qualidade_dados\n",
        "# Carregar dados a serem verificados\n",
        "\n",
        "# Definir caminhos\n",
        "df = pd.read_csv('/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv')\n",
        "path_output = '/content/Piloto_Day_Trade/reports/diagnostico_qualidade.csv'\n",
        "\n",
        "# Aplicar função diagnostica\n",
        "diagnosticar_qualidade_dados(df, path_output)\n"
      ],
      "metadata": {
        "id": "JaWR0BFwfRMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relatorio = pd.read_csv('/content/Piloto_Day_Trade/reports/diagnostico_qualidade.csv')\n",
        "print(relatorio.head())"
      ],
      "metadata": {
        "id": "DleOzYBkldss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Banco de dados"
      ],
      "metadata": {
        "id": "Ap-meZswIhir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/modelagem/esquema/esquema_dimensional.md\n",
        "\n",
        "#@title Definição do esquema - Modelo Estrela\n",
        "\"\"\"\n",
        "O modelo estrela foi escolhido por sua simplicidade e clareza na organização dos dados para análise. Ele é ideal para consultas rápidas e análise preditiva. No nosso projeto, temos um único fato (preços OHLC) e múltiplas variáveis explicativas que os influenciam.\n",
        "\n",
        "A estrutura facilita agregações temporais e análises do comportamento dos preços, sendo também eficiente para alimentar o pipeline de machine learning. Ao organizar as variáveis preditoras ao redor das medidas de preço, conseguimos isolar responsabilidades e tornar as análises mais precisas e escaláveis.\n",
        "\"\"\"\n",
        "\n",
        "## Tabela Fato: `fato_precos`\n",
        "| Coluna         | Tipo   | Descrição                                   |\n",
        "|----------------|--------|---------------------------------------------|\n",
        "| id_fato_precos | int    | PK, identificador único da linha            |\n",
        "| id_tempo       | int    | FK para a dimensão tempo                    |\n",
        "| abertura       | float  | Preço de abertura                           |\n",
        "| minimo         | float  | Preço mínimo                                |\n",
        "| maximo         | float  | Preço máximo                                |\n",
        "| fechamento     | float  | Preço de fechamento (variável alvo)         |\n",
        "\n",
        "## Dimensão: `dim_tempo`\n",
        "| Coluna                | Tipo   | Descrição                                 |\n",
        "|------------------------|--------|-------------------------------------------|\n",
        "| id_tempo              | int    | PK                                        |\n",
        "| data                  | object | Data da observação                        |\n",
        "| hora                  | object | Hora da observação                        |\n",
        "| dia_da_semana_entrada | int    | Dia da semana da entrada (0=Seg, 6=Dom)   |\n",
        "| hora_num              | int    | Hora como número inteiro                  |\n",
        "| minuto                | int    | Minuto da observação                      |\n",
        "\n",
        "## Dimensão: `dim_indicadores`\n",
        "| Coluna       | Tipo   | Descrição                                       |\n",
        "|--------------|--------|--------------------------------------------------|\n",
        "| id_indicadores | int  | PK                                               |\n",
        "| id_tempo     | int    | FK para a dimensão tempo                        |\n",
        "| SMA_10       | float  | Média móvel simples de 10 períodos              |\n",
        "| EMA_10       | float  | Média móvel exponencial de 10 períodos          |\n",
        "| MACD         | float  | Moving Average Convergence Divergence           |\n",
        "| Signal_Line  | float  | Linha de sinal do MACD                          |\n",
        "| rsi          | float  | Índice de força relativa                        |\n",
        "| OBV          | float  | On-Balance Volume                               |\n",
        "| CCI          | float  | Commodity Channel Index                         |\n",
        "| ATR          | float  | Average True Range                              |\n",
        "| retorno      | float  | Retorno do período                              |\n",
        "| volatilidade | float  | Volatilidade do período                         |\n",
        "\n",
        "## Dimensão: `dim_lags`\n",
        "| Coluna          | Tipo   | Descrição                                       |\n",
        "|-----------------|--------|--------------------------------------------------|\n",
        "| id_lags         | int    | PK                                              |\n",
        "| id_tempo        | int    | FK para a dimensão tempo                        |\n",
        "| fechamento_lag1 | float  | Fechamento no candle anterior (1 lag)           |\n",
        "| retorno_lag1    | float  | Retorno do candle anterior (1 lag)              |\n",
        "| volume_lag1     | float  | Volume do candle anterior (1 lag)               |\n",
        "| fechamento_lag2 | float  | Fechamento dois candles atrás (2 lags)          |\n",
        "| retorno_lag2    | float  | Retorno dois candles atrás (2 lags)             |\n",
        "| volume_lag2     | float  | Volume dois candles atrás (2 lags)              |\n",
        "| fechamento_lag3 | float  | Fechamento três candles atrás (3 lags)          |\n",
        "| retorno_lag3    | float  | Retorno três candles atrás (3 lags)             |\n",
        "| volume_lag3     | float  | Volume três candles atrás (3 lags)              |\n",
        "\n",
        "## Dimensão: `dim_operacional`\n",
        "| Coluna                 | Tipo   | Descrição                                      |\n",
        "|------------------------|--------|------------------------------------------------|\n",
        "| id_operacional         | int    | PK                                             |\n",
        "| id_tempo               | int    | FK para a dimensão tempo                       |\n",
        "| data_previsao          | object | Data prevista para o modelo                    |\n",
        "| dia_da_semana_previsao | int    | Dia da semana da previsão                      |\n",
        "| hora_num               | int    | Hora como número inteiro                       |\n",
        "| minuto                 | int    | Minuto da observação                           |\n",
        "| mercado_aberto         | int    | Indicador binário (1=aberto, 0=fechado)        |\n",
        "| fechamento_dia         | float  | Fechamento do dia (valor agregado diário)      |\n",
        "| volume_dia             | float  | Volume do dia (valor agregado diário)          |\n",
        "| maximo_dia             | float  | Máximo do dia                                  |\n",
        "| minimo_dia             | float  | Mínimo do dia                                  |\n",
        "| fechamento_dia_anterior| float  | Fechamento do dia anterior                     |\n",
        "| volume_dia_anterior    | float  | Volume do dia anterior                         |\n",
        "| maximo_dia_anterior    | float  | Máximo do dia anterior                         |\n",
        "| minimo_dia_anterior    | float  | Mínimo do dia anterior                         |\n"
      ],
      "metadata": {
        "id": "kuxIPKOgzMvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/operacional/gerar_catalogo_dados_versao_final.py\n",
        "\n",
        "#@title Script para geração do catálogo de dados\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def gerar_catalogo_dados(df: pd.DataFrame, path_output: str) -> None:\n",
        "    \"\"\"\n",
        "    Gera o catálogo de dados a partir do DataFrame df,\n",
        "    salvando em path_output no formato CSV.\n",
        "    \"\"\"\n",
        "\n",
        "    # Detecta tipos das colunas\n",
        "    col_types = df.dtypes.apply(lambda x: 'float' if 'float' in str(x) else ('int' if 'int' in str(x) else 'object')).to_dict()\n",
        "\n",
        "    # Define tabelas com base nas colunas\n",
        "    tabelas = {\n",
        "        \"fato_precos\": {},\n",
        "        \"dim_tempo\": {},\n",
        "        \"dim_indicadores\": {},\n",
        "        \"dim_lags\": {},\n",
        "        \"dim_operacional\": {}\n",
        "    }\n",
        "\n",
        "    for col, tipo in col_types.items():\n",
        "        if col == \"id_fato_precos\":\n",
        "            tabelas[\"fato_precos\"][col] = tipo\n",
        "        elif col == \"id_tempo\":\n",
        "            for t in [\"dim_tempo\", \"dim_indicadores\", \"dim_lags\", \"dim_operacional\"]:\n",
        "                tabelas[t][col] = tipo\n",
        "        elif \"abertura\" in col or \"fechamento\" in col or \"minimo\" in col or \"maximo\" in col:\n",
        "            tabelas[\"fato_precos\"][col] = tipo\n",
        "        elif any(ind in col for ind in [\"SMA\", \"EMA\", \"MACD\", \"Signal\", \"rsi\", \"OBV\", \"volatilidade\", \"retorno\"]):\n",
        "            tabelas[\"dim_indicadores\"][col] = tipo\n",
        "        elif \"lag\" in col:\n",
        "            tabelas[\"dim_lags\"][col] = tipo\n",
        "        elif col in [\"data\", \"hora\", \"dia_da_semana_entrada\"]:\n",
        "            tabelas[\"dim_tempo\"][col] = tipo\n",
        "        elif col in [\"data_previsao\", \"dia_da_semana_previsao\", \"hora_num\", \"minuto\", \"mercado_aberto\",\n",
        "                     \"fechamento_dia\", \"volume_dia\", \"maximo_dia\", \"minimo_dia\",\n",
        "                     \"fechamento_dia_anterior\", \"volume_dia_anterior\", \"maximo_dia_anterior\", \"minimo_dia_anterior\"]:\n",
        "            tabelas[\"dim_operacional\"][col] = tipo\n",
        "\n",
        "    def dominio(col, tipo):\n",
        "        if tipo in [\"float\", \"int\"]:\n",
        "            if \"retorno\" in col:\n",
        "                return \"-0.05 a 0.05 (retorno percentual por intervalo de 5 min)\"\n",
        "            elif \"volatilidade\" in col:\n",
        "                return \"0 a 0.1 (desvio padrão do retorno por janela de tempo)\"\n",
        "            elif \"abertura\" in col or \"fechamento\" in col or \"minimo\" in col or \"maximo\" in col:\n",
        "                return \"10.0 a 50.0 (valores típicos para BBDC4)\"\n",
        "            elif \"MACD\" in col or \"Signal\" in col:\n",
        "                return \"-5 a 5\"\n",
        "            elif \"rsi\" in col:\n",
        "                return \"0 a 100\"\n",
        "            elif \"OBV\" in col:\n",
        "                return \"valor acumulativo, depende do ativo\"\n",
        "            elif \"volume\" in col:\n",
        "                return \"0 a 1.000.000 (valores inteiros positivos)\"\n",
        "            elif \"dia_da_semana\" in col:\n",
        "                return \"0=Segunda, ..., 6=Domingo\"\n",
        "            elif \"mercado_aberto\" in col:\n",
        "                return \"0=Fechado, 1=Aberto\"\n",
        "            else:\n",
        "                return \"valores numéricos contínuos\"\n",
        "        elif tipo == \"object\":\n",
        "            if \"data\" in col:\n",
        "                return \"formato YYYY-MM-DD\"\n",
        "            elif \"hora\" in col:\n",
        "                return \"formato HH:MM:SS\"\n",
        "            else:\n",
        "                return \"texto livre\"\n",
        "        return \"não especificado\"\n",
        "\n",
        "    def descricao(col):\n",
        "        descricoes = {\n",
        "            \"abertura\": \"Preço de abertura do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "            \"minimo\": \"Menor preço do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "            \"maximo\": \"Maior preço do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "            \"fechamento\": \"Preço de fechamento do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "            \"retorno\": \"Retorno percentual do ativo no intervalo de 5 minutos\",\n",
        "            \"volatilidade\": \"Volatilidade dos retornos do ativo em janela deslizante\",\n",
        "            \"SMA_10\": \"Média móvel simples de 10 períodos calculada sobre os preços\",\n",
        "            \"EMA_10\": \"Média móvel exponencial de 10 períodos\",\n",
        "            \"MACD\": \"Moving Average Convergence Divergence, indicador técnico\",\n",
        "            \"Signal_Line\": \"Linha de sinal do MACD\",\n",
        "            \"rsi\": \"Índice de força relativa (RSI), oscilador técnico\",\n",
        "            \"OBV\": \"On Balance Volume, indicador técnico baseado em volume\",\n",
        "            \"hora_num\": \"Hora expressa como número inteiro\",\n",
        "            \"minuto\": \"Minuto do intervalo de tempo\",\n",
        "            \"mercado_aberto\": \"Indica se o mercado está aberto no horário (1) ou não (0)\",\n",
        "            \"fechamento_dia\": \"Preço de fechamento do ativo no dia corrente\",\n",
        "            \"volume_dia\": \"Volume de negociações do ativo no dia corrente\",\n",
        "            \"maximo_dia\": \"Maior preço do ativo no dia corrente\",\n",
        "            \"minimo_dia\": \"Menor preço do ativo no dia corrente\",\n",
        "            \"fechamento_dia_anterior\": \"Preço de fechamento do ativo no dia anterior\",\n",
        "            \"volume_dia_anterior\": \"Volume de negociações do ativo no dia anterior\",\n",
        "            \"maximo_dia_anterior\": \"Maior preço do ativo no dia anterior\",\n",
        "            \"minimo_dia_anterior\": \"Menor preço do ativo no dia anterior\"\n",
        "        }\n",
        "        for key in descricoes:\n",
        "            if key in col:\n",
        "                return descricoes[key]\n",
        "        if \"lag\" in col:\n",
        "            return f\"Valor defasado de {col.replace('_lag', '')}\"\n",
        "        if \"dia_da_semana\" in col:\n",
        "            return \"Dia da semana correspondente à data\"\n",
        "        if \"id_\" in col:\n",
        "            return \"Identificador único para relacionar com outras tabelas\"\n",
        "        return \"\"\n",
        "\n",
        "    def tecnica(col):\n",
        "        if any(ind in col for ind in [\"SMA\", \"EMA\", \"MACD\", \"Signal\", \"rsi\", \"OBV\"]):\n",
        "            return \"calculado internamente via engenharia de features técnicas\"\n",
        "        if \"lag\" in col:\n",
        "            return \"calculado como valor defasado (lag)\"\n",
        "        if col in [\"data\", \"hora\", \"hora_num\", \"minuto\", \"dia_da_semana_entrada\", \"dia_da_semana_previsao\"]:\n",
        "            return \"extraído de data/hora original\"\n",
        "        if col == \"mercado_aberto\":\n",
        "            return \"derivado da data/hora com base em calendário de mercado\"\n",
        "        return \"cópia ou identificador\"\n",
        "\n",
        "    # Geração do catálogo\n",
        "    linhas = []\n",
        "    for tabela, colunas in tabelas.items():\n",
        "        for col, tipo in colunas.items():\n",
        "            linhas.append({\n",
        "                \"tabela\": tabela,\n",
        "                \"coluna\": col,\n",
        "                \"tipo\": tipo,\n",
        "                \"descricao\": descricao(col),\n",
        "                \"dominio\": dominio(col, tipo),\n",
        "                \"tecnica\": tecnica(col),\n",
        "                \"linhagem\": \"Fonte: Yahoo Finance via yfinance\"\n",
        "            })\n",
        "\n",
        "    catalogo_df = pd.DataFrame(linhas)\n",
        "    Path(path_output).parent.mkdir(parents=True, exist_ok=True)\n",
        "    catalogo_df.to_csv(path_output, index=False)\n",
        "    print(f\"Catálogo de dados gerado em: {path_output}\")\n"
      ],
      "metadata": {
        "id": "2CsipDX8mkB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Piloto_Day_Trade.scripts.operacional.gerar_catalogo_dados_versao_final import gerar_catalogo_dados\n",
        "\n",
        "#@title Aplicar gerar catálogo de dados\n",
        "df = pd.read_csv('/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv')\n",
        "gerar_catalogo_dados(df, \"/content/Piloto_Day_Trade/modelagem/catalog/catalogo_de_dados_vf.csv\")\n",
        "print(\"Catálogo de dados gerado com sucesso!\")"
      ],
      "metadata": {
        "id": "lWV3Q_3angb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/operacional/criar_banco_dimensional_vf.py\n",
        "# @title Script para criar banco e tabelas\n",
        "\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "def criar_banco(db_path):\n",
        "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    sql_script = \"\"\"\n",
        "    -- Tabela Fato\n",
        "    CREATE TABLE IF NOT EXISTS fato_precos (\n",
        "        id_fato_precos INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        abertura REAL,\n",
        "        minimo REAL,\n",
        "        maximo REAL,\n",
        "        fechamento REAL,\n",
        "        volume REAL,\n",
        "        fechamento_dia REAL,  -- Novo campo de fechamento diário\n",
        "        volume_dia REAL,      -- Novo campo de volume diário\n",
        "        maximo_dia REAL,     -- Novo campo de máximo diário\n",
        "        minimo_dia REAL,     -- Novo campo de mínimo diário\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "\n",
        "    -- Dimensão Tempo\n",
        "    CREATE TABLE IF NOT EXISTS dim_tempo (\n",
        "        id_tempo INTEGER PRIMARY KEY,\n",
        "        data TEXT,\n",
        "        hora TEXT,\n",
        "        dia_da_semana_entrada INTEGER\n",
        "    );\n",
        "\n",
        "    -- Dimensão Indicadores Técnicos\n",
        "    CREATE TABLE IF NOT EXISTS dim_indicadores (\n",
        "        id_indicadores INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        SMA_10 REAL,\n",
        "        EMA_10 REAL,\n",
        "        MACD REAL,\n",
        "        Signal_Line REAL,\n",
        "        rsi REAL,\n",
        "        OBV REAL,\n",
        "        retorno REAL,\n",
        "        volatilidade REAL,\n",
        "        fechamento_dia REAL,  -- Novo campo de fechamento diário\n",
        "        volume_dia REAL,      -- Novo campo de volume diário\n",
        "        maximo_dia REAL,     -- Novo campo de máximo diário\n",
        "        minimo_dia REAL,     -- Novo campo de mínimo diário\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "\n",
        "    -- Dimensão Lags\n",
        "    CREATE TABLE IF NOT EXISTS dim_lags (\n",
        "        id_lags INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        fechamento_lag1 REAL,\n",
        "        retorno_lag1 REAL,\n",
        "        volume_lag1 REAL,\n",
        "        fechamento_lag2 REAL,\n",
        "        retorno_lag2 REAL,\n",
        "        volume_lag2 REAL,\n",
        "        fechamento_lag3 REAL,\n",
        "        retorno_lag3 REAL,\n",
        "        volume_lag3 REAL,\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "\n",
        "    -- Dimensão Operacional\n",
        "    CREATE TABLE IF NOT EXISTS dim_operacional (\n",
        "        id_operacional INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        dia_da_semana_previsao INTEGER,\n",
        "        hora_num INTEGER,\n",
        "        minuto INTEGER,\n",
        "        mercado_aberto INTEGER,\n",
        "        fechamento_dia_anterior REAL,  -- Novo campo de fechamento diário anterior\n",
        "        volume_dia_anterior REAL,      -- Novo campo de volume diário anterior\n",
        "        maximo_dia_anterior REAL,     -- Novo campo de máximo diário anterior\n",
        "        minimo_dia_anterior REAL,     -- Novo campo de mínimo diário anterior\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    cursor.executescript(sql_script)\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(\"Banco e tabelas criados com sucesso.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional_vf.db\"\n",
        "    criar_banco(db_path)\n"
      ],
      "metadata": {
        "id": "31PpnrBPqNCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Excecutar criar banco e tabelas\n",
        "!python /content/Piloto_Day_Trade/scripts/operacional/criar_banco_dimensional_vf.py"
      ],
      "metadata": {
        "id": "NGaGzrS1AQcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Gerando catálogo de dados\")"
      ],
      "metadata": {
        "id": "DC29xOqV95Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar criação do banco\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "print(cursor.fetchall())\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "JsDCNhgeA2WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar colunas\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional_vf.db\")\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"PRAGMA table_info(dim_tempo);\")\n",
        "print(cursor.fetchall())\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "rM7plBnSFNHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Carga de dados\n",
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/carga_dados_vf.py\n",
        "\n",
        "# @title Carga de dados\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def carregar_dados(df: pd.DataFrame, db_path: str):\n",
        "    # Lista de colunas obrigatórias que devem existir no DataFrame\n",
        "    colunas_obrigatorias = [\n",
        "        'data', 'hora', 'dia_da_semana_entrada',\n",
        "        'SMA_10', 'EMA_10', 'MACD', 'Signal_Line', 'rsi', 'OBV', 'retorno', 'volatilidade',\n",
        "        'fechamento_lag1', 'retorno_lag1', 'volume_lag1',\n",
        "        'fechamento_lag2', 'retorno_lag2', 'volume_lag2',\n",
        "        'fechamento_lag3', 'retorno_lag3', 'volume_lag3',\n",
        "        'hora_num', 'minuto', 'abertura', 'minimo', 'maximo', 'fechamento'\n",
        "    ]\n",
        "\n",
        "    # Verifica se todas as colunas obrigatórias estão presentes no DataFrame\n",
        "    colunas_faltantes = [col for col in colunas_obrigatorias if col not in df.columns]\n",
        "    if colunas_faltantes:\n",
        "        raise ValueError(f\"Colunas ausentes no DataFrame: {colunas_faltantes}\")\n",
        "\n",
        "    # Conexão com o banco de dados SQLite\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    registros_inseridos = 0\n",
        "\n",
        "    # Loop para inserir os dados do DataFrame nas tabelas do banco\n",
        "    for _, row in df.iterrows():\n",
        "        # Verifica se já existe um registro com a mesma data e hora\n",
        "        cursor.execute(\"SELECT id_tempo FROM dim_tempo WHERE data = ? AND hora = ?\", (row['data'], row['hora']))\n",
        "        resultado = cursor.fetchone()\n",
        "        if resultado:\n",
        "            continue  # Se o registro já existir, pula para o próximo\n",
        "\n",
        "        # Obtém o ID da tabela dim_tempo\n",
        "        cursor.execute(\"SELECT MAX(id_tempo) FROM dim_tempo\")\n",
        "        max_id = cursor.fetchone()[0]\n",
        "        id_tempo = 1 if max_id is None else max_id + 1\n",
        "\n",
        "        # 1. Inserção na tabela dim_tempo (data, hora, dia da semana)\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_tempo (id_tempo, data, hora, dia_da_semana_entrada)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "        \"\"\", (id_tempo, row['data'], row['hora'], row['dia_da_semana_entrada']))\n",
        "\n",
        "        # 2. Inserção na tabela dim_indicadores (indicadores técnicos como SMA, EMA, etc.)\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_indicadores (id_indicadores, id_tempo, SMA_10, EMA_10, MACD, Signal_Line, rsi, OBV, retorno, volatilidade, fechamento_dia, volume_dia, maximo_dia, minimo_dia)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (\n",
        "            id_tempo, id_tempo, row['SMA_10'], row['EMA_10'], row['MACD'],\n",
        "            row['Signal_Line'], row['rsi'], row['OBV'], row['retorno'], row['volatilidade'],\n",
        "            row.get('fechamento_dia', None), row.get('volume_dia', None),\n",
        "            row.get('maximo_dia', None), row.get('minimo_dia', None)\n",
        "        ))\n",
        "\n",
        "        # 3. Inserção na tabela dim_lags (valores defasados de fechamento, retorno e volume)\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_lags (\n",
        "                id_lags, id_tempo,\n",
        "                fechamento_lag1, retorno_lag1, volume_lag1,\n",
        "                fechamento_lag2, retorno_lag2, volume_lag2,\n",
        "                fechamento_lag3, retorno_lag3, volume_lag3\n",
        "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (\n",
        "            id_tempo, id_tempo,\n",
        "            row['fechamento_lag1'], row['retorno_lag1'], row['volume_lag1'],\n",
        "            row['fechamento_lag2'], row['retorno_lag2'], row['volume_lag2'],\n",
        "            row['fechamento_lag3'], row['retorno_lag3'], row['volume_lag3']\n",
        "        ))\n",
        "\n",
        "        # 4. Inserção na tabela dim_operacional (informações operacionais como hora e minuto)\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_operacional (\n",
        "                id_operacional, id_tempo,\n",
        "                dia_da_semana_previsao, hora_num, minuto, mercado_aberto,\n",
        "                fechamento_dia_anterior, volume_dia_anterior, maximo_dia_anterior, minimo_dia_anterior\n",
        "            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (\n",
        "            id_tempo, id_tempo,\n",
        "            row.get('dia_da_semana_previsao', row['dia_da_semana_entrada']),\n",
        "            row['hora_num'], row['minuto'],\n",
        "            row.get('mercado_aberto', 1),\n",
        "            row.get('fechamento_dia_anterior', None), row.get('volume_dia_anterior', None),\n",
        "            row.get('maximo_dia_anterior', None), row.get('minimo_dia_anterior', None)\n",
        "        ))\n",
        "\n",
        "        # 5. Inserção na tabela fato_precos (preços e volume)\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO fato_precos (id_fato_precos, id_tempo, abertura, minimo, maximo, fechamento, fechamento_dia, volume_dia, maximo_dia, minimo_dia)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (\n",
        "            id_tempo, id_tempo, row['abertura'], row['minimo'], row['maximo'], row['fechamento'],\n",
        "            row.get('fechamento_dia', None), row.get('volume_dia', None),\n",
        "            row.get('maximo_dia', None), row.get('minimo_dia', None)\n",
        "        ))\n",
        "\n",
        "        registros_inseridos += 1  # Conta o número de registros inseridos\n",
        "\n",
        "    # Commit das alterações no banco e fechamento da conexão\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"Carga incremental concluída. {registros_inseridos} novos registros inseridos.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Caminho do banco de dados e arquivo CSV de dados transformados\n",
        "    db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional_vf.db\"\n",
        "    df_path = \"/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv\"\n",
        "\n",
        "    # Verifica se o arquivo CSV existe\n",
        "    if not os.path.exists(df_path):\n",
        "        raise FileNotFoundError(f\"O arquivo {df_path} não existe.\")\n",
        "\n",
        "    # Carrega o DataFrame do arquivo CSV\n",
        "    df = pd.read_csv(df_path)\n",
        "\n",
        "    # Verifica se o DataFrame está vazio\n",
        "    if df.empty:\n",
        "        raise ValueError(\"O DataFrame carregado está vazio.\")\n",
        "\n",
        "    # Chama a função para carregar os dados no banco\n",
        "    carregar_dados(df, db_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "3qMQHkxtrNrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Executar realizar a carga de dados\n",
        "!python /content/Piloto_Day_Trade/scripts/pipeline/carga_dados_vf.py\n"
      ],
      "metadata": {
        "id": "zidwW0A7DfMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Verificando a carga de dados\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "def verificar_carga_dados(db_path: str):\n",
        "    \"\"\"\n",
        "    Função para verificar a carga de dados nas tabelas do banco dimensional.\n",
        "\n",
        "    Parâmetros:\n",
        "    - Caminho para o banco de dados.\n",
        "\n",
        "    A função realiza uma consulta de contagem de registros em cada uma das principais tabelas\n",
        "    do banco e imprime o número de registros encontrados.\n",
        "    \"\"\"\n",
        "\n",
        "    # Conexão com o banco de dados\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Definir as tabelas principais a serem verificadas\n",
        "    tabelas = ['dim_tempo', 'dim_indicadores', 'dim_lags', 'dim_operacional', 'fato_precos']\n",
        "\n",
        "    # Laço para realizar a consulta de contagem de registros em cada tabela\n",
        "    for tabela in tabelas:\n",
        "        # Consulta SQL para contar o número de registros em cada tabela\n",
        "        cursor.execute(f\"SELECT COUNT(*) FROM {tabela}\")\n",
        "        count = cursor.fetchone()[0]  # Recupera o número de registros\n",
        "        print(f\"{tabela}: {count} registros\")  # Exibe o resultado\n",
        "\n",
        "    # Fechamento da conexão com o banco de dados\n",
        "    conn.close()\n",
        "\n",
        "# Caminho para o banco de dados\n",
        "db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional_vf.db\"\n",
        "\n",
        "# Chamada da função para verificar a carga de dados\n",
        "verificar_carga_dados(db_path)\n"
      ],
      "metadata": {
        "id": "ksO0iYGrEEXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar os primeiros 5 registros da fato_precos\n",
        "import pandas as pd\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "df_check = pd.read_sql_query(\"SELECT * FROM fato_precos LIMIT 5\", conn)\n",
        "print(df_check)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "v0OFgWADGalR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consulta com JOIN para verificar o relacionamento entre as tabelas\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    ft.id_fato_precos,\n",
        "    dt.data,\n",
        "    dt.hora,\n",
        "    ft.abertura,\n",
        "    ft.fechamento,\n",
        "    di.SMA_10,\n",
        "    dl.fechamento_lag1,\n",
        "    do.dia_da_semana_previsao,  -- Corrigido para usar dia_da_semana_previsao\n",
        "    do.mercado_aberto\n",
        "FROM fato_precos ft\n",
        "JOIN dim_tempo dt ON ft.id_tempo = dt.id_tempo\n",
        "JOIN dim_indicadores di ON ft.id_tempo = di.id_tempo\n",
        "JOIN dim_lags dl ON ft.id_tempo = dl.id_tempo\n",
        "JOIN dim_operacional do ON ft.id_tempo = do.id_tempo\n",
        "LIMIT 5;\n",
        "\"\"\"\n",
        "\n",
        "# Executando e exibindo\n",
        "conn = sqlite3.connect(db_path)\n",
        "df_verificacao = pd.read_sql_query(query, conn)\n",
        "print(df_verificacao)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "dPdhcpp464Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Ajustando a consulta para considerar os campos corretos\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    MAX(dt.data || ' ' || dt.hora) AS data_hora_mais_recente,\n",
        "    MAX(ft.id_fato_precos) AS id_fato_mais_recente,\n",
        "    MAX(do.dia_da_semana_previsao) AS dia_da_semana_previsao_mais_recente -- Corrigido para dia_da_semana_previsao\n",
        "FROM fato_precos ft\n",
        "JOIN dim_tempo dt ON ft.id_tempo = dt.id_tempo\n",
        "JOIN dim_indicadores di ON ft.id_tempo = di.id_tempo\n",
        "JOIN dim_lags dl ON ft.id_tempo = dl.id_tempo\n",
        "JOIN dim_operacional do ON ft.id_tempo = do.id_tempo;\n",
        "\"\"\"\n",
        "\n",
        "# Executando e exibindo\n",
        "conn = sqlite3.connect(db_path)\n",
        "df_max_data = pd.read_sql_query(query, conn)\n",
        "print(\"Datas mais recentes no banco:\")\n",
        "print(df_max_data)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "8qCrzosX5ZQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Escrevendo o Readme do projeto\n",
        "%%writefile /content/Piloto_Day_Trade/README.md\n",
        "\n",
        "# Objetivo do Projeto\n",
        "\n",
        "## 1. Propósito do MVP\n",
        "\n",
        "Este projeto tem como objetivo principal a criação de um pipeline para extração, transformação, carga, análise e previsão da movimentação intradiária dos preços de um ativo financeiro em intervalos de 5 minutos. O modelo preditivo central será baseado em redes neurais recorrentes (LSTM), mas outras abordagens serão exploradas. O MVP visa garantir previsões para embasar decisões estratégicas de day trade.\n",
        "\n",
        "## 2. Problema a Ser Resolvido\n",
        "\n",
        "A alta volatilidade dos mercados financeiros exige ferramentas robustas para antecipação de movimentos de preço. A dificuldade está em capturar padrões de curto prazo e projetá-los com precisão. Traders e investidores necessitam de um modelo que consiga interpretar os padrões históricos e transformá-los em previsões úteis.\n",
        "\n",
        "## 3. Pipeline do Projeto\n",
        "\n",
        "O pipeline está estruturado em sete etapas principais:\n",
        "\n",
        "### 3.1. Extração e armazenamento dos dados brutos\n",
        "- Coleta de dados históricos do ativo BBDC4 em intervalos de 5 minutos, via API do Yahoo Finance (yfinance).\n",
        "- Armazenamento dos dados no GitHub sincronizado com Google Colab, com backup em nuvem.\n",
        "- Salvo como `dados_brutos.csv`\n",
        "\n",
        "### 3.2. Limpeza e organização dos dados\n",
        "- Padronização dos tipos de dados\n",
        "- Padronização dos nomes de colunas\n",
        "- Remoção de valores nulos ou duplicados\n",
        "- Remoção de colunas desnecessárias\n",
        "- Ordenação cronológica\n",
        "- Salvo como `dados_limpos.csv`\n",
        "\n",
        "### 3.3. Transformação de dados e engenharia de features\n",
        "- Cálculo de indicadores técnicos (SMA, EMA, MACD, RSI, OBV)\n",
        "- Cálculo de retornos e variância (volatilidade)\n",
        "- Criação de variáveis de lag de preço, volume e retorno\n",
        "- Adição de variáveis temporais (hora, dia da semana, mercado aberto)\n",
        "- Salvo como `dados_transformados.csv`\n",
        "\n",
        "### 3.4. Modelagem e estruturação do banco dimensional\n",
        "- **Fato**: `fato_precos` com preços e chave para `dim_tempo`\n",
        "- **Dimensões**:\n",
        "  - `dim_tempo`: data, hora, dia da semana\n",
        "  - `dim_indicadores`: indicadores técnicos\n",
        "  - `dim_lags`: lags de preço, volume, retorno\n",
        "  - `dim_operacional`: hora, minuto, data da previsão, mercado aberto\n",
        "- Banco gerado em SQLite via script automatizado (`banco_dimensional.db`)\n",
        "\n",
        "### 3.5. Carga (ETL)\n",
        "- **Extração:** via API (automatizada)\n",
        "- **Limpeza:** padronização, remoção de nulos/duplicatas\n",
        "- **Transformação:** features técnicas e derivadas\n",
        "- **Carga:** população das tabelas do banco dimensional\n",
        "- ETL organizado em scripts Python e automatizado\n",
        "\n",
        "### 3.6. Treinamento e ajuste do modelo preditivo\n",
        "- **Preparação dos dados**:\n",
        "  - Padronização com StandardScaler para retornos e indicadores\n",
        "  - Normalização com MinMaxScaler para preços e volumes\n",
        "  - Separar features (X) e targets (y)\n",
        "  - Divisão treino/teste com base em dias útis\n",
        "- **Modelo base:** LSTM com duas camadas ocultas, camada densa e MSE como perda\n",
        "- **Avaliação:** Métricas de MSE, R², comparação com targets reais\n",
        "\n",
        "### 3.7. Análise dos resultados\n",
        "- Comparativo entre preços previstos vs. reais\n",
        "- Validação das previsões para abertura, máxima, mínima e fechamento\n",
        "- Importância das variáveis\n",
        "- Interpretação dos erros e possíveis melhorias\n",
        "\n",
        "## 4. Perguntas a Serem Respondidas\n",
        "- É possível prever com precisão a movimentação intradiária a cada 5 minutos?\n",
        "- Os dados do dia anterior são suficientes para prever o comportamento do dia seguinte?\n",
        "- O modelo LSTM é eficaz para padrões de curtíssimo prazo?\n",
        "- É viável derivar os targets globais do dia a partir das previsões intradiárias?\n",
        "- Quais indicadores mais contribuem para a previsão?\n",
        "- Como lidar corretamente com fins de semana e feriados?\n",
        "- A padronização/normalização das variáveis afeta o desempenho?\n",
        "\n",
        "## 5. Critérios de Sucesso\n",
        "- Pipeline funcional de extração → transformação → carga → previsão\n",
        "- Modelo com bom desempenho em MSE e R²\n",
        "- Targets globais coerentes com valores reais\n",
        "- Correta gestão de datas (incluindo segundas-feiras)\n",
        "- Previsões utilizáveis para tomada de decisão\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SCo7OFDwSG35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Escrevendo Licença do projeto\n",
        "%%writefile /content/Piloto_Day_Trade/LICENSE\n",
        "\n",
        "Copyright (c) 2025 Carolina Brescowitt\n",
        "\n",
        "Todos os direitos reservados.\n",
        "\n",
        "Este software é fornecido gratuitamente apenas para uso **pessoal, acadêmico e de pesquisa**.\n",
        "\n",
        "O uso comercial deste software é estritamente proibido sem uma **licença comercial paga**, a ser negociada com o autor.\n",
        "\n",
        "Empresas, startups, desenvolvedores ou qualquer entidade que deseje utilizar este código em produtos, serviços ou plataformas comerciais devem entrar em contato com o autor para **negociar os termos de licenciamento** (incluindo percentual, royalties ou valores fixos).\n",
        "\n",
        "É proibida a redistribuição ou sublicenciamento sem autorização por escrito.\n",
        "\n",
        "Para mais informações, entre em contato: carolbrescowitt@yahoo.com.br\n",
        "\n"
      ],
      "metadata": {
        "id": "guTRpOgpsxDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Atualizando a licença do projeto\")"
      ],
      "metadata": {
        "id": "FGw2RgsT7bIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelagem MAchine Learning"
      ],
      "metadata": {
        "id": "ai-nFsCPCE4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformado = pd.read_csv('/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv')\n",
        "df_transformado.head()"
      ],
      "metadata": {
        "id": "_xrWU1sFKiqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_transformado.columns.tolist())\n"
      ],
      "metadata": {
        "id": "mbqIbvssCB0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/preparar_dados_modelagem_LSTM_global.py\n",
        "\n",
        "# @title Preparação dos dados para o modelo LSTM Global\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preparar_dados_lstm_global(df, janela=16, test_size=0.2, val_size=0.1):\n",
        "    \"\"\"\n",
        "    Prepara os dados para um modelo LSTM prever os alvos globais do próximo dia (mínimo, máximo, fechamento, volume),\n",
        "    usando sequências de candles intradiários (ex: 5 em 5 minutos) do(s) dia(s) anterior(es).\n",
        "    \"\"\"\n",
        "\n",
        "    # Ordenar temporalmente\n",
        "    df = df.sort_values(['data', 'hora']).reset_index(drop=True)\n",
        "\n",
        "    # Definir colunas\n",
        "    features = [\n",
        "        'abertura', 'minimo', 'maximo', 'fechamento', 'volume',\n",
        "        'SMA_10', 'EMA_10', 'OBV', 'retorno', 'volatilidade',\n",
        "        'MACD', 'Signal_Line', 'rsi', 'ADX', 'BB_MA20', 'BB_STD20',\n",
        "        'BB_upper', 'BB_lower', '%K', '%D', 'CCI', 'ATR',\n",
        "        'fechamento_lag1', 'retorno_lag1', 'volume_lag1',\n",
        "        'fechamento_lag2', 'retorno_lag2', 'volume_lag2',\n",
        "        'fechamento_lag3', 'retorno_lag3', 'volume_lag3',\n",
        "        'hora_num', 'minuto'\n",
        "    ]\n",
        "    targets = ['minimo_dia', 'maximo_dia', 'fechamento_dia', 'volume_dia']\n",
        "\n",
        "    # Normalização\n",
        "    scaler_features = MinMaxScaler()\n",
        "    scaler_targets = MinMaxScaler()\n",
        "    X_scaled = scaler_features.fit_transform(df[features])\n",
        "    y_scaled = scaler_targets.fit_transform(df[targets])\n",
        "\n",
        "    # Construir DataFrames auxiliares\n",
        "    df_seq = pd.DataFrame(X_scaled, columns=features)\n",
        "    df_seq['data'] = df['data'].values\n",
        "    df_targets = pd.DataFrame(y_scaled, columns=targets)\n",
        "    df_targets['data'] = df['data'].values\n",
        "\n",
        "    # Geração de sequências por dia\n",
        "    X, y = [], []\n",
        "    dias_unicos = df_seq['data'].unique()\n",
        "    for i in range(len(dias_unicos) - 1):\n",
        "        dia = dias_unicos[i]\n",
        "        proximo = dias_unicos[i + 1]\n",
        "\n",
        "        dados_dia = df_seq[df_seq['data'] == dia].drop(columns='data')\n",
        "        alvo_proximo = df_targets[df_targets['data'] == proximo][targets]\n",
        "\n",
        "        if len(dados_dia) >= janela and not alvo_proximo.empty:\n",
        "            X.append(dados_dia.iloc[-janela:].values)\n",
        "            y.append(alvo_proximo.iloc[0].values)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Salvar dados completos antes do split\n",
        "    np.savez_compressed(\n",
        "        '/content/Piloto_Day_Trade/models/LSTM/lstm_global_dataset_completo.npz',\n",
        "        X=X, y=y\n",
        "    )\n",
        "\n",
        "    # Split sem embaralhar\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, shuffle=False)\n",
        "\n",
        "    # Salvar splits\n",
        "    np.savez_compressed(\n",
        "        '/content/Piloto_Day_Trade/models/LSTM/lstm_global_datasets.npz',\n",
        "        X_train=X_train, X_val=X_val, X_test=X_test,\n",
        "        y_train=y_train, y_val=y_val, y_test=y_test\n",
        "    )\n",
        "\n",
        "    # Salvar scalers\n",
        "    joblib.dump(scaler_features, '/content/Piloto_Day_Trade/models/LSTM/scaler_features_lstm_global.save')\n",
        "    joblib.dump(scaler_targets, '/content/Piloto_Day_Trade/models/LSTM/scaler_targets_lstm_global.save')\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, {\n",
        "        'scaler_features': scaler_features,\n",
        "        'scaler_targets': scaler_targets\n",
        "    }\n",
        "\n",
        "# Bloco de teste local (não roda se importado)\n",
        "if __name__ == \"__main__\":\n",
        "    df_transformado = pd.read_csv('/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv')\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, scalers = preparar_dados_lstm_global(df_transformado)\n",
        "\n",
        "    print(\"Pré-visualização de uma sequência de entrada normalizada:\")\n",
        "    print(X_train[0])\n",
        "\n",
        "    print(\"\\nTarget correspondente (normalizado):\")\n",
        "    print(y_train[0])\n"
      ],
      "metadata": {
        "id": "UAnYe7b4DsKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/preparar_dados_modelagem_LSTM_intradiario.py\n",
        "\n",
        "# @title Preparação dos dados para o modelo LSTM Intradiário\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import joblib  # Para salvar os scalers\n",
        "\n",
        "def preparar_dados_lstm_intradiario(\n",
        "    df: pd.DataFrame,                    # DataFrame com os dados históricos intradiários\n",
        "    colunas_features: list,              # Lista com nomes das colunas usadas como entrada (X)\n",
        "    colunas_targets: list,               # Lista com nomes das colunas de saída (y)\n",
        "    dias_entrada: int = 3,               # Quantos dias anteriores usar como entrada\n",
        "    n_pontos_dia: int = None,            # Número de candles por dia (detectado automaticamente se não informado)\n",
        "    verbose: bool = True                 # Imprimir ou não os logs de progresso\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepara os dados normalizados e sequenciais para treinar LSTM em séries temporais intradiárias.\n",
        "\n",
        "    Retorna:\n",
        "        X: np.ndarray, formato (amostras, timesteps, features)\n",
        "        y: np.ndarray, formato (amostras, timesteps, targets)\n",
        "        datas_validas: list, datas correspondentes a cada amostra\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Verifica se a coluna 'data' está presente\n",
        "    if \"data\" not in df.columns:\n",
        "        raise ValueError(\"A coluna 'data' é obrigatória.\")\n",
        "\n",
        "    # Verifica se todas as colunas necessárias estão no DataFrame\n",
        "    for col in colunas_features + colunas_targets:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Coluna '{col}' ausente no DataFrame.\")\n",
        "\n",
        "    # Converte 'data' para string (evita bugs de agrupamento por datetime)\n",
        "    df[\"data\"] = df[\"data\"].astype(str)\n",
        "\n",
        "    # Conta quantos candles existem por dia e determina o valor mais comum\n",
        "    contagem_por_dia = df.groupby(\"data\").size()\n",
        "    valor_mais_comum = Counter(contagem_por_dia).most_common(1)[0][0]\n",
        "    n_pontos_dia = n_pontos_dia or valor_mais_comum\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[Info] Detectado {n_pontos_dia} candles por dia.\")\n",
        "\n",
        "    # Inicializa os normalizadores\n",
        "    scaler_features = MinMaxScaler()\n",
        "    scaler_targets = MinMaxScaler()\n",
        "\n",
        "    # Aplica normalização\n",
        "    df[colunas_features] = scaler_features.fit_transform(df[colunas_features])\n",
        "    df[colunas_targets] = scaler_targets.fit_transform(df[colunas_targets])\n",
        "\n",
        "    # Salva os dados normalizados em CSV\n",
        "    path_norm = '/content/Piloto_Day_Trade/data/prepared/dados_normalizados.csv'\n",
        "    os.makedirs(os.path.dirname(path_norm), exist_ok=True)\n",
        "    df.to_csv(path_norm, index=False)\n",
        "    print(df.head())\n",
        "    if verbose:\n",
        "        print(f\"[Info] Dados normalizados salvos em: {path_norm}\")\n",
        "\n",
        "    # Salva os scalers para reutilizar depois (ex: prever novos dados ou fazer inverse_transform)\n",
        "    scaler_path = '/content/Piloto_Day_Trade/models/LSTM'\n",
        "    os.makedirs(scaler_path, exist_ok=True)\n",
        "    joblib.dump(scaler_features, os.path.join(scaler_path, 'scaler_features.pkl'))\n",
        "    joblib.dump(scaler_targets, os.path.join(scaler_path, 'scaler_targets.pkl'))\n",
        "    if verbose:\n",
        "        print(f\"[Info] Scalers salvos em: {scaler_path}\")\n",
        "\n",
        "    # Gera as sequências de entrada (X) e saída (y) para cada dia válido\n",
        "    dias_unicos = sorted(df[\"data\"].unique())\n",
        "    X, y, datas_validas = [], [], []\n",
        "\n",
        "    for i in range(dias_entrada, len(dias_unicos)):\n",
        "        dias_passados = dias_unicos[i - dias_entrada:i]  # Dias usados como entrada\n",
        "        dia_target = dias_unicos[i]                      # Dia de saída (alvo)\n",
        "\n",
        "        entradas = []\n",
        "        sequencia_valida = True\n",
        "\n",
        "        # Junta os dados dos dias anteriores\n",
        "        for dia in dias_passados:\n",
        "            dados_dia = df[df[\"data\"] == dia][colunas_features].values\n",
        "            if len(dados_dia) != n_pontos_dia:\n",
        "                sequencia_valida = False\n",
        "                break\n",
        "            entradas.append(dados_dia)\n",
        "\n",
        "        # Dados de saída (target)\n",
        "        saida = df[df[\"data\"] == dia_target][colunas_targets].values\n",
        "        if len(saida) != n_pontos_dia:\n",
        "            sequencia_valida = False\n",
        "\n",
        "        # Se tudo certo, adiciona à lista final\n",
        "        if sequencia_valida:\n",
        "            X.append(np.vstack(entradas))\n",
        "            y.append(saida)\n",
        "            datas_validas.append(dia_target)\n",
        "        elif verbose:\n",
        "            print(f\"[Aviso] Ignorado: {dia_target} com sequência incompleta.\")\n",
        "\n",
        "    # Converte listas em arrays numpy\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[Info] X shape: {X.shape}, y shape: {y.shape}, Amostras válidas: {len(datas_validas)}\")\n",
        "        print(\"[Info] Preparação finalizada com sucesso.\")\n",
        "\n",
        "    # Salva os arrays como .npy para uso futuro\n",
        "    np.save(os.path.join(scaler_path, 'X.npy'), X)\n",
        "    np.save(os.path.join(scaler_path, 'y.npy'), y)\n",
        "\n",
        "    return X, y, datas_validas\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import joblib\n",
        "\n",
        "    # Caminho dos arquivos\n",
        "    caminho_dados = '/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv'\n",
        "    caminho_scaler_features = '/content/Piloto_Day_Trade/models/LSTM/scaler_features.pkl'\n",
        "    caminho_scaler_targets = '/content/Piloto_Day_Trade/models/LSTM/scaler_targets.pkl'\n",
        "\n",
        "    # Lê o dataset transformado\n",
        "    df_transformado = pd.read_csv(caminho_dados)\n",
        "\n",
        "    # Define colunas de entrada (X) e saída (y)\n",
        "    colunas_features = [\n",
        "        'abertura', 'minimo', 'maximo', 'fechamento', 'volume',\n",
        "        'SMA_10', 'EMA_10', 'OBV', 'retorno', 'volatilidade',\n",
        "        'MACD', 'Signal_Line', 'rsi', 'ADX', 'BB_MA20', 'BB_STD20',\n",
        "        'BB_upper', 'BB_lower', '%K', '%D', 'CCI', 'ATR'\n",
        "    ]\n",
        "    colunas_targets = ['minimo_dia', 'maximo_dia', 'fechamento_dia', 'volume_dia']\n",
        "\n",
        "    # Executa o pipeline de preparação\n",
        "    X, y, datas = preparar_dados_lstm_intradiario(\n",
        "        df=df_transformado,\n",
        "        colunas_features=colunas_features,\n",
        "        colunas_targets=colunas_targets,\n",
        "        dias_entrada=3,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Resumo Final]\")\n",
        "    print(f\"X shape: {X.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "    print(f\"Amostras válidas: {len(datas)}\")\n",
        "    print(f\"Primeira data válida: {datas[0] if datas else 'Nenhuma'}\")\n",
        "\n",
        "    # Testa carregamento dos scalers\n",
        "    scaler_features = joblib.load(caminho_scaler_features)\n",
        "    scaler_targets = joblib.load(caminho_scaler_targets)\n",
        "    print(\"\\n[Info] Scalers carregados com sucesso.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "J7iizk3fNipg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Executar preparar dados para o modelo LSTM intradiario\n",
        "import sys\n",
        "sys.path.append('/content/Piloto_Day_Trade/scripts/modelagem_machine_learning')\n",
        "\n",
        "from preparar_dados_modelagem_LSTM_intradiario import preparar_dados_lstm_intradiario\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv')\n",
        "\n",
        "colunas_features = [\n",
        "    'abertura', 'minimo', 'maximo', 'fechamento', 'volume',\n",
        "    'SMA_10', 'EMA_10', 'OBV', 'retorno', 'volatilidade',\n",
        "    'MACD', 'Signal_Line', 'rsi', 'ADX', 'BB_MA20', 'BB_STD20',\n",
        "    'BB_upper', 'BB_lower', '%K', '%D', 'CCI', 'ATR'\n",
        "]\n",
        "\n",
        "colunas_targets = ['minimo_dia', 'maximo_dia', 'fechamento_dia', 'volume_dia']\n",
        "\n",
        "X, y, datas = preparar_dados_lstm_intradiario(\n",
        "    df=df,\n",
        "    colunas_features=colunas_features,\n",
        "    colunas_targets=colunas_targets\n",
        ")\n"
      ],
      "metadata": {
        "id": "0m05llHqceuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/validar_dados_preparados.py\n",
        "\n",
        "#@title Validação dos dados preparados\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def validar_dados(path_arquivo):\n",
        "    \"\"\"Valida integridade e consistência dos dados preparados para modelagem.\"\"\"\n",
        "\n",
        "    assert os.path.exists(path_arquivo), f\"[ERRO] Arquivo não encontrado: {path_arquivo}\"\n",
        "    df = pd.read_csv(path_arquivo)\n",
        "\n",
        "    print(\"[INFO] Arquivo carregado com sucesso.\")\n",
        "    print(f\"[INFO] Shape: {df.shape}\")\n",
        "    print(\"\\n[INFO] Colunas disponíveis:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Verifica valores nulos e infinitos\n",
        "    total_nulos = df.isnull().sum().sum()\n",
        "    total_infinitos = np.isinf(df.values).sum()\n",
        "    assert total_nulos == 0, f\"[ERRO] Dados contêm {total_nulos} valores nulos.\"\n",
        "    assert total_infinitos == 0, f\"[ERRO] Dados contêm {total_infinitos} valores infinitos.\"\n",
        "    print(\"[OK] Sem valores nulos ou infinitos.\")\n",
        "\n",
        "    # Estatísticas descritivas\n",
        "    print(\"\\n[INFO] Estatísticas descritivas (valores arredondados):\")\n",
        "    print(df.describe().T[['mean', 'std', 'min', 'max']].round(4))\n",
        "\n",
        "    # Identificação de colunas normalizadas (0 a 1)\n",
        "    normalizadas = [col for col in df.columns if df[col].min() >= 0.0 and df[col].max() <= 1.0]\n",
        "\n",
        "    # Identificação de colunas padronizadas (média ≈ 0, std ≈ 1)\n",
        "    padronizadas = []\n",
        "    for col in df.columns:\n",
        "        if col not in normalizadas:\n",
        "            media = df[col].mean()\n",
        "            desvio = df[col].std()\n",
        "            if abs(media) < 0.1 and abs(desvio - 1.0) < 0.1:\n",
        "                padronizadas.append(col)\n",
        "\n",
        "    outras = [col for col in df.columns if col not in normalizadas + padronizadas]\n",
        "\n",
        "    print(f\"\\n[INFO] {len(normalizadas)} colunas normalizadas (0 a 1): {normalizadas}\")\n",
        "    print(f\"[INFO] {len(padronizadas)} colunas padronizadas (média ~0, std ~1): {padronizadas}\")\n",
        "    if outras:\n",
        "        print(f\"[ALERTA] {len(outras)} colunas fora dos padrões esperados: {outras}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    caminho = '/content/Piloto_Day_Trade/data/transformed/dados_preparados_para_modelagem.csv'\n",
        "    validar_dados(caminho)\n"
      ],
      "metadata": {
        "id": "vltY2yMrFmmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# Adiciona o caminho raiz ao sys.path\n",
        "sys.path.append('/content')\n",
        "\n",
        "from Piloto_Day_Trade.scripts.modelagem_machine_learning.validar_dados_preparados import validar_dados\n",
        "\n",
        "# Chamando a função\n",
        "caminho = '/content/Piloto_Day_Trade/data/transformed/dados_preparados_para_modelagem.csv'\n",
        "validar_dados(caminho)\n"
      ],
      "metadata": {
        "id": "7DMYXPKI2Kap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/criar_modelo_LSTM.py\n",
        "\n",
        "# @title Definindo Script para criar o modelo LSTM Encoder-Decoder\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Dropout\n",
        "\n",
        "def LSTM_seq2seq(input_timesteps, input_features, output_timesteps):\n",
        "    \"\"\"\n",
        "    Cria um modelo LSTM do tipo encoder-decoder:\n",
        "    - Entrada: sequência dos dias anteriores (3 dias de 5 em 5 min)\n",
        "    - Saída: sequência do próximo dia (também em 5 em 5 min)\n",
        "\n",
        "    Args:\n",
        "        input_timesteps: total de timesteps da entrada (ex: 3 dias * 32 = 96)\n",
        "        input_features: número de features por timestep\n",
        "        output_timesteps: total de timesteps da saída (ex: 32 para 1 dia)\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras.Model): modelo compilado\n",
        "    \"\"\"\n",
        "    # ENCODER\n",
        "    encoder_inputs = Input(shape=(input_timesteps, input_features))\n",
        "    encoder_lstm = LSTM(64, return_state=True, dropout=0.2)\n",
        "    _, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # DECODER\n",
        "    decoder_inputs = RepeatVector(output_timesteps)(state_h)  # Repete o estado oculto para cada passo de saída\n",
        "    decoder_lstm = LSTM(64, return_sequences=True, dropout=0.2)\n",
        "    decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "    # Saída com 4 valores por timestep (abertura, máximo, mínimo, fechamento)\n",
        "    decoder_dense = TimeDistributed(Dense(4))\n",
        "    outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Modelo final\n",
        "    model = Model(encoder_inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "T6lFmK-FMncY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Treinamento do modelo LSTM v1\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from scripts.modelagem_machine_learning.criar_modelo_LSTM import LSTM_seq2seq\n",
        "from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "\n",
        "# Parâmetros do modelo\n",
        "input_timesteps = 96         # 3 dias úteis * 32 janelas de 15 min\n",
        "output_timesteps = 32        # 1 dia útil = 32 janelas de 15 min\n",
        "input_features = 25          # Número de features por timestep (ajuste conforme seu dataset)\n",
        "batch_size = 16\n",
        "epochs = 20\n",
        "patience = 5\n",
        "\n",
        "# Carregar e preparar os dados (substitua pelos parâmetros corretos se necessário)\n",
        "# Ajuste o caminho e os parâmetros conforme sua estrutura de dados\n",
        "data_path = '/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv'\n",
        "\n",
        "# Chama a função de preparação dos dados e retorna os dados preparados para treinamento e teste\n",
        "X_treino, X_teste, y_treino, y_teste, scaler_X, scaler_y = preparar_dados_lstm(data_path, input_timesteps, output_timesteps, input_features)\n",
        "\n",
        "# Criação do modelo LSTM v1\n",
        "model = LSTM_seq2seq(input_timesteps, input_features, output_timesteps)\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_path = '/content/Piloto_Day_Trade/modelos/modelo_LSTM_v1.h5'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='loss', save_best_only=True, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True)\n",
        "\n",
        "# Treinamento\n",
        "model.fit(\n",
        "    X_treino,\n",
        "    y_treino,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_teste, y_teste),  # Adicionando validação durante o treinamento\n",
        "    callbacks=[early_stop, checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Mensagem de sucesso\n",
        "print(f\"Modelo LSTM v1 treinado e salvo em: {checkpoint_path}\")\n"
      ],
      "metadata": {
        "id": "Mdegdkw99ee1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Incluindo Modelo LSTM\")"
      ],
      "metadata": {
        "id": "DiGwgsI6p9IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calcular métricas e avaliar modelo LSTM\n",
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/calcular_metricas_avaliar_modelo_LSTM.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def avaliar_modelo_lstm(modelo, X_teste, y_teste, caminho_scaler='/content/Piloto_Day_Trade/models/LSTM/scalers/scaler_normalizacao_preco.pkl'):\n",
        "    \"\"\"\n",
        "    Avalia o modelo LSTM, imprimindo as principais métricas e comparação entre previsões e valores reais.\n",
        "    \"\"\"\n",
        "    print(\"Realizando previsões...\")\n",
        "    y_previsto = modelo.predict(X_teste)\n",
        "\n",
        "    print(\"Carregando scaler de preços para inversão...\")\n",
        "    scaler_precos = joblib.load(caminho_scaler)\n",
        "    colunas_precos = ['abertura', 'maximo', 'minimo', 'fechamento']\n",
        "\n",
        "    y_previsto_reshape = y_previsto.reshape(-1, 4)\n",
        "    y_teste_reshape = y_teste.reshape(-1, 4)\n",
        "\n",
        "    y_previsto_original = scaler_precos.inverse_transform(y_previsto_reshape)\n",
        "    y_teste_original = scaler_precos.inverse_transform(y_teste_reshape)\n",
        "\n",
        "    df_previsto = pd.DataFrame(y_previsto_original, columns=colunas_precos)\n",
        "    df_real = pd.DataFrame(y_teste_original, columns=colunas_precos)\n",
        "\n",
        "    comparacao = pd.DataFrame({\n",
        "        'Abertura_Real': df_real['abertura'],\n",
        "        'Abertura_Prevista': df_previsto['abertura'],\n",
        "        'Maximo_Real': df_real['maximo'],\n",
        "        'Maximo_Previsto': df_previsto['maximo'],\n",
        "        'Minimo_Real': df_real['minimo'],\n",
        "        'Minimo_Previsto': df_previsto['minimo'],\n",
        "        'Fechamento_Real': df_real['fechamento'],\n",
        "        'Fechamento_Previsto': df_previsto['fechamento']\n",
        "    })\n",
        "\n",
        "    print(\"\\nComparação de previsões (valores reais):\")\n",
        "    print(comparacao.head(10))\n",
        "\n",
        "    def calcular_metricas(y_real, y_previsto, nome):\n",
        "        mae = mean_absolute_error(y_real, y_previsto)\n",
        "        mse = mean_squared_error(y_real, y_previsto)\n",
        "        r2 = r2_score(y_real, y_previsto)\n",
        "        print(f\"{nome} - MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "    print(\"\\n Métricas de desempenho por coluna:\")\n",
        "    calcular_metricas(df_real['abertura'], df_previsto['abertura'], \"Abertura\")\n",
        "    calcular_metricas(df_real['maximo'], df_previsto['maximo'], \"Máximo\")\n",
        "    calcular_metricas(df_real['minimo'], df_previsto['minimo'], \"Mínimo\")\n",
        "    calcular_metricas(df_real['fechamento'], df_previsto['fechamento'], \"Fechamento\")\n",
        "\n",
        "    return df_real, df_previsto, comparacao\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Importando scripts de modelo e dados...\")\n",
        "    from tensorflow.keras.models import load_model\n",
        "    from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "\n",
        "    print(\"Preparando dados para avaliação...\")\n",
        "    X_treino, X_teste, y_treino, y_teste = preparar_dados_lstm(\n",
        "        path_dados='/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv',\n",
        "        tam_seq=96,\n",
        "        tx_treino=0.8\n",
        "    )\n",
        "\n",
        "    print(\"📡 Carregando modelo salvo...\")\n",
        "    modelo_lstm_v1 = load_model('/content/Piloto_Day_Trade/models/LSTM/modelo_LSTM_v1.keras')\n",
        "\n",
        "    print(\"Avaliando modelo...\")\n",
        "    df_real, df_previsto, comparacao = avaliar_modelo_lstm(\n",
        "        modelo=modelo_lstm_v1,\n",
        "        X_teste=X_teste,\n",
        "        y_teste=y_teste\n",
        "    )\n"
      ],
      "metadata": {
        "id": "2FOx4USVH-3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Avaliar o modelo LSTM v1 treinado\n",
        "from scripts.modelagem_machine_learning.calcular_metricas_avaliar_modelo_LSTM import avaliar_modelo_lstm\n",
        "from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Preparar os dados\n",
        "X_treino, X_teste, y_treino, y_teste = preparar_dados_lstm(\n",
        "    path_dados='/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv',\n",
        "    tam_seq=96,\n",
        "    tx_treino=0.8\n",
        ")\n",
        "\n",
        "# Carregar o modelo salvo\n",
        "modelo_lstm_v1 = load_model('/content/Piloto_Day_Trade/models/LSTM/modelo_LSTM_v1.keras')\n",
        "\n",
        "# Avaliar o modelo\n",
        "df_real, df_previsto, comparacao = avaliar_modelo_lstm(\n",
        "    modelo=modelo_lstm_v1,\n",
        "    X_teste=X_teste,\n",
        "    y_teste=y_teste,\n",
        "    caminho_scaler='/content/Piloto_Day_Trade/models/LSTM/scalers/scaler_normalizacao_preco.pkl'\n",
        ")\n"
      ],
      "metadata": {
        "id": "KOeVxRPDI-hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Finalizando Modelo LSTM\")"
      ],
      "metadata": {
        "id": "1YGdD_mkKPCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline de dados"
      ],
      "metadata": {
        "id": "TTzGcdW3uxIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def listar_arquivos_em_subpastas(pasta_base):\n",
        "    for raiz, subpastas, arquivos in os.walk(pasta_base):\n",
        "        nivel = raiz.replace(pasta_base, '').count(os.sep)\n",
        "        indent = ' ' * 4 * nivel\n",
        "        print(f\"{indent}{os.path.basename(raiz)}/\")\n",
        "        subindent = ' ' * 4 * (nivel + 1)\n",
        "        for arquivo in arquivos:\n",
        "            print(f\"{subindent}{arquivo}\")\n",
        "\n",
        "# Caminho da pasta onde estão os scripts\n",
        "pasta_scripts = \"/content/Piloto_Day_Trade/scripts\"\n",
        "listar_arquivos_em_subpastas(pasta_scripts)\n"
      ],
      "metadata": {
        "id": "v5_tfm2NxWQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/executar_pipeline.py\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from scripts.pipeline.extracao_dados import extrair_dados\n",
        "from scripts.pipeline.limpeza_dados import limpeza_dados\n",
        "from scripts.pipeline.transformacao_dados import transformar_dados\n",
        "from scripts.pipeline.carga_dados import carregar_dados\n",
        "from scripts.pipeline.criar_banco_dimensional import criar_banco\n",
        "from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "\n",
        "def executar_pipeline(ticker, intervalo, dias, caminho_bruto, caminho_limpo, caminho_transformado, db_path, tam_seq, tx_treino):\n",
        "    print(\"\\nIniciando execução completa do pipeline...\")\n",
        "\n",
        "    # Etapa 1: Extração\n",
        "    print(\"Executando: Extração de dados\")\n",
        "    extrair_dados(ticker, dias, intervalo, caminho_bruto)\n",
        "\n",
        "    # Etapa 2: Limpeza\n",
        "    print(\"Executando: Limpeza de dados\")\n",
        "    df_bruto = pd.read_csv(caminho_bruto, index_col=0, parse_dates=True, dayfirst=True)\n",
        "    limpeza_dados(df_bruto, caminho_limpo)\n",
        "\n",
        "    # Etapa 3: Transformação\n",
        "    print(\"Executando: Transformação de dados\")\n",
        "    transformar_dados(caminho_limpo, caminho_transformado)\n",
        "\n",
        "    # Etapa 4: Criar banco dimensional\n",
        "    print(\"Executando: Criação do banco dimensional\")\n",
        "    criar_banco(db_path)  # Passando db_path para a função\n",
        "\n",
        "    # Etapa 5: Carga de dados\n",
        "    print(\"Executando: Carga de dados\")\n",
        "    df_transformado = pd.read_csv(caminho_transformado)\n",
        "    carregar_dados(df_transformado, db_path)\n",
        "\n",
        "    # Etapa 7: Preparação dos dados para modelagem\n",
        "    print(\"Executando: Preparação de dados para LSTM\")\n",
        "    preparar_dados_lstm(\n",
        "        path_dados=caminho_transformado,\n",
        "        tam_seq=tam_seq,\n",
        "        tx_treino=tx_treino\n",
        "    )\n",
        "\n",
        "    print(\"\\nPipeline finalizado com sucesso.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Chamada com parâmetros do projeto\n",
        "    ticker = \"BBDC4.SA\"\n",
        "    intervalo = \"5m\"\n",
        "    dias = 45\n",
        "    caminho_bruto = \"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\"\n",
        "    caminho_limpo = \"/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv\"\n",
        "    caminho_transformado = \"/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv\"\n",
        "    db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional.db\"\n",
        "    tam_seq = 96\n",
        "    tx_treino = 0.8\n",
        "\n",
        "    executar_pipeline(ticker, intervalo, dias, caminho_bruto, caminho_limpo, caminho_transformado, db_path, tam_seq, tx_treino)\n"
      ],
      "metadata": {
        "id": "9tHLaJNGMGuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Testar pipeline\n",
        "\n",
        "# Importando a função do pipeline\n",
        "from Piloto_Day_Trade.scripts.pipeline.executar_pipeline import executar_pipeline\n",
        "\n",
        "# Definindo os parâmetros a serem passados\n",
        "ticker = \"BBDC4.SA\"\n",
        "intervalo = \"5m\"\n",
        "dias = 45\n",
        "caminho_bruto = \"/content/Piloto_Day_Trade/data/raw/dados_brutos_teste.csv\"\n",
        "caminho_limpo = \"/content/Piloto_Day_Trade/data/cleaned/dados_limpos_teste.csv\"\n",
        "caminho_transformado = \"/content/Piloto_Day_Trade/data/transformed/dados_transformados_teste.csv\"\n",
        "db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional_teste.db\"\n",
        "tam_seq = 96\n",
        "tx_treino = 0.8\n",
        "\n",
        "# Executando o pipeline com os parâmetros definidos\n",
        "executar_pipeline(ticker, intervalo, dias, caminho_bruto, caminho_limpo, caminho_transformado, db_path, tam_seq, tx_treino)\n"
      ],
      "metadata": {
        "id": "7rxNatUoW_7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Caminhos dos arquivos\n",
        "caminhos = [\n",
        "    \"/content/Piloto_Day_Trade/data/raw/dados_brutos_teste.csv\",\n",
        "    \"/content/Piloto_Day_Trade/data/cleaned/dados_limpos_teste.csv\",\n",
        "    \"/content/Piloto_Day_Trade/data/transformed/dados_transformados_teste.csv\",\n",
        "    \"/content/Piloto_Day_Trade/data/raw/dados_brutos1304.csv\",\n",
        "    \"/content/Piloto_Day_Trade/data/raw/dados_brutostemp.csv\",\n",
        "    \"/content/Piloto_Day_Trade/data/raw/dados_brutostemp2.csv\"\n",
        "]\n",
        "\n",
        "# Função para deletar arquivos se existirem\n",
        "def deletar_arquivos(lista_caminhos):\n",
        "    for caminho in lista_caminhos:\n",
        "        if os.path.exists(caminho):\n",
        "            os.remove(caminho)\n",
        "            print(f\"Arquivo deletado: {caminho}\")\n",
        "\n",
        "# Execução direta\n",
        "if __name__ == \"__main__\":\n",
        "    deletar_arquivos(caminhos)\n"
      ],
      "metadata": {
        "id": "27WcDvWgwk0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/.github/workflows/pipeline.yml\n",
        "\n",
        "name: Executar Pipeline Completo\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [main]\n",
        "  pull_request:\n",
        "    branches: [main]\n",
        "\n",
        "jobs:\n",
        "  pipeline:\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    steps:\n",
        "      - name: Clonar repositório\n",
        "        uses: actions/checkout@v3\n",
        "\n",
        "      - name: Configurar Python\n",
        "        uses: actions/setup-python@v4\n",
        "        with:\n",
        "          python-version: '3.11'\n",
        "\n",
        "      - name: Instalar dependências\n",
        "        run: |\n",
        "          python -m pip install --upgrade pip\n",
        "          pip install -r requirements.txt\n",
        "\n",
        "      - name: Executar pipeline\n",
        "        run: |\n",
        "          python scripts/pipeline/executar_pipeline_completo.py\n"
      ],
      "metadata": {
        "id": "j7_m_o8cXdlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/.github/workflows/workflow_doc.md\n",
        "\n",
        "#@title Definindo Documentação do workflow completo para Execução do Pipeline\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Este documento descreve o fluxo de trabalho para a execução automatizada do pipeline de dados, utilizando o GitHub Actions. O pipeline abrange desde a extração até a modelagem de dados, e foi estruturado para garantir a automação da execução do processo, facilitando atualizações contínuas e execuções programadas no repositório GitHub.\n",
        "\n",
        "## Estrutura do Pipeline\n",
        "\n",
        "O pipeline é composto por várias etapas que são executadas em sequência. Cada uma das etapas envolve um script específico para garantir a correta manipulação dos dados:\n",
        "\n",
        "1. **Extração de Dados:** Obtém os dados brutos de uma fonte externa, como o Yahoo Finance ou qualquer outra fonte configurada.\n",
        "2. **Limpeza de Dados:** Realiza a limpeza e formatação dos dados brutos.\n",
        "3. **Transformação de Dados:** Aplica transformações necessárias para deixar os dados prontos para a modelagem.\n",
        "4. **Criação de Banco Dimensional:** Estrutura os dados de maneira que sejam facilmente analisáveis.\n",
        "5. **Carga de Dados:** Carrega os dados transformados para o banco de dados.\n",
        "6. **Geração de Catálogo de Dados:** Cria um catálogo de metadados para facilitar o uso futuro dos dados.\n",
        "7. **Preparação de Dados para Modelagem LSTM:** Prepara os dados específicos para alimentar o modelo de LSTM.\n",
        "8. **Modelagem e Avaliação:** Treina e avalia o modelo LSTM.\n",
        "\n",
        "### Scripts Responsáveis por Cada Etapa\n",
        "\n",
        "1. **`extracao_dados.py`:** Extração dos dados brutos.\n",
        "2. **`limpeza_dados.py`:** Limpeza e pré-processamento dos dados brutos.\n",
        "3. **`transformacao_dados.py`:** Aplicação das transformações necessárias nos dados.\n",
        "4. **`criar_banco_dimensional.py`:** Criação do banco dimensional para armazenar os dados.\n",
        "5. **`carga_dados.py`:** Carga dos dados transformados no banco dimensional.\n",
        "6. **`gerar_catalogo_dados.py`:** Geração do catálogo de dados.\n",
        "7. **`preparar_dados_modelagem_LSTM.py`:** Preparação final dos dados para treinamento do modelo LSTM.\n",
        "\n",
        "## GitHub Actions: Workflow\n",
        "\n",
        "O objetivo é configurar um workflow automatizado no GitHub Actions para que ele execute todo o pipeline a cada novo push ou evento programado. Para isso, criamos um arquivo YAML no repositório do GitHub.\n",
        "\n",
        "### Workflow: `pipeline.yml`\n",
        "\n",
        "Este workflow será responsável por orquestrar todas as etapas de execução. Abaixo está o conteúdo do arquivo YAML:\n",
        "\n",
        "```yaml\n",
        "name: Pipeline Completo de Dados\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches:\n",
        "      - main\n",
        "  schedule:\n",
        "    - cron: '0 0 * * 1'  # Executa toda segunda-feira às 00:00 (UTC)\n",
        "\n",
        "jobs:\n",
        "  run_pipeline:\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    steps:\n",
        "    - name: Checkout do repositório\n",
        "      uses: actions/checkout@v2\n",
        "\n",
        "    - name: Configurar Python\n",
        "      uses: actions/setup-python@v2\n",
        "      with:\n",
        "        python-version: '3.8'\n",
        "\n",
        "    - name: Instalar dependências\n",
        "      run: |\n",
        "        python -m pip install --upgrade pip\n",
        "        pip install -r requirements.txt\n",
        "\n",
        "    - name: Executar Extração de Dados\n",
        "      run: python scripts/pipeline/extracao_dados.py\n",
        "\n",
        "    - name: Executar Limpeza de Dados\n",
        "      run: python scripts/pipeline/limpeza_dados.py\n",
        "\n",
        "    - name: Executar Transformação de Dados\n",
        "      run: python scripts/pipeline/transformacao_dados.py\n",
        "\n",
        "    - name: Criar Banco Dimensional\n",
        "      run: python scripts/pipeline/criar_banco_dimensional.py\n",
        "\n",
        "    - name: Carregar Dados\n",
        "      run: python scripts/pipeline/carga_dados.py\n",
        "\n",
        "    - name: Gerar Catálogo de Dados\n",
        "      run: python scripts/pipeline/gerar_catalogo_dados.py\n",
        "\n",
        "    - name: Preparar Dados para Modelagem LSTM\n",
        "      run: python scripts/modelagem_machine_learning/preparar_dados_modelagem_LSTM.py\n",
        "\n",
        "    - name: Avaliar Modelo LSTM\n",
        "      run: python scripts/modelagem_machine_learning/calcular_metricas_avaliar_modelo_LSTM.py\n",
        "\n",
        "\n",
        "  ### Explicação do Workflow:\n",
        "\n",
        "    - Evento de Acionamento:\n",
        "\n",
        "    O workflow é acionado por dois eventos principais:\n",
        "\n",
        "    Push para a branch main: Sempre que um novo commit for enviado para a branch main, o pipeline será executado automaticamente.\n",
        "\n",
        "    Agendamento Semanal: O pipeline é executado toda segunda-feira às 00:00 UTC, garantindo que os dados sejam atualizados regularmente.\n",
        "\n",
        "    - Jobs:\n",
        "\n",
        "    run_pipeline: Este job é o responsável por executar todas as etapas do pipeline.\n",
        "\n",
        "    Ele é executado em uma máquina virtual Ubuntu, configurada com Python 3.8.\n",
        "\n",
        "    - Etapas:\n",
        "\n",
        "    Checkout do Repositório: Faz o checkout do código do repositório.\n",
        "\n",
        "    Configuração do Python: Configura o ambiente Python necessário.\n",
        "\n",
        "    Instalação de Dependências: Instala as dependências listadas no requirements.txt.\n",
        "\n",
        "    Execução das Etapas do Pipeline: Cada etapa do pipeline é executada com o comando python apontando para o script correspondente.\n",
        "\n",
        "    - Requisitos para o Workflow:\n",
        "    requirements.txt: Um arquivo contendo todas as dependências necessárias para rodar o pipeline. Ele deve estar no repositório e ser mantido atualizado.\n",
        "\n",
        "    Acesso ao Repositório: O repositório deve conter todos os scripts e arquivos de dados necessários para a execução do pipeline.\n"
      ],
      "metadata": {
        "id": "5OPmFsn9Y4wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Finalizando Pipeline\")"
      ],
      "metadata": {
        "id": "xZi27YhVdwWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Função operacional para esvaziar a pasta de dados para testes\n",
        "\n",
        "import os\n",
        "\n",
        "def esvaziar_pasta_data(caminho='/content/Piloto_Day_Trade/data'):\n",
        "    for root, dirs, files in os.walk(caminho):\n",
        "        for file in files:\n",
        "            caminho_arquivo = os.path.join(root, file)\n",
        "            os.remove(caminho_arquivo)\n",
        "            print(f'Removido: {caminho_arquivo}')\n",
        "    print('Pasta data esvaziada com sucesso.')\n",
        "\n",
        "# Executar\n",
        "esvaziar_pasta_data()\n"
      ],
      "metadata": {
        "id": "W8p1Xg-a6HJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RLWL1a8K0ol"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}