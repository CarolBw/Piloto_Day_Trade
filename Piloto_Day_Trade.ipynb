{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzawfp7U3Teejv6UHXbaeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarolBw/Piloto_Day_Trade/blob/main/Piloto_Day_Trade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/MVP_Objetivo.md\n",
        "#@title **Objetivo do Projeto**\n",
        "\n",
        "### 1. Propósito do MVP\n",
        "\n",
        "Este projeto tem como objetivo principal a criação de um pipeline para extração, transformação, carga, análise e previsão da movimentação intradiária dos preços de um ativo financeiro em intervalos de 5 minutos. O modelo preditivo central será baseado em redes neurais recorrentes (LSTM), mas outras abordagens serão exploradas. O MVP visa garantir previsões para embasar decisões estratégicas de day trade.\n",
        "\n",
        "### 2. Problema a Ser Resolvido\n",
        "\n",
        "A alta volatilidade dos mercados financeiros exige ferramentas robustas para antecipação de movimentos de preço. A dificuldade está em capturar padrões de curto prazo e projetá-los com precisão. Traders e investidores necessitam de um modelo que consiga interpretar os padrões históricos e transformá-los em previsões úteis.\n",
        "\n",
        "### 3. Pipeline do Projeto\n",
        "\n",
        "O projeto será estruturado em sete etapas principais:\n",
        "\n",
        "1. **Extração e armazenamento dos dados brutos:** Coleta de dados históricos de ativos financeiros em intervalos de 15 minutos utilizando a API do Yahoo Finance (yfinance). Os dados serão armazenados em um repositório GitHub sincronizado com Google Colab, garantindo acesso remoto e backup na nuvem.\n",
        "\n",
        "2. **Limpeza e organização dos dados:** Padronização de colunas, tratamento de dados ausentes, eliminação de duplicatas e organização cronológica. Resultado salvo como `dados_limpos.csv`.\n",
        "\n",
        "3. **Transformação e engenharia de features:** Adição de indicadores técnicos (como médias móveis, RSI, MACD), criação de variáveis de lag e retornos. Resultado salvo como `dados_transformados.csv`.\n",
        "\n",
        "4. **Modelagem e estruturação do banco de dados:**\n",
        "\n",
        "   a) Organização em arquivos CSV:\n",
        "\n",
        "   - `dados_brutos.csv`: dados originais extraídos da API.\n",
        "   - `dados_limpos.csv`: após limpeza e padronização.\n",
        "   - `dados_transformados.csv`: após adição de features técnicas.\n",
        "   - `dados_final.csv`: versão padronizada e normalizada dos dados.\n",
        "\n",
        "   b) Banco de dados dimensional:\n",
        "\n",
        "   - **Fato**: `fato_precos`, contendo os valores de fechamento e chaves para dimensões.\n",
        "   - **Dimensões**:\n",
        "     - `dim_tempo`: atributos temporais.\n",
        "     - `dim_indicadores`: indicadores técnicos.\n",
        "     - `dim_lags`: variações e lags recentes.\n",
        "\n",
        "   Um **Catálogo de Dados** será elaborado com descrição, domínio, categorias e linhagem de cada variável.\n",
        "\n",
        "5. **Carga e Pipeline ETL:** Pipeline estruturado com as seguintes etapas:\n",
        "\n",
        "   - **Extração:** via API do Yahoo Finance.\n",
        "   - **Limpeza:** tratamento e estruturação básica.\n",
        "   - **Transformação:** geração de variáveis técnicas e derivadas.\n",
        "   - **Carga:** integração dos dados transformados no banco dimensional (fato e dimensões).\n",
        "\n",
        "6. **Treinamento e ajuste do modelo:** Implementação e ajuste de modelos preditivos (LSTM como baseline), com avaliação por métricas como MSE e R².\n",
        "\n",
        "7. **Interpretação dos resultados e resposta às perguntas:** Validação das previsões, análise de variáveis relevantes e contribuição dos resultados para decisões de trading.\n",
        "\n",
        "### 4. Perguntas a Serem Respondidas\n",
        "\n",
        "- É possível prever com precisão a movimentação intradiária de um ativo a cada 15 minutos?\n",
        "- Os dados do dia anterior fornecem informações suficientes para a previsão do dia seguinte?\n",
        "- A modelagem com LSTM captura corretamente as tendências de curto prazo?\n",
        "- A previsão da movimentação intradiária também permite derivar com precisão as targets globais do dia (abertura, mínima, máxima e fechamento)?\n",
        "- Quais indicadores técnicos e features são mais relevantes para melhorar a acurácia do modelo?\n",
        "- Como considerar corretamente as quebras de fim de semana (exemplo: prever a segunda-feira usando os dados de sexta-feira)?\n",
        "- A normalização e padronização das variáveis melhora a precisão do modelo?\n",
        "\n",
        "### 5. Critérios de Sucesso\n",
        "\n",
        "Para que o MVP seja considerado bem-sucedido o esperado é que:\n",
        "\n",
        "1. O pipeline de extração, transformação e previsão funcione de forma eficiente.\n",
        "2. O modelo consiga prever a movimentação dos preços com um erro médio aceitável (avaliado por MSE ou R2).\n",
        "3. A previsão de targets globais (abertura, máxima, mínima, fechamento) seja consistente com os valores reais.\n",
        "4. O modelo consiga lidar corretamente com fins de semana e feriados.\n",
        "5. As previsões sejam suficientes para auxiliar na tomada de decisão de trading.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj0A_HHDW-dj",
        "outputId": "4d8771ca-9bf1-4192-dc18-e1c8ec27aa9b",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/MVP_Objetivo.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "No6u9fG0h8-A",
        "outputId": "6ba884cf-c61c-40f8-fe90-cf9563c2ce6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/.env\n"
          ]
        }
      ],
      "source": [
        "#@title ## Escrevendo variáveis sensiveis\n",
        "\n",
        "%%writefile /content/.env\n",
        "\n",
        "PROJECT_NAME=Piloto_Day_Trade\n",
        "\n",
        "# Variáveis de ambiente para o Github\n",
        "GITHUB_USERNAME=CarolBw\n",
        "GITHUB_TOKEN =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\n",
        "EMAIL=carolbrescowitt@yahoo.com.br\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Instalando dependências\n",
        "'''\n",
        "Usamos `-q` para ocultar a saída detalhada e mostrar apenas a barra de progresso\n",
        "\n",
        "'''\n",
        "!pip install -q tensorflow > /dev/null  # Framework para redes neurais e deep learning\n",
        "!pip install -q keras > /dev/null  # Biblioteca de alto nível para redes neurais\n",
        "!pip install -q pandas > /dev/null  # Manipulação e análise de dados\n",
        "!pip install -q numpy > /dev/null  # Computação numérica eficiente\n",
        "!pip install -q matplotlib > /dev/null  # Visualização de gráficos e análise exploratória\n",
        "!pip install -q scikit-learn > /dev/null  # Ferramentas para pré-processamento e métricas de avaliação\n",
        "!pip install -q gitpython > /dev/null  # Gerenciamento de repositórios Git via Python\n",
        "!pip install -q python-dotenv > /dev/null  # Manipulação de variáveis de ambiente\n",
        "!pip install -q seaborn > /dev/null  # Biblioteca de visualização estatística baseada no Matplotlib\n",
        "!pip install -q yfinance > /dev/null  # Coleta de dados financeiros diretamente do Yahoo Finance\n",
        "!pip install -q sqlalchemy > /dev/null  # ORM para interagir com bancos de dados relacionais\n",
        "!pip install -q dotenv > /dev/null # Manipulação de variáveis de ambiente\n",
        "\n",
        "# Importações das bibliotecas\n",
        "import pandas as pd  # Manipulação de DataFrames\n",
        "import numpy as np  # Cálculos numéricos e matrizes\n",
        "import matplotlib.pyplot as plt  # Geração de gráficos\n",
        "import sqlite3  # Integração com banco de dados SQLite\n",
        "\n",
        "# Pré-processamento dos dados\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # Normalização e padronização dos dados\n",
        "from sklearn.model_selection import train_test_split  # Divisão dos dados em treino e teste\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Avaliação do desempenho do modelo\n",
        "\n",
        "# Construção do modelo preditivo\n",
        "from keras.models import Sequential  # Modelo sequencial de rede neural\n",
        "from keras.layers import Dense  # Camada densa para aprendizado profundo\n",
        "\n",
        "# Controle de versão e variáveis de ambiente\n",
        "import git  # Gerenciamento do repositório Git\n",
        "import dotenv  # Carregamento de variáveis de ambiente\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LWOFHzJ6IupA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Capturamdo todas as dependencias do ambiente nesta primeira etapa\n",
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "ujEHOrH9cTIQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Configurando sincronização com Github\n",
        "\n",
        "%%writefile /content/configurar_git.py\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def git_config():\n",
        "    \"\"\"Configura o Git localmente e sincroniza com o repositório remoto no GitHub.\"\"\"\n",
        "\n",
        "    # Carregar variáveis de ambiente do arquivo .env\n",
        "    load_dotenv(dotenv_path='/content/.env')\n",
        "\n",
        "    # Obter as variáveis de ambiente do .env para o GitHub\n",
        "    GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
        "    EMAIL = os.getenv('EMAIL')\n",
        "    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
        "    PROJECT_NAME = os.getenv('PROJECT_NAME')\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{PROJECT_NAME}.git\"\n",
        "\n",
        "    # Configurar o Git localmente com as credenciais\n",
        "    os.system(f'git config --global user.name \"{GITHUB_USERNAME}\"')\n",
        "    os.system(f'git config --global user.email \"{EMAIL}\"')\n",
        "\n",
        "    # Verificar se o diretório do projeto já existe e se é um repositório Git válido\n",
        "    if os.path.isdir(PROJECT_NAME):\n",
        "        print(f\"O diretório '{PROJECT_NAME}' já existe. Entrando no diretório e sincronizando...\")\n",
        "\n",
        "        os.chdir(PROJECT_NAME)  # Entrar na pasta do projeto\n",
        "\n",
        "        # Garantir que estamos na branch main\n",
        "        os.system(\"git branch -M main\")\n",
        "\n",
        "        # Remover qualquer configuração errada do repositório remoto e adicionar novamente\n",
        "        os.system(\"git remote remove origin\")\n",
        "        os.system(\"git remote add origin \" + REPO_URL)\n",
        "\n",
        "        # Puxar as últimas atualizações do GitHub, tratando históricos não relacionados\n",
        "        os.system(\"git pull origin main --allow-unrelated-histories --no-rebase\")\n",
        "    else:\n",
        "        print(f\"Clonando o repositório '{PROJECT_NAME}'...\")\n",
        "\n",
        "        # Clonar o repositório remoto\n",
        "        os.system(f\"git clone {REPO_URL}\")\n",
        "        os.chdir(PROJECT_NAME)  # Entrar no diretório após o clone\n",
        "\n",
        "        # Inicializar o repositório Git local (se necessário) e configurar remoto\n",
        "        os.system(\"git branch -M main\")\n",
        "        os.system(\"git remote add origin \" + REPO_URL)\n",
        "\n",
        "        # Realizar o pull inicial para garantir que a branch main está sincronizada\n",
        "        os.system(\"git pull origin main --allow-unrelated-histories --no-rebase\")\n",
        "\n",
        "    print(f\"Configuração do Git concluída e sincronizada com a branch main do repositório{REPO_URL}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    git_config()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSdFzemI9yY",
        "outputId": "1d3ae2c6-6257-485e-9926-c2f34f15a5e9",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/configurar_git.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sincronizando repositório\n",
        "\n",
        "from configurar_git import git_config\n",
        "git_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLWe41oYxhcr",
        "outputId": "1fe75478-f314-4c12-b328-39a1f8604c8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clonando o repositório 'Piloto_Day_Trade'...\n",
            "Configuração do Git concluída e sincronizada com a branch main do repositóriohttps://CarolBw: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@github.com/CarolBw/Piloto_Day_Trade.git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Definindo estrutura de pastas do projeto\n",
        "\n",
        "\"\"\"\n",
        "Estrutura inicial do repositório Piloto_Day_Trade:\n",
        "\n",
        "|- notebooks/         → Jupyter Notebooks para exploração e análises\n",
        "|- scripts/           → Funções reutilizáveis (pré-processamento, modelagem, avaliação, etc.)\n",
        "   |-modelagem_machine_learning\n",
        "   |-operacional\n",
        "   |-pipeline\n",
        "|- data/              → Dados organizados em 3 níveis:\n",
        "   |- raw/            → Dados brutos extraídos diretamente de fontes externas\n",
        "   |- cleaned/        → Dados limpos com tratamento básico (ex: datas, nulos, nomes de colunas)\n",
        "   |- transformed/    → Dados com features criadas e prontos para modelagem\n",
        "|- modelagem/         → Modelagem do banco de dados.\n",
        "   |- database/       → Banco de dados\n",
        "   |- catalog/        → Catálogo de dados\n",
        "   |- esquema/        → Esquema do banco de dados\n",
        "|- .github/\n",
        "   |-workflows\n",
        "|- models/            → Modelos treinados\n",
        "|- reports/           → Resultados, gráficos, relatórios de performance\n",
        "|- MVP_Objetivo.md    → Documento explicando o objetivo do projeto\n",
        "|- README.md          → Instruções gerais do projeto\n",
        "|-.env                → Variáveis de ambiente e configurações sensíveis\n",
        "|- requirements.txt   → Lista de dependências\n",
        "|- LICENSE            → Licença do projeto\n",
        "\"\"\"\n",
        "\n",
        "# Criar estrutura de diretórios\n",
        "%cd /content/\n",
        "\n",
        "!mkdir -p /content/Piloto_Day_Trade/notebooks\n",
        "!mkdir -p /content/Piloto_Day_Trade/scripts/modelagem_machine_learning\n",
        "!mkdir -p /content/Piloto_Day_Trade/scripts/operacional\n",
        "!mkdir -p /content/Piloto_Day_Trade/scripts/pipeline\n",
        "!mkdir -p /content/Piloto_Day_Trade/data/raw\n",
        "!mkdir -p /content/Piloto_Day_Trade/data/cleaned\n",
        "!mkdir -p /content/Piloto_Day_Trade/data/transformed\n",
        "!mkdir -p /content/Piloto_Day_Trade/modelagem/catalog\n",
        "!mkdir -p /content/Piloto_Day_Trade/modelagem/esquema\n",
        "!mkdir -p /content/Piloto_Day_Trade/modelagem/database\n",
        "!mkdir -p /content/Piloto_Day_Trade/models\n",
        "!mkdir -p /content/Piloto_Day_Trade/reports\n",
        "!mkdir -p /content/Piloto_Day_Trade/.github/workflows\n",
        "\n",
        "# Criar arquivos principais\n",
        "!touch /content/Piloto_Day_Trade/.gitignore\n",
        "!touch /content/Piloto_Day_Trade/README.md\n"
      ],
      "metadata": {
        "id": "vVkhl3kbZKbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343e9998-f6da-443b-8827-7909f2d6b3c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mover arquivos existentes (ajuste conforme seus arquivos reais)\n",
        "!mv /content/.env /content/Piloto_Day_Trade/.env\n",
        "!mv /content/configurar_git.py /content/Piloto_Day_Trade/scripts/operacional/configurar_git.py\n",
        "!mv /content/requirements.txt /content/Piloto_Day_Trade/requirements.txt\n",
        "!mv /content/MVP_Objetivo.md /content/Piloto_Day_Trade/MVP_Objetivo.md"
      ],
      "metadata": {
        "id": "6YEIOMuuWI4K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver estrutura de diretórios\n",
        "!apt-get install tree -y > /dev/null 2>&1 # Instala o tree e oculta a saida da instalação\n",
        "!tree /content/Piloto_Day_Trade -d\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X7oQ6eFTvuc",
        "outputId": "ced3823f-56cb-48a5-ac69-34f666574ad5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/Piloto_Day_Trade\u001b[0m\n",
            "├── \u001b[01;34mdata\u001b[0m\n",
            "│   ├── \u001b[01;34mcleaned\u001b[0m\n",
            "│   ├── \u001b[01;34mraw\u001b[0m\n",
            "│   └── \u001b[01;34mtransformed\u001b[0m\n",
            "├── \u001b[01;34mmodelagem\u001b[0m\n",
            "│   ├── \u001b[01;34mcatalog\u001b[0m\n",
            "│   ├── \u001b[01;34mdatabase\u001b[0m\n",
            "│   └── \u001b[01;34mesquema\u001b[0m\n",
            "├── \u001b[01;34mmodels\u001b[0m\n",
            "│   ├── \u001b[01;34mLSTM\u001b[0m\n",
            "│   │   └── \u001b[01;34mscalers\u001b[0m\n",
            "│   └── \u001b[01;34mXGBoost\u001b[0m\n",
            "├── \u001b[01;34mnotebooks\u001b[0m\n",
            "├── \u001b[01;34mreports\u001b[0m\n",
            "└── \u001b[01;34mscripts\u001b[0m\n",
            "    ├── \u001b[01;34mmodelagem_machine_learning\u001b[0m\n",
            "    │   └── \u001b[01;34m__pycache__\u001b[0m\n",
            "    ├── \u001b[01;34moperacional\u001b[0m\n",
            "    ├── \u001b[01;34mpipeline\u001b[0m\n",
            "    │   └── \u001b[01;34m__pycache__\u001b[0m\n",
            "    └── \u001b[01;34m__pycache__\u001b[0m\n",
            "\n",
            "21 directories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Função operacional para atualizar o repositório\n",
        "# facilmente ao longo do desenvolvimento sem erros de sincronização\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def atualizar_repo(commit_message):\n",
        "    \"\"\"Atualiza o repositório remoto e mostra quantos arquivos foram comitados.\"\"\"\n",
        "    repo_path = \"/content/Piloto_Day_Trade\"\n",
        "\n",
        "    if not os.path.exists(os.path.join(repo_path, \".git\")):\n",
        "        print(\"Este diretório não é um repositório Git.\")\n",
        "        return\n",
        "\n",
        "    os.chdir(repo_path)\n",
        "\n",
        "    # Adiciona todos os arquivos\n",
        "    subprocess.run([\"git\", \"add\", \".\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # Executa o commit e captura a saída completa (stdout + stderr)\n",
        "    result = subprocess.run(\n",
        "        [\"git\", \"commit\", \"-m\", commit_message],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    # Juntamos as saídas para análise\n",
        "    output = result.stdout + result.stderr\n",
        "\n",
        "    if \"files changed\" in output:\n",
        "        # Procura a linha com o resumo da alteração\n",
        "        for linha in output.splitlines():\n",
        "            if \"files changed\" in linha:\n",
        "                print(f\"{linha}\")\n",
        "                break\n",
        "    elif \"nothing to commit\" in output:\n",
        "        print(\"ℹNenhuma alteração para comitar.\")\n",
        "    else:\n",
        "        print(\"Resultado inesperado do Git:\")\n",
        "        print(output)\n",
        "\n",
        "    # Envia para o repositório remoto\n",
        "    subprocess.run([\"git\", \"push\", \"origin\", \"main\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    print(\"Repositório atualizado.\")\n"
      ],
      "metadata": {
        "id": "TGqg5UeqMKbl",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Extração de dados\n",
        "\n",
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/extracao_dados.py\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "def extrair_dados(ticker, dias, intervalo, dados_brutos):\n",
        "    \"\"\"Extrai e organiza dados do Yahoo Finance no intervalo correto.\"\"\"\n",
        "\n",
        "    df_total = pd.DataFrame()  # DataFrame para armazenar os dados\n",
        "    data_inicio = datetime.today() - timedelta(days=dias)  # Data inicial\n",
        "    data_fim = datetime.today() + timedelta(days=1)\n",
        "\n",
        "    # Verifica se o arquivo de dados brutos existe\n",
        "    if os.path.exists(dados_brutos):\n",
        "        df = pd.read_csv(dados_brutos, index_col=0, parse_dates=True)\n",
        "\n",
        "        if not df.empty:\n",
        "            # Atualiza a data de início para a última data disponível nos dados brutos\n",
        "            ultima_data = pd.to_datetime(df.index.max())\n",
        "            data_inicio = ultima_data + timedelta(minutes=30)\n",
        "\n",
        "    print(f\"Extraindo dados de {data_inicio.strftime('%Y-%m-%d %H:%M:%S')} até {data_fim.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Extrai os dados do Yahoo Finance\n",
        "    df_novo = yf.download(\n",
        "        ticker,\n",
        "        start=data_inicio.strftime(\"%Y-%m-%d\"),\n",
        "        end=data_fim.strftime(\"%Y-%m-%d\"),\n",
        "        interval=intervalo,\n",
        "        progress=True\n",
        "    )\n",
        "\n",
        "    if not df_novo.empty:\n",
        "        # Apenas converte para \"America/Sao_Paulo\" se já tiver timezone\n",
        "        if df_novo.index.tzinfo is not None:\n",
        "            df_novo.index = df_novo.index.tz_convert(\"America/Sao_Paulo\")\n",
        "\n",
        "        # Concatena os novos dados com os existentes e remove duplicatas\n",
        "        df_total = pd.concat([df_total, df_novo]).drop_duplicates().sort_index()\n",
        "\n",
        "        # Remove linhas com mais de 50% de valores nulos\n",
        "        df_total = df_total.dropna(thresh=df_total.shape[1] * 0.5)\n",
        "\n",
        "        # Salva os dados atualizados no arquivo CSV\n",
        "        df_total.to_csv(dados_brutos)\n",
        "        print(\"Dados salvos com sucesso.\")\n",
        "\n",
        "    # Filtra os dados para o horário entre 10:00 e 18:00\n",
        "    df_filtrado = df_total.between_time(\"10:00\", \"18:00\")\n",
        "\n",
        "    # Exibe os 10 primeiros e os 10 últimos registros\n",
        "    print(\"Últimos 10 dados filtrados:\")\n",
        "    print(df_filtrado.tail(10))\n",
        "    print(\"Primeiros 10 dados filtrados:\")\n",
        "    print(df_filtrado.head(10))\n",
        "\n",
        "    return df_filtrado\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker = \"BBDC4.SA\"  # Ticker da ação\n",
        "    intervalo = \"5m\"  # Intervalo de tempo (5 minutos)\n",
        "    dias = 45  # Número de dias a partir de hoje para buscar os dados\n",
        "    dados_brutos = \"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\"  # Caminho do arquivo de dados brutos\n",
        "    df = extrair_dados(ticker, dias, intervalo, dados_brutos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3JYaCsNw5fQ",
        "outputId": "8193ea6b-4521-4019-8562-71512a7a59e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Piloto_Day_Trade/scripts/pipeline/extracao_dados.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Executando extração de dados extração de dados\n",
        "from Piloto_Day_Trade.scripts.pipeline.extracao_dados import extrair_dados\n",
        "\n",
        "ticker = \"BBDC4.SA\"  # Ticker da ação\n",
        "intervalo = \"5m\"  # Intervalo de tempo\n",
        "dias = 45  # Número de dias a partir de hoje para buscar os dados\n",
        "dados_brutos = \"/content/Piloto_Day_Trade/data/raw/dados_brutos1104.csv\"  # Caminho do arquivo de dados brutos para extração atual\n",
        "df = extrair_dados(ticker, dias, intervalo, dados_brutos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "OgEhS9Rwx_Im",
        "outputId": "6dfcd45a-23d5-4daf-f050-7137cd68c5fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Piloto_Day_Trade/scripts/pipeline/extracao_dados.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(dados_brutos, index_col=0, parse_dates=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DateParseError",
          "evalue": "Unknown datetime string format, unable to parse: Ticker, at position 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0d5f0d60f395>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m45\u001b[0m  \u001b[0;31m# Número de dias a partir de hoje para buscar os dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdados_brutos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/Piloto_Day_Trade/data/raw/dados_brutos1104.csv\"\u001b[0m  \u001b[0;31m# Caminho do arquivo de dados brutos para extração atual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextrair_dados\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintervalo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdados_brutos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Piloto_Day_Trade/scripts/pipeline/extracao_dados.py\u001b[0m in \u001b[0;36mextrair_dados\u001b[0;34m(ticker, dias, intervalo, dados_brutos)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Atualiza a data de início para a última data disponível nos dados brutos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0multima_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mdata_inicio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0multima_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminutes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: avoid this kludge.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_strptime_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     result, tz_parsed = objects_to_datetime64(\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[1;32m   2396\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2398\u001b[0;31m     result, tz_parsed = tslib.array_to_datetime(\n\u001b[0m\u001b[1;32m   2399\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mconversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: Ticker, at position 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Limpeza de dados\n",
        "\n",
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/limpeza_dados.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def limpeza_dados(df, path_dados_limpos):\n",
        "    # Verificar se os dados estão corretos\n",
        "    print(\"Dados originais:\")\n",
        "    print(df.head())\n",
        "    print(df.info())\n",
        "\n",
        "    # Remover as primeiras duas linhas (com 'Ticker' e 'Datetime')\n",
        "    df = df.iloc[2:].copy()\n",
        "\n",
        "    # Verificar após a remoção\n",
        "    print(\"Após remoção das duas primeiras linhas:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Garantir que o índice esteja no formato de data e hora (timezone UTC)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "\n",
        "    # Definir o fuso horário como \"America/Sao_Paulo\"\n",
        "    df.index = df.index.tz_convert(\"America/Sao_Paulo\")\n",
        "\n",
        "    # Remover a referência de fuso horário\n",
        "    df.index = df.index.tz_localize(None)\n",
        "\n",
        "    # Criar a coluna 'hora' com base no índice\n",
        "    df['hora'] = df.index.strftime('%H:%M:%S')\n",
        "\n",
        "    # Renomear o índice para 'data'\n",
        "    df.index.name = 'data'\n",
        "\n",
        "    # Resetar o índice para transformar o Datetime em uma coluna normal\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # Verificar após a transformação do índice\n",
        "    print(\"\\nApós conversão de índice:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Remover o horário da coluna 'data', mantendo apenas a data\n",
        "    df['data'] = df['data'].dt.date\n",
        "\n",
        "    # Mapeamento das colunas para nomes padronizados\n",
        "    mapeamento_colunas = {\n",
        "        'Close': 'fechamento',\n",
        "        'High': 'maximo',\n",
        "        'Low': 'minimo',\n",
        "        'Open': 'abertura',\n",
        "        'Volume': 'volume'\n",
        "    }\n",
        "\n",
        "    # Renomear as colunas\n",
        "    df.rename(columns=mapeamento_colunas, inplace=True)\n",
        "\n",
        "    # Converte e arredonda as colunas numéricas\n",
        "    for col in ['abertura', 'minimo', 'maximo', 'fechamento']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').round(2)\n",
        "\n",
        "    # Converte a coluna 'volume' para número inteiro\n",
        "    df['volume'] = pd.to_numeric(df['volume'], errors='coerce', downcast='integer')\n",
        "\n",
        "    # Reorganiza as colunas na ordem desejada\n",
        "    df = df[['data', 'hora', 'abertura', 'minimo', 'maximo', 'fechamento', 'volume']]\n",
        "\n",
        "    # Verificar após reorganizar as colunas\n",
        "    print(\"\\nApós reorganizar as colunas:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Verificar e remover duplicatas mantendo a primeira ocorrência\n",
        "    df = df.drop_duplicates(keep='first')\n",
        "\n",
        "    # Remover as linhas com 50% ou mais de valores nulos\n",
        "    df = df.dropna(thresh=df.shape[1] * 0.5)\n",
        "\n",
        "    # Verificar após remoção de duplicatas e nulos\n",
        "    print(\"\\nApós remover duplicatas e nulos:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Garantir que 'data' e 'hora' estejam no formato datetime\n",
        "    df['data'] = pd.to_datetime(df['data'], format='%Y-%m-%d')\n",
        "    df['hora'] = pd.to_datetime(df['hora'], format='%H:%M:%S').dt.time\n",
        "\n",
        "    # Filtra apenas os dias úteis (segunda a sexta)\n",
        "    df = df[df['data'].dt.weekday < 5]\n",
        "\n",
        "    # Verificar após filtrar dias úteis\n",
        "    print(\"\\nApós filtrar apenas os dias úteis:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Filtra apenas horários entre 09:55 e 18:05\n",
        "    df = df[(df['hora'] >= pd.to_datetime('09:55:00').time()) &\n",
        "            (df['hora'] <= pd.to_datetime('18:05:00').time())]\n",
        "\n",
        "    # Verificar após filtrar o intervalo de horário\n",
        "    print(\"\\nApós filtrar o intervalo de horário (09:55-18:05):\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Caso o DataFrame fique vazio, informar o motivo\n",
        "    if df.empty:\n",
        "        print(\"O DataFrame ficou vazio após o filtro de horário. Verifique se os dados estão dentro do intervalo de 09:55-18:05.\")\n",
        "    else:\n",
        "        print(\"\\nLimpeza de dados concluída com sucesso.\")\n",
        "\n",
        "    # Ordenar os dados\n",
        "    df = df.sort_values([\"data\", \"hora\"], ascending=[False, True])\n",
        "    print(\"\\nDados limpos e ordenados:\")\n",
        "    print(df.head(10))\n",
        "\n",
        "    # Salva os dados limpos em CSV\n",
        "    df.to_csv(path_dados_limpos, index=False)\n",
        "    print(f\"\\nOs dados foram limpos e salvos em csv.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ler os dados brutos\n",
        "    dados_brutos = pd.read_csv(f\"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\", index_col=0, parse_dates=True, dayfirst=True)\n",
        "    path_dados_limpos = '/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv'\n",
        "    # Aplicar limpeza nos dados\n",
        "    df_limpo = limpeza_dados(dados_brutos, path_dados_limpos)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DIHkNWR1-QS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aplicando limpeza de dados a primeira versao dos dados brutos\n",
        "from Piloto_Day_Trade.scripts.pipeline.limpeza_dados import limpeza_dados\n",
        "\n",
        "dados_brutos = pd.read_csv(f\"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\", index_col=0, parse_dates=True, dayfirst=True)\n",
        "path_dados_limpos = '/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv'\n",
        "df_limpo = limpeza_dados(dados_brutos, path_dados_limpos)\n",
        "\n"
      ],
      "metadata": {
        "id": "97mwKCsf1V3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Transformação de dados\n",
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/transformacao_dados.py\n",
        "\n",
        "\"\"\"\n",
        "Função de Transformação de Dados para Modelagem Preditiva\n",
        "Processa e transforma os dados para análise e previsão, gerando um conjunto\n",
        "de características para serem utilizadas no treinamento dos modelos.\n",
        "\n",
        "Objetivos:\n",
        "- Criar um dataset com variáveis relevantes para o modelo.\n",
        "- Incluir indicadores técnicos, estatísticas de volatilidade, médias móveis e outras features.\n",
        "- Permitir a experimentação com diferentes combinações de features.\n",
        "\n",
        "Estratégia:\n",
        "- Durante os testes de parametrização e treinamento, serão geradas diferentes versões do dataset,\n",
        "refinando a seleção de features a medida que geramos acurácia.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Carregar variáveis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "def carregar_dados(arquivo):\n",
        "    \"\"\"Carrega um CSV e retorna um DataFrame, ou um DataFrame vazio se o arquivo não existir.\"\"\"\n",
        "    if isinstance(arquivo, pd.DataFrame):\n",
        "        return arquivo  # Se já for um DataFrame, retorna diretamente\n",
        "\n",
        "    if not os.path.exists(arquivo):\n",
        "        print(f\"O arquivo {arquivo} não existe. Criando um novo DataFrame vazio.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(arquivo, parse_dates=[\"data\"])\n",
        "        print(f\"Arquivo {arquivo} carregado com {len(df)} linhas.\")\n",
        "        return df if not df.empty else pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar {arquivo}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def obter_ultima_data(df):\n",
        "    \"\"\"Retorna a última data disponível nos dados.\"\"\"\n",
        "    if \"data\" in df.columns and not df.empty:\n",
        "        ultima_data = df[\"data\"].max()\n",
        "        print(f\"Última data encontrada nos dados: {ultima_data}\")\n",
        "        return ultima_data\n",
        "    return None\n",
        "\n",
        "def filtrar_novos_dados(df, ultima_data):\n",
        "    \"\"\"Filtra os dados para incluir apenas os novos registros.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"Nenhum dado limpo disponível.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if ultima_data:\n",
        "        df_novo = df[df[\"data\"] > ultima_data]\n",
        "        print(f\"Dados novos filtrados: {len(df_novo)} registros encontrados.\")\n",
        "        return df_novo\n",
        "    return df\n",
        "\n",
        "def calcular_indicadores(df):\n",
        "    \"\"\"Calcula indicadores técnicos e gera novas features para análise de dados financeiros.\"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Nenhum dado disponível para calcular indicadores.\")\n",
        "        return df\n",
        "\n",
        "    colunas_necessarias = [\"data\", \"hora\", \"abertura\", \"minimo\", \"maximo\", \"fechamento\", \"volume\"]\n",
        "\n",
        "    if not all(col in df.columns for col in colunas_necessarias):\n",
        "        print(\"Dados insuficientes para cálculo de indicadores.\")\n",
        "        return df\n",
        "\n",
        "    # Ordenação correta dos dados\n",
        "    df = df.sort_values(by=['data', 'hora'], ascending=[True, True])\n",
        "\n",
        "    # Cálculo do retorno percentual e volatilidade\n",
        "    df['retorno'] = df['fechamento'].pct_change()\n",
        "    df['volatilidade'] = df['retorno'].rolling(20).std()\n",
        "\n",
        "    # Médias móveis\n",
        "    df['SMA_10'] = df['fechamento'].rolling(10).mean()\n",
        "    df['EMA_10'] = df['fechamento'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    # MACD e linha de sinal\n",
        "    df['MACD'] = df['fechamento'].ewm(span=12).mean() - df['fechamento'].ewm(span=26).mean()\n",
        "    df['Signal_Line'] = df['MACD'].ewm(span=9).mean()\n",
        "\n",
        "    # RSI (Índice de Força Relativa)\n",
        "    ganho = df['retorno'].clip(lower=0)\n",
        "    perda = -df['retorno'].clip(upper=0)\n",
        "    media_ganho = ganho.ewm(span=14).mean()\n",
        "    media_perda = perda.ewm(span=14).mean() + 1e-10\n",
        "    df['rsi'] = 100 - (100 / (1 + (media_ganho / media_perda)))\n",
        "\n",
        "    # OBV (On Balance Volume)\n",
        "    df['OBV'] = (df['volume'] * np.sign(df['fechamento'].diff())).fillna(0).cumsum()\n",
        "\n",
        "    # Criar lags para fechamento, retorno e volume\n",
        "    for lag in range(1, 4):\n",
        "        df[f'fechamento_lag{lag}'] = df['fechamento'].shift(lag)\n",
        "        df[f'retorno_lag{lag}'] = df['retorno'].shift(lag)\n",
        "        df[f'volume_lag{lag}'] = df['volume'].shift(lag)\n",
        "\n",
        "    # Substituir NaN por zero onde necessário\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Ordenação final\n",
        "    df = df.sort_values(by=['data', 'hora'], ascending=[False, True])\n",
        "\n",
        "    print(f\"Indicadores calculados. Tamanho final do DataFrame: {len(df)} linhas.\")\n",
        "    return df\n",
        "\n",
        "def adicionar_features_temporais(df):\n",
        "    \"\"\"Adiciona colunas temporais para análise de séries temporais.\"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"Nenhum dado disponível para processamento.\")\n",
        "        return df\n",
        "\n",
        "    # Converter 'data' para datetime se necessário\n",
        "    df['data'] = pd.to_datetime(df['data'], errors='coerce')\n",
        "\n",
        "    # Criar coluna do dia da semana para entrada e previsão\n",
        "    df['dia_da_semana_entrada'] = df['data'].dt.weekday  # 0 = Segunda, 6 = Domingo\n",
        "    df['data_previsao'] = df['data'] + pd.DateOffset(days=1)\n",
        "    df['dia_da_semana_previsao'] = df['data_previsao'].dt.weekday\n",
        "\n",
        "    # Ajustar casos de sexta-feira para segunda-feira\n",
        "    df.loc[df['dia_da_semana_entrada'] == 4, 'data_previsao'] += pd.DateOffset(days=2)\n",
        "    df['dia_da_semana_previsao'] = df['data_previsao'].dt.weekday\n",
        "\n",
        "    # Verificar se 'hora' está presente e converter corretamente\n",
        "    if 'hora' in df.columns:\n",
        "        df['hora'] = pd.to_datetime(df['hora'].astype(str), format='%H:%M:%S', errors='coerce').dt.time\n",
        "\n",
        "        # Criar colunas de hora e minuto\n",
        "        df['hora_num'] = df['hora'].apply(lambda x: x.hour if pd.notnull(x) else np.nan)\n",
        "        df['minuto'] = df['hora'].apply(lambda x: x.minute if pd.notnull(x) else np.nan)\n",
        "\n",
        "        # Criar coluna indicando se o mercado está aberto (entre 10h e 17h)\n",
        "        df['mercado_aberto'] = ((df['hora_num'] >= 10) & (df['hora_num'] <= 17)).astype(int)\n",
        "    else:\n",
        "        df['hora_num'] = np.nan\n",
        "        df['minuto'] = np.nan\n",
        "        df['mercado_aberto'] = 0\n",
        "\n",
        "    return df\n",
        "\n",
        "def transformar_dados(dados_limpos, dados_transformados):\n",
        "    \"\"\"Executa o processo de transformação dos dados.\"\"\"\n",
        "\n",
        "    df_transformado = carregar_dados(dados_transformados)\n",
        "    df_limpo = carregar_dados(dados_limpos)\n",
        "\n",
        "\n",
        "    if df_transformado.empty:\n",
        "        print(\"Nenhum dado transformado encontrado. Criando novo DataFrame.\")\n",
        "\n",
        "    ultima_data = obter_ultima_data(df_transformado)\n",
        "    novos_dados = filtrar_novos_dados(df_limpo, ultima_data)\n",
        "\n",
        "    if not novos_dados.empty:\n",
        "        novos_dados = calcular_indicadores(novos_dados)\n",
        "        novos_dados = adicionar_features_temporais(novos_dados)\n",
        "        df_final = pd.concat([df_transformado, novos_dados], ignore_index=True) if not df_transformado.empty else novos_dados\n",
        "\n",
        "        pasta = os.path.dirname(dados_transformados)\n",
        "        if not os.path.exists(pasta):\n",
        "            os.makedirs(pasta)\n",
        "            print(f\"Criando diretório: {pasta}\")\n",
        "\n",
        "        df_final.to_csv(dados_transformados, index=False)\n",
        "        print(f\"Dados transformados salvos em {dados_transformados} ({len(df_final)} registros)\")\n",
        "        print(f\"Última data disponível nos dados: {df_final['data'].max()}\")\n",
        "        print(f\"df_final: {df_final.head(5)}\")\n",
        "        return df_final\n",
        "    else:\n",
        "        print(\"⏭Nenhum novo dado para processar.\")\n",
        "        print(f\"Última data disponível nos dados: {df_transformado['data'].max()}\")\n",
        "        print(f\"df_transformado: {df_transformado.head(5)}\")\n",
        "        return df_transformado\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    path_dados_limpos = '/content/Piloto_Day_Trade/data/dados_limpos.csv'\n",
        "    path_dados_transformados = '/content/Piloto_Day_Trade/data/dados_transformados_3103.csv'\n",
        "    df_transformado =transformar_dados(path_dados_limpos, path_dados_transformados)\n",
        "\n"
      ],
      "metadata": {
        "id": "sH35zARUD6jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Piloto_Day_Trade.scripts.pipeline.transformacao_dados import transformar_dados\n",
        "\n",
        "#@title Aplicando transformação de dados\n",
        "path_dados_limpos = '/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv'\n",
        "path_dados_transformados = '/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv'\n",
        "df_transformado = transformar_dados(path_dados_limpos, path_dados_transformados)\n"
      ],
      "metadata": {
        "id": "5bG6Yqpyuawu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando os dados\n",
        "df_transformado = pd.read_csv('/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv')\n"
      ],
      "metadata": {
        "id": "6Kd2TDjqmGVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando os tipos de dados\n",
        "df_transformado.dtypes"
      ],
      "metadata": {
        "id": "4G3hB-sknn-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Banco de dados"
      ],
      "metadata": {
        "id": "Ap-meZswIhir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/modelagem/definicao_esquema_estrela.md\n",
        "\n",
        "#@title Definição do esquema - Modelo Estrela\n",
        "\n",
        "O modelo estrela foi escolhido por sua simplicidade e clareza na organização dos dados para análise. Ele é ideal para consultas rápidas e análise preditiva. No nosso projeto, temos um único fato (preços OHLC) e múltiplas variáveis explicativas que os influenciam.\n",
        "\n",
        "A estrutura facilita agregações temporais e análises do comportamento dos preços, sendo também eficiente para alimentar o pipeline de machine learning. Ao organizar as variáveis preditoras ao redor das medidas de preço, conseguimos isolar responsabilidades e tornar as análises mais precisas e escaláveis.\n",
        "\n",
        "## Tabela Fato: `fato_precos`\n",
        "| Coluna         | Tipo   | Descrição                                   |\n",
        "|----------------|--------|---------------------------------------------|\n",
        "| id_fato_precos | int    | PK, identificador único da linha            |\n",
        "| id_tempo       | int    | FK para a dimensão tempo                    |\n",
        "| abertura       | float  | Preço de abertura                           |\n",
        "| minimo         | float  | Preço mínimo                                |\n",
        "| maximo         | float  | Preço máximo                                |\n",
        "| fechamento     | float  | Preço de fechamento (variável alvo)         |\n",
        "\n",
        "## Dimensão: `dim_tempo`\n",
        "| Coluna                | Tipo   | Descrição                                 |\n",
        "|------------------------|--------|-------------------------------------------|\n",
        "| id_tempo              | int    | PK                                        |\n",
        "| data                  | object | Data da observação                        |\n",
        "| hora                  | object | Hora da observação                        |\n",
        "| dia_da_semana_entrada | int    | Dia da semana da entrada (0=Seg, 6=Dom)   |\n",
        "\n",
        "## Dimensão: `dim_indicadores`\n",
        "| Coluna       | Tipo   | Descrição                                       |\n",
        "|--------------|--------|--------------------------------------------------|\n",
        "| id_indicadores | int  | PK                                               |\n",
        "| id_tempo     | int    | FK para a dimensão tempo                        |\n",
        "| SMA_10       | float  | Média móvel simples de 10 períodos              |\n",
        "| EMA_10       | float  | Média móvel exponencial de 10 períodos          |\n",
        "| MACD         | float  | Moving Average Convergence Divergence           |\n",
        "| Signal_Line  | float  | Linha de sinal do MACD                          |\n",
        "| rsi          | float  | Índice de força relativa                        |\n",
        "| OBV          | float  | On-Balance Volume                               |\n",
        "| retorno      | float  | Retorno do período                              |\n",
        "| volatilidade | float  | Volatilidade do período                         |\n",
        "\n",
        "## Dimensão: `dim_lags`\n",
        "| Coluna          | Tipo   | Descrição                                       |\n",
        "|-----------------|--------|--------------------------------------------------|\n",
        "| id_lags         | int    | PK                                              |\n",
        "| id_tempo        | int    | FK para a dimensão tempo                        |\n",
        "| fechamento_lag1 | float  | Fechamento no candle anterior (1 lag)          |\n",
        "| retorno_lag1    | float  | Retorno do candle anterior (1 lag)             |\n",
        "| volume_lag1     | float  | Volume do candle anterior (1 lag)              |\n",
        "| fechamento_lag2 | float  | Fechamento dois candles atrás (2 lags)         |\n",
        "| retorno_lag2    | float  | Retorno dois candles atrás (2 lags)            |\n",
        "| volume_lag2     | float  | Volume dois candles atrás (2 lags)             |\n",
        "| fechamento_lag3 | float  | Fechamento três candles atrás (3 lags)         |\n",
        "| retorno_lag3    | float  | Retorno três candles atrás (3 lags)            |\n",
        "| volume_lag3     | float  | Volume três candles atrás (3 lags)             |\n",
        "\n",
        "## Dimensão: `dim_operacional`\n",
        "| Coluna                 | Tipo   | Descrição                                      |\n",
        "|------------------------|--------|------------------------------------------------|\n",
        "| id_operacional         | int    | PK                                             |\n",
        "| id_tempo               | int    | FK para a dimensão tempo                       |\n",
        "| data_previsao          | object | Data prevista para o modelo                    |\n",
        "| dia_da_semana_previsao | int    | Dia da semana da previsão                      |\n",
        "| hora_num               | int    | Hora como número inteiro                       |\n",
        "| minuto                 | int    | Minuto da observação                           |\n",
        "| mercado_aberto         | int    | Indicador binário (1=aberto, 0=fechado)        |\n"
      ],
      "metadata": {
        "id": "kuxIPKOgzMvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/gerar_catalogo_dados.py\n",
        "\n",
        "#@title Script para gerar o catálogo de dados\n",
        "\"\"\"\n",
        "Catálogo de Dados contendo minimamente uma descrição detalhada dos dados e seus domínios,\n",
        "contendo valores mínimos e máximos esperados para dados numéricos, e possíveis categorias para dados categóricos.\n",
        "\n",
        "Este modelo deve também descrever a linhagem dos dados, de onde os mesmos foram baixados\n",
        "e qual técnica foi utilizada para compor o conjunto de dados, caso haja.\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Define colunas de cada tabela com tipos\n",
        "tabelas = {\n",
        "    \"fato_precos\": {\n",
        "        \"id_fato_precos\": \"int\",\n",
        "        \"id_tempo\": \"int\",\n",
        "        \"abertura\": \"float\",\n",
        "        \"minimo\": \"float\",\n",
        "        \"maximo\": \"float\",\n",
        "        \"fechamento\": \"float\"\n",
        "    },\n",
        "    \"dim_tempo\": {\n",
        "        \"id_tempo\": \"int\",\n",
        "        \"data\": \"object\",\n",
        "        \"hora\": \"object\",\n",
        "        \"dia_da_semana_entrada\": \"int\"\n",
        "    },\n",
        "    \"dim_indicadores\": {\n",
        "        \"id_indicadores\": \"int\",\n",
        "        \"id_tempo\": \"int\",\n",
        "        \"SMA_10\": \"float\",\n",
        "        \"EMA_10\": \"float\",\n",
        "        \"MACD\": \"float\",\n",
        "        \"Signal_Line\": \"float\",\n",
        "        \"rsi\": \"float\",\n",
        "        \"OBV\": \"float\",\n",
        "        \"retorno\": \"float\",\n",
        "        \"volatilidade\": \"float\"\n",
        "    },\n",
        "    \"dim_lags\": {\n",
        "        \"id_lags\": \"int\",\n",
        "        \"id_tempo\": \"int\",\n",
        "        \"fechamento_lag1\": \"float\",\n",
        "        \"retorno_lag1\": \"float\",\n",
        "        \"volume_lag1\": \"float\",\n",
        "        \"fechamento_lag2\": \"float\",\n",
        "        \"retorno_lag2\": \"float\",\n",
        "        \"volume_lag2\": \"float\",\n",
        "        \"fechamento_lag3\": \"float\",\n",
        "        \"retorno_lag3\": \"float\",\n",
        "        \"volume_lag3\": \"float\"\n",
        "    },\n",
        "    \"dim_operacional\": {\n",
        "        \"id_operacional\": \"int\",\n",
        "        \"id_tempo\": \"int\",\n",
        "        \"data_previsao\": \"object\",\n",
        "        \"dia_da_semana_previsao\": \"int\",\n",
        "        \"hora_num\": \"int\",\n",
        "        \"minuto\": \"int\",\n",
        "        \"mercado_aberto\": \"int\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def dominio(col, tipo):\n",
        "    if tipo in [\"float\", \"int\"]:\n",
        "        if \"retorno\" in col:\n",
        "            return \"-0.05 a 0.05 (retorno percentual por intervalo de 5 min)\"\n",
        "        elif \"volatilidade\" in col:\n",
        "            return \"0 a 0.1 (desvio padrão do retorno por janela de tempo)\"\n",
        "        elif \"abertura\" in col or \"fechamento\" in col or \"minimo\" in col or \"maximo\" in col:\n",
        "            return \"10.0 a 50.0 (valores típicos para BBDC4)\"\n",
        "        elif \"MACD\" in col or \"Signal\" in col:\n",
        "            return \"-5 a 5\"\n",
        "        elif \"rsi\" in col:\n",
        "            return \"0 a 100\"\n",
        "        elif \"OBV\" in col:\n",
        "            return \"valor acumulativo, depende do ativo\"\n",
        "        elif \"volume\" in col:\n",
        "            return \"0 a 1.000.000 (valores inteiros positivos)\"\n",
        "        elif \"dia_da_semana\" in col:\n",
        "            return \"0=Segunda, ..., 6=Domingo\"\n",
        "        elif \"mercado_aberto\" in col:\n",
        "            return \"0=Fechado, 1=Aberto\"\n",
        "        else:\n",
        "            return \"valores numéricos contínuos\"\n",
        "    elif tipo == \"object\":\n",
        "        if \"data\" in col:\n",
        "            return \"formato YYYY-MM-DD\"\n",
        "        elif \"hora\" in col:\n",
        "            return \"formato HH:MM:SS\"\n",
        "        else:\n",
        "            return \"texto livre\"\n",
        "    return \"não especificado\"\n",
        "\n",
        "def descricao(col):\n",
        "    descricoes = {\n",
        "        \"abertura\": \"Preço de abertura do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "        \"minimo\": \"Menor preço do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "        \"maximo\": \"Maior preço do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "        \"fechamento\": \"Preço de fechamento do ativo BBDC4 no intervalo de 5 minutos\",\n",
        "        \"retorno\": \"Retorno percentual do ativo no intervalo de 5 minutos\",\n",
        "        \"volatilidade\": \"Volatilidade dos retornos do ativo em janela deslizante\",\n",
        "        \"SMA_10\": \"Média móvel simples de 10 períodos calculada sobre os preços\",\n",
        "        \"EMA_10\": \"Média móvel exponencial de 10 períodos\",\n",
        "        \"MACD\": \"Moving Average Convergence Divergence, indicador técnico\",\n",
        "        \"Signal_Line\": \"Linha de sinal do MACD\",\n",
        "        \"rsi\": \"Índice de força relativa (RSI), oscilador técnico\",\n",
        "        \"OBV\": \"On Balance Volume, indicador técnico baseado em volume\",\n",
        "        \"hora_num\": \"Hora expressa como número inteiro\",\n",
        "        \"minuto\": \"Minuto do intervalo de tempo\",\n",
        "        \"mercado_aberto\": \"Indica se o mercado está aberto no horário (1) ou não (0)\"\n",
        "    }\n",
        "    for key in descricoes:\n",
        "        if key in col:\n",
        "            return descricoes[key]\n",
        "    if \"lag\" in col:\n",
        "        return f\"Valor defasado de {col.replace('_lag', '')}\"\n",
        "    if \"dia_da_semana\" in col:\n",
        "        return \"Dia da semana correspondente à data\"\n",
        "    if \"id_\" in col:\n",
        "        return \"Identificador único para relacionar com outras tabelas\"\n",
        "    return \"\"\n",
        "\n",
        "def tecnica(col):\n",
        "    if any(ind in col for ind in [\"SMA\", \"EMA\", \"MACD\", \"Signal\", \"rsi\", \"OBV\"]):\n",
        "        return \"calculado internamente via engenharia de features técnicas\"\n",
        "    if \"lag\" in col:\n",
        "        return \"calculado como valor defasado (lag)\"\n",
        "    if col in [\"data\", \"hora\", \"hora_num\", \"minuto\", \"dia_da_semana_entrada\", \"dia_da_semana_previsao\"]:\n",
        "        return \"extraído de data/hora original\"\n",
        "    if col == \"mercado_aberto\":\n",
        "        return \"derivado da data/hora com base em calendário de mercado\"\n",
        "    return \"cópia ou identificador\"\n",
        "\n",
        "linhagem = \"Fonte: Yahoo Finance via yfinance\"\n",
        "\n",
        "linhas = []\n",
        "for tabela, colunas in tabelas.items():\n",
        "    for col, tipo in colunas.items():\n",
        "        linhas.append({\n",
        "            \"tabela\": tabela,\n",
        "            \"coluna\": col,\n",
        "            \"tipo\": tipo,\n",
        "            \"descricao\": descricao(col),\n",
        "            \"dominio\": dominio(col, tipo),\n",
        "            \"tecnica\": tecnica(col),\n",
        "            \"linhagem\": linhagem\n",
        "        })\n",
        "\n",
        "catalogo_df = pd.DataFrame(linhas)\n",
        "catalogo_df.to_csv(\"/content/Piloto_Day_Trade/modelagem/catalogo_dados.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "_Gn9fb5M4S1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Executar a geração do Catalogo de dados\n",
        "!python /content/Piloto_Day_Trade/scripts/pipeline/gerar_catalogo_dados.py\n"
      ],
      "metadata": {
        "id": "o6QQCeAF1qvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Gerando catálogo de dados\")"
      ],
      "metadata": {
        "id": "DC29xOqV95Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Script para criar banco de dados e tabelas\n",
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/criar_banco_dimensional.py\n",
        "\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "def criar_banco(db_path):\n",
        "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
        "\n",
        "    # Conecta ao banco (cria se não existir)\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Comandos SQL para criar as tabelas\n",
        "    sql_script = \"\"\"\n",
        "    -- Criação da Tabela Fato\n",
        "    CREATE TABLE IF NOT EXISTS fato_precos (\n",
        "        id_fato_precos INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        abertura REAL,\n",
        "        minimo REAL,\n",
        "        maximo REAL,\n",
        "        fechamento REAL,\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "\n",
        "    -- Criação da Dimensão Tempo\n",
        "    CREATE TABLE IF NOT EXISTS dim_tempo (\n",
        "        id_tempo INTEGER PRIMARY KEY,\n",
        "        data TEXT,\n",
        "        hora TEXT,\n",
        "        dia_da_semana_entrada INTEGER\n",
        "    );\n",
        "\n",
        "    -- Criação da Dimensão Indicadores Técnicos\n",
        "    CREATE TABLE IF NOT EXISTS dim_indicadores (\n",
        "        id_indicadores INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        SMA_10 REAL,\n",
        "        EMA_10 REAL,\n",
        "        MACD REAL,\n",
        "        Signal_Line REAL,\n",
        "        rsi REAL,\n",
        "        OBV REAL,\n",
        "        retorno REAL,\n",
        "        volatilidade REAL,\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "\n",
        "    -- Criação da Dimensão Lags\n",
        "    CREATE TABLE IF NOT EXISTS dim_lags (\n",
        "        id_lags INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        fechamento_lag1 REAL,\n",
        "        retorno_lag1 REAL,\n",
        "        volume_lag1 REAL,\n",
        "        fechamento_lag2 REAL,\n",
        "        retorno_lag2 REAL,\n",
        "        volume_lag2 REAL,\n",
        "        fechamento_lag3 REAL,\n",
        "        retorno_lag3 REAL,\n",
        "        volume_lag3 REAL,\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "\n",
        "    -- Criação da Dimensão Operacional\n",
        "    CREATE TABLE IF NOT EXISTS dim_operacional (\n",
        "        id_operacional INTEGER PRIMARY KEY,\n",
        "        id_tempo INTEGER,\n",
        "        data_previsao TEXT,\n",
        "        dia_da_semana_previsao INTEGER,\n",
        "        hora_num INTEGER,\n",
        "        minuto INTEGER,\n",
        "        mercado_aberto INTEGER,\n",
        "        FOREIGN KEY (id_tempo) REFERENCES dim_tempo(id_tempo)\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    # Executa o script SQL\n",
        "    cursor.executescript(sql_script)\n",
        "\n",
        "    # Confirma e fecha\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(\"Banco e tabelas criados com sucesso.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OxeXvgYa_jC",
        "outputId": "25b17cd3-61af-4098-f84f-e46c3c7caa4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/Piloto_Day_Trade/scripts/pipeline/criar_banco_dimensional.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Excecutar criar banco e tabelas\n",
        "!python /content/Piloto_Day_Trade/scripts/pipeline/criar_banco_dimensional.py"
      ],
      "metadata": {
        "id": "NGaGzrS1AQcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar criação do banco\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "print(cursor.fetchall())\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsDCNhgeA2WU",
        "outputId": "adf1d0c3-7287-4b57-f7d0-03c9f62cc93e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('fato_precos',), ('dim_tempo',), ('dim_indicadores',), ('dim_lags',), ('dim_operacional',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar colunas\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional.db\")\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"PRAGMA table_info(dim_tempo);\")\n",
        "print(cursor.fetchall())\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "rM7plBnSFNHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/carga_dados.py\n",
        "\n",
        "#@title ## Definindo script de carga de dados\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Função para carregar dados\n",
        "def carregar_dados(df: pd.DataFrame):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    registros_inseridos = 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Verifica se já existe registro com a mesma data e hora na dim_tempo\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT id_tempo FROM dim_tempo WHERE data = ? AND hora = ?\n",
        "        \"\"\", (row['data'], row['hora']))\n",
        "        resultado = cursor.fetchone()\n",
        "\n",
        "        if resultado:\n",
        "            continue  # Já existe, pula\n",
        "\n",
        "        # Gerar próximo id_tempo\n",
        "        cursor.execute(\"SELECT MAX(id_tempo) FROM dim_tempo\")\n",
        "        max_id = cursor.fetchone()[0]\n",
        "        id_tempo = 1 if max_id is None else max_id + 1\n",
        "\n",
        "        # 1. Inserir na dim_tempo\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_tempo (id_tempo, data, hora, dia_da_semana_entrada)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "        \"\"\", (id_tempo, row['data'], row['hora'], row['dia_da_semana_entrada']))\n",
        "\n",
        "        # 2. Inserir na dim_indicadores\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_indicadores (id_indicadores, id_tempo, SMA_10, EMA_10, MACD, Signal_Line, rsi, OBV, retorno, volatilidade)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (id_tempo, id_tempo, row['SMA_10'], row['EMA_10'], row['MACD'], row['Signal_Line'], row['rsi'],\n",
        "              row['OBV'], row['retorno'], row['volatilidade']))\n",
        "\n",
        "        # 3. Inserir na dim_lags\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_lags (id_lags, id_tempo, fechamento_lag1, retorno_lag1, volume_lag1,\n",
        "                                  fechamento_lag2, retorno_lag2, volume_lag2,\n",
        "                                  fechamento_lag3, retorno_lag3, volume_lag3)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (id_tempo, id_tempo,\n",
        "              row['fechamento_lag1'], row['retorno_lag1'], row['volume_lag1'],\n",
        "              row['fechamento_lag2'], row['retorno_lag2'], row['volume_lag2'],\n",
        "              row['fechamento_lag3'], row['retorno_lag3'], row['volume_lag3']))\n",
        "\n",
        "        # 4. Inserir na dim_operacional\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO dim_operacional (id_operacional, id_tempo, data_previsao, dia_da_semana_previsao, hora_num, minuto, mercado_aberto)\n",
        "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (id_tempo, id_tempo, row['data_previsao'], row['dia_da_semana_previsao'],\n",
        "              row['hora_num'], row['minuto'], row['mercado_aberto']))\n",
        "\n",
        "        # 5. Inserir na fato_precos\n",
        "        cursor.execute(\"\"\"\n",
        "            INSERT INTO fato_precos (id_fato_precos, id_tempo, abertura, minimo, maximo, fechamento)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "        \"\"\", (id_tempo, id_tempo, row['abertura'], row['minimo'], row['maximo'], row['fechamento']))\n",
        "\n",
        "        registros_inseridos += 1\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"Carga incremental concluída. {registros_inseridos} novos registros inseridos.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Caminho para o banco de dados\n",
        "    db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional.db\"\n",
        "    assert os.path.exists(db_path), f\"Banco de dados não encontrado em {db_path}\"\n",
        "\n",
        "    # Leitura dos dados a serem carregados\n",
        "    df = pd.read_csv(\"/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv\")\n",
        "    carregar_dados(df)\n"
      ],
      "metadata": {
        "id": "0ahqd6uB1DFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Executar realizar a carga de dados\n",
        "!python /content/Piloto_Day_Trade/scripts/pipeline/carga_dados.py\n"
      ],
      "metadata": {
        "id": "zidwW0A7DfMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando a carga de dados\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "# Caminho para o banco\n",
        "db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional.db\"\n",
        "\n",
        "# Conexão e cursor\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Consulta nas tabelas principais\n",
        "tabelas = ['dim_tempo', 'dim_indicadores', 'dim_lags', 'dim_operacional', 'fato_precos']\n",
        "for tabela in tabelas:\n",
        "    cursor.execute(f\"SELECT COUNT(*) FROM {tabela}\")\n",
        "    count = cursor.fetchone()[0]\n",
        "    print(f\"{tabela}: {count} registros\")\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "ksO0iYGrEEXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar os primeiros 5 registros da fato_precos\n",
        "import pandas as pd\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "df_check = pd.read_sql_query(\"SELECT * FROM fato_precos LIMIT 5\", conn)\n",
        "print(df_check)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "v0OFgWADGalR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consulta com JOIN para verificar o relacionamento entre as tabelas\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    ft.id_fato_precos,\n",
        "    dt.data,\n",
        "    dt.hora,\n",
        "    ft.abertura,\n",
        "    ft.fechamento,\n",
        "    di.SMA_10,\n",
        "    dl.fechamento_lag1,\n",
        "    do.data_previsao,\n",
        "    do.mercado_aberto\n",
        "FROM fato_precos ft\n",
        "JOIN dim_tempo dt ON ft.id_tempo = dt.id_tempo\n",
        "JOIN dim_indicadores di ON ft.id_tempo = di.id_tempo\n",
        "JOIN dim_lags dl ON ft.id_tempo = dl.id_tempo\n",
        "JOIN dim_operacional do ON ft.id_tempo = do.id_tempo\n",
        "LIMIT 5;\n",
        "\"\"\"\n",
        "\n",
        "# Executando e exibindo\n",
        "conn = sqlite3.connect(db_path)\n",
        "df_verificacao = pd.read_sql_query(query, conn)\n",
        "print(df_verificacao)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrrqdeR8Gq1p",
        "outputId": "265b85a5-2808-441d-b032-d15cdd92c49d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id_fato_precos        data      hora  abertura  fechamento  SMA_10  \\\n",
            "0               1  2025-04-08  10:00:00     12.47       12.41  12.379   \n",
            "1               2  2025-04-08  10:05:00     12.41       12.41  12.384   \n",
            "2               3  2025-04-08  10:10:00     12.41       12.44  12.389   \n",
            "3               4  2025-04-08  10:15:00     12.44       12.39  12.390   \n",
            "4               5  2025-04-08  10:20:00     12.40       12.43  12.397   \n",
            "\n",
            "   fechamento_lag1 data_previsao  mercado_aberto  \n",
            "0            12.40    2025-04-09               1  \n",
            "1            12.41    2025-04-09               1  \n",
            "2            12.41    2025-04-09               1  \n",
            "3            12.44    2025-04-09               1  \n",
            "4            12.39    2025-04-09               1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Escrevendo o Readme do projeto\n",
        "%%writefile /content/Piloto_Day_Trade/README.md\n",
        "\n",
        "# Objetivo do Projeto\n",
        "\n",
        "## 1. Propósito do MVP\n",
        "\n",
        "Este projeto tem como objetivo principal a criação de um pipeline para extração, transformação, carga, análise e previsão da movimentação intradiária dos preços de um ativo financeiro em intervalos de 5 minutos. O modelo preditivo central será baseado em redes neurais recorrentes (LSTM), mas outras abordagens serão exploradas. O MVP visa garantir previsões para embasar decisões estratégicas de day trade.\n",
        "\n",
        "## 2. Problema a Ser Resolvido\n",
        "\n",
        "A alta volatilidade dos mercados financeiros exige ferramentas robustas para antecipação de movimentos de preço. A dificuldade está em capturar padrões de curto prazo e projetá-los com precisão. Traders e investidores necessitam de um modelo que consiga interpretar os padrões históricos e transformá-los em previsões úteis.\n",
        "\n",
        "## 3. Pipeline do Projeto\n",
        "\n",
        "O pipeline está estruturado em sete etapas principais:\n",
        "\n",
        "### 3.1. Extração e armazenamento dos dados brutos\n",
        "- Coleta de dados históricos do ativo BBDC4 em intervalos de 5 minutos, via API do Yahoo Finance (yfinance).\n",
        "- Armazenamento dos dados no GitHub sincronizado com Google Colab, com backup em nuvem.\n",
        "- Salvo como `dados_brutos.csv`\n",
        "\n",
        "### 3.2. Limpeza e organização dos dados\n",
        "- Padronização dos tipos de dados\n",
        "- Padronização dos nomes de colunas\n",
        "- Remoção de valores nulos ou duplicados\n",
        "- Remoção de colunas desnecessárias\n",
        "- Ordenação cronológica\n",
        "- Salvo como `dados_limpos.csv`\n",
        "\n",
        "### 3.3. Transformação de dados e engenharia de features\n",
        "- Cálculo de indicadores técnicos (SMA, EMA, MACD, RSI, OBV)\n",
        "- Cálculo de retornos e variância (volatilidade)\n",
        "- Criação de variáveis de lag de preço, volume e retorno\n",
        "- Adição de variáveis temporais (hora, dia da semana, mercado aberto)\n",
        "- Salvo como `dados_transformados.csv`\n",
        "\n",
        "### 3.4. Modelagem e estruturação do banco dimensional\n",
        "- **Fato**: `fato_precos` com preços e chave para `dim_tempo`\n",
        "- **Dimensões**:\n",
        "  - `dim_tempo`: data, hora, dia da semana\n",
        "  - `dim_indicadores`: indicadores técnicos\n",
        "  - `dim_lags`: lags de preço, volume, retorno\n",
        "  - `dim_operacional`: hora, minuto, data da previsão, mercado aberto\n",
        "- Banco gerado em SQLite via script automatizado (`banco_dimensional.db`)\n",
        "\n",
        "### 3.5. Carga (ETL)\n",
        "- **Extração:** via API (automatizada)\n",
        "- **Limpeza:** padronização, remoção de nulos/duplicatas\n",
        "- **Transformação:** features técnicas e derivadas\n",
        "- **Carga:** população das tabelas do banco dimensional\n",
        "- ETL organizado em scripts Python e automatizado\n",
        "\n",
        "### 3.6. Treinamento e ajuste do modelo preditivo\n",
        "- **Preparação dos dados**:\n",
        "  - Padronização com StandardScaler para retornos e indicadores\n",
        "  - Normalização com MinMaxScaler para preços e volumes\n",
        "  - Separar features (X) e targets (y)\n",
        "  - Divisão treino/teste com base em dias útis\n",
        "- **Modelo base:** LSTM com duas camadas ocultas, camada densa e MSE como perda\n",
        "- **Avaliação:** Métricas de MSE, R², comparação com targets reais\n",
        "\n",
        "### 3.7. Análise dos resultados\n",
        "- Comparativo entre preços previstos vs. reais\n",
        "- Validação das previsões para abertura, máxima, mínima e fechamento\n",
        "- Importância das variáveis\n",
        "- Interpretação dos erros e possíveis melhorias\n",
        "\n",
        "## 4. Perguntas a Serem Respondidas\n",
        "- É possível prever com precisão a movimentação intradiária a cada 5 minutos?\n",
        "- Os dados do dia anterior são suficientes para prever o comportamento do dia seguinte?\n",
        "- O modelo LSTM é eficaz para padrões de curtíssimo prazo?\n",
        "- É viável derivar os targets globais do dia a partir das previsões intradiárias?\n",
        "- Quais indicadores mais contribuem para a previsão?\n",
        "- Como lidar corretamente com fins de semana e feriados?\n",
        "- A padronização/normalização das variáveis afeta o desempenho?\n",
        "\n",
        "## 5. Critérios de Sucesso\n",
        "- Pipeline funcional de extração → transformação → carga → previsão\n",
        "- Modelo com bom desempenho em MSE e R²\n",
        "- Targets globais coerentes com valores reais\n",
        "- Correta gestão de datas (incluindo segundas-feiras)\n",
        "- Previsões utilizáveis para tomada de decisão\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "SCo7OFDwSG35",
        "outputId": "ec105b84-d543-4c8d-c113-f28d2629b781"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Piloto_Day_Trade/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Escrevendo Licença do projeto\n",
        "%%writefile /content/Piloto_Day_Trade/LICENSE\n",
        "\n",
        "Copyright (c) 2025 Carolina Brescowitt\n",
        "\n",
        "Todos os direitos reservados.\n",
        "\n",
        "Este software é fornecido gratuitamente apenas para uso **pessoal, acadêmico e de pesquisa**.\n",
        "\n",
        "O uso comercial deste software é estritamente proibido sem uma **licença comercial paga**, a ser negociada com o autor.\n",
        "\n",
        "Empresas, startups, desenvolvedores ou qualquer entidade que deseje utilizar este código em produtos, serviços ou plataformas comerciais devem entrar em contato com o autor para **negociar os termos de licenciamento** (incluindo percentual, royalties ou valores fixos).\n",
        "\n",
        "É proibida a redistribuição ou sublicenciamento sem autorização por escrito.\n",
        "\n",
        "Para mais informações, entre em contato: carolbrescowitt@yahoo.com.br\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guTRpOgpsxDC",
        "outputId": "0954d491-bcdd-46f9-cdb5-ec3248fde0f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Piloto_Day_Trade/LICENSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelagem de dados"
      ],
      "metadata": {
        "id": "ai-nFsCPCE4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/preparar_dados_modelagem_LSTM.py\n",
        "#@title Preparar dados modelagem LSTM\n",
        "\n",
        "\"\"\"\n",
        "Script de preparação de dados para modelagem com LSTM:\n",
        "- Aplica normalização e padronização\n",
        "- Salva scaler de preço\n",
        "- Salva versão tratada em CSV\n",
        "- Cria sequências de entrada e saída\n",
        "- Divide em treino e teste\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "def preparar_dados(path_dados):\n",
        "    caminho_scaler_preco = '/content/Piloto_Day_Trade/models/LSTM/scalers/scaler_normalizacao_preco.pkl'\n",
        "    os.makedirs(os.path.dirname(caminho_scaler_preco), exist_ok=True)\n",
        "    df = pd.read_csv(path_dados)\n",
        "\n",
        "    df['data'] = pd.to_datetime(df['data'], errors='coerce')\n",
        "    df['data_previsao'] = pd.to_datetime(df['data_previsao'], errors='coerce')\n",
        "\n",
        "    preco_cols = ['abertura', 'maximo', 'minimo', 'fechamento']\n",
        "    df = df.dropna(subset=preco_cols)\n",
        "\n",
        "    scaler_preco = MinMaxScaler()\n",
        "    scaler_preco.fit(df[preco_cols])\n",
        "    joblib.dump(scaler_preco, caminho_scaler_preco)\n",
        "\n",
        "    padronizar_cols = ['retorno', 'volatilidade', 'MACD', 'Signal_Line', 'rsi']\n",
        "    normalizar_cols = [\n",
        "        'abertura', 'minimo', 'maximo', 'fechamento', 'volume', 'SMA_10', 'EMA_10', 'OBV',\n",
        "        'fechamento_lag1', 'retorno_lag1', 'volume_lag1',\n",
        "        'fechamento_lag2', 'retorno_lag2', 'volume_lag2',\n",
        "        'fechamento_lag3', 'retorno_lag3', 'volume_lag3'\n",
        "    ]\n",
        "\n",
        "    df[padronizar_cols] = StandardScaler().fit_transform(df[padronizar_cols])\n",
        "    df[normalizar_cols] = MinMaxScaler().fit_transform(df[normalizar_cols])\n",
        "\n",
        "    categorias = ['dia_da_semana_entrada', 'dia_da_semana_previsao', 'hora_num', 'minuto', 'mercado_aberto']\n",
        "    df[categorias] = df[categorias].astype(int)\n",
        "\n",
        "    df = df.select_dtypes(include='number').dropna()\n",
        "\n",
        "    caminho_preparado = '/content/Piloto_Day_Trade/data/transformed/dados_preparados_para_modelagem.csv'\n",
        "    os.makedirs(os.path.dirname(caminho_preparado), exist_ok=True)\n",
        "    df.to_csv(caminho_preparado, index=False)\n",
        "    print(f\"Dados preparados salvos em: {caminho_preparado}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def criar_sequencias(df, tam_seq=96):\n",
        "    entradas, saidas = [], []\n",
        "    for i in range(len(df) - 2*tam_seq):\n",
        "        entrada = df.iloc[i : i + tam_seq].values\n",
        "        saida = df.iloc[i + tam_seq : i + 2*tam_seq][['abertura', 'maximo', 'minimo', 'fechamento']].values\n",
        "        entradas.append(entrada)\n",
        "        saidas.append(saida)\n",
        "    return np.array(entradas), np.array(saidas)\n",
        "\n",
        "def dividir_treino_teste(X, y, tx_treino=0.8):\n",
        "    tamanho_treino = int(tx_treino * len(X))\n",
        "    return X[:tamanho_treino], X[tamanho_treino:], y[:tamanho_treino], y[tamanho_treino:]\n",
        "\n",
        "def preparar_dados_lstm(path_dados, tam_seq=96, tx_treino=0.8):\n",
        "    df_preparado = preparar_dados(path_dados)\n",
        "    X, y = criar_sequencias(df_preparado, tam_seq)\n",
        "    X_treino, X_teste, y_treino, y_teste = dividir_treino_teste(X, y, tx_treino)\n",
        "    return X_treino, X_teste, y_treino, y_teste\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    path_dados = '/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv'\n",
        "    X_treino, X_teste, y_treino, y_teste = preparar_dados_lstm(path_dados, tam_seq=96, tx_treino=0.8)\n",
        "\n",
        "    print(\"Dados de treino e teste prontos:\")\n",
        "    print(f\"X_treino: {X_treino.shape}, y_treino: {y_treino.shape}\")\n",
        "    print(f\"X_teste: {X_teste.shape}, y_teste: {y_teste.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUTR1S1f9y-f",
        "outputId": "f331bef1-fe83-43dd-954c-d2f24bfadac7",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/preparar_dados_modelagem_LSTM.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Executar preparar dados\n",
        "from Piloto_Day_Trade.scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "\n",
        "X_treino, X_teste, y_treino, y_teste = preparar_dados_lstm(\n",
        "    path_dados='/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv',\n",
        "    tam_seq=96,\n",
        "    tx_treino=0.8\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjm1mWwD7gBf",
        "outputId": "e0ff0a55-1058-409e-905b-0518af3073a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados preparados salvos em: /content/Piloto_Day_Trade/data/transformed/dados_preparados_para_modelagem.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/criar_modelo_LSTM.py\n",
        "#@title Definindo Script para criar o modelo LSTM\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed\n",
        "\n",
        "def LSTM_model(input_shape):\n",
        "    \"\"\"\n",
        "    Cria um modelo LSTM com:\n",
        "    - 2 camadas LSTM com Dropout\n",
        "    - Uma camada TimeDistributed com 4 saídas por timestep (abertura, maximo, minimo, fechamento)\n",
        "    - Compilado com otimizador Adam e perda MSE\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): formato da entrada (timesteps, n_features)\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras.Model): modelo compilado pronto para treino\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Primeira camada LSTM com 64 neurônios e retorno de sequência\n",
        "    model.add(LSTM(units=64, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))  # Dropout para evitar overfitting\n",
        "\n",
        "    # Segunda camada LSTM com 32 neurônios\n",
        "    model.add(LSTM(units=32, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Camada final: 4 saídas (abertura, max, min, fechamento) por timestep\n",
        "    model.add(TimeDistributed(Dense(4)))\n",
        "\n",
        "    # Compilando o modelo\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6lFmK-FMncY",
        "outputId": "e1b9c59d-7b08-4444-ed91-34c388cee846"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/criar_modelo_LSTM.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Criar, treinar e salvar modelo LSTM v1\n",
        "\n",
        "# Importar função para criar modelo e preparar dados\n",
        "from scripts.modelagem_machine_learning.criar_modelo_LSTM import LSTM_model\n",
        "from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "import os\n",
        "\n",
        "# Preparar os dados de entrada e saída para a LSTM\n",
        "X_treino, X_teste, y_treino, y_teste = preparar_dados_lstm(\n",
        "    path_dados='/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv',\n",
        "    tam_seq=96,      # Janela de 96 passos de tempo (8 horas em dados de 5 minutos)\n",
        "    tx_treino=0.8    # Proporção para treino\n",
        ")\n",
        "\n",
        "# Definir o formato de entrada para a rede LSTM: (timesteps, n_features)\n",
        "input_shape = (X_treino.shape[1], X_treino.shape[2])\n",
        "\n",
        "# Criar instância do modelo com os parâmetros padrão\n",
        "modelo_v1 = LSTM_model(input_shape=input_shape)\n",
        "\n",
        "# Treinar o modelo\n",
        "modelo_v1.fit(\n",
        "    X_treino, y_treino,\n",
        "    epochs=20,           # Número de épocas\n",
        "    batch_size=32,       # Tamanho do batch\n",
        "    verbose=1            # Mostrar progresso\n",
        ")\n",
        "\n",
        "# Criar diretório onde o modelo será salvo (caso não exista)\n",
        "os.makedirs('/content/Piloto_Day_Trade/models/LSTM', exist_ok=True)\n",
        "\n",
        "# Salvar o modelo treinado como versão 1\n",
        "modelo_v1.save('/content/Piloto_Day_Trade/models/LSTM/modelo_LSTM_v1.keras')\n",
        "\n",
        "print(\"Modelo LSTM v1 treinado e salvo com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSGaNfmtKwU6",
        "outputId": "6e1f48d5-c2be-4833-c670-462a156bb0f3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados preparados salvos em: /content/Piloto_Day_Trade/data/transformed/dados_preparados_para_modelagem.csv\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 168ms/step - loss: 0.1607\n",
            "Epoch 2/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 127ms/step - loss: 0.0510\n",
            "Epoch 3/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 142ms/step - loss: 0.0270\n",
            "Epoch 4/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 128ms/step - loss: 0.0206\n",
            "Epoch 5/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - loss: 0.0175\n",
            "Epoch 6/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 115ms/step - loss: 0.0155\n",
            "Epoch 7/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 136ms/step - loss: 0.0132\n",
            "Epoch 8/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 154ms/step - loss: 0.0120\n",
            "Epoch 9/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 112ms/step - loss: 0.0107\n",
            "Epoch 10/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 152ms/step - loss: 0.0099\n",
            "Epoch 11/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 124ms/step - loss: 0.0090\n",
            "Epoch 12/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 116ms/step - loss: 0.0083\n",
            "Epoch 13/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 114ms/step - loss: 0.0076\n",
            "Epoch 14/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 134ms/step - loss: 0.0071\n",
            "Epoch 15/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 129ms/step - loss: 0.0065\n",
            "Epoch 16/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 125ms/step - loss: 0.0062\n",
            "Epoch 17/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - loss: 0.0058\n",
            "Epoch 18/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 139ms/step - loss: 0.0056\n",
            "Epoch 19/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 114ms/step - loss: 0.0053\n",
            "Epoch 20/20\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 140ms/step - loss: 0.0051\n",
            "Modelo LSTM v1 treinado e salvo com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Incluindo Modelo LSTM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiGwgsI6p9IC",
        "outputId": "bc788ac4-a6a8-4719-e839-c72c84fec2e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2 files changed, 0 insertions(+), 0 deletions(-)\n",
            "Repositório atualizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calcular métricas e avaliar modelo LSTM\n",
        "%%writefile /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/calcular_metricas_avaliar_modelo_LSTM.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def avaliar_modelo_lstm(modelo, X_teste, y_teste, caminho_scaler='/content/Piloto_Day_Trade/models/LSTM/scalers/scaler_normalizacao_preco.pkl'):\n",
        "    \"\"\"\n",
        "    Avalia o modelo LSTM, imprimindo as principais métricas e comparação entre previsões e valores reais.\n",
        "    \"\"\"\n",
        "    print(\"Realizando previsões...\")\n",
        "    y_previsto = modelo.predict(X_teste)\n",
        "\n",
        "    print(\"Carregando scaler de preços para inversão...\")\n",
        "    scaler_precos = joblib.load(caminho_scaler)\n",
        "    colunas_precos = ['abertura', 'maximo', 'minimo', 'fechamento']\n",
        "\n",
        "    y_previsto_reshape = y_previsto.reshape(-1, 4)\n",
        "    y_teste_reshape = y_teste.reshape(-1, 4)\n",
        "\n",
        "    y_previsto_original = scaler_precos.inverse_transform(y_previsto_reshape)\n",
        "    y_teste_original = scaler_precos.inverse_transform(y_teste_reshape)\n",
        "\n",
        "    df_previsto = pd.DataFrame(y_previsto_original, columns=colunas_precos)\n",
        "    df_real = pd.DataFrame(y_teste_original, columns=colunas_precos)\n",
        "\n",
        "    comparacao = pd.DataFrame({\n",
        "        'Abertura_Real': df_real['abertura'],\n",
        "        'Abertura_Prevista': df_previsto['abertura'],\n",
        "        'Maximo_Real': df_real['maximo'],\n",
        "        'Maximo_Previsto': df_previsto['maximo'],\n",
        "        'Minimo_Real': df_real['minimo'],\n",
        "        'Minimo_Previsto': df_previsto['minimo'],\n",
        "        'Fechamento_Real': df_real['fechamento'],\n",
        "        'Fechamento_Previsto': df_previsto['fechamento']\n",
        "    })\n",
        "\n",
        "    print(\"\\nComparação de previsões (valores reais):\")\n",
        "    print(comparacao.head(10))\n",
        "\n",
        "    def calcular_metricas(y_real, y_previsto, nome):\n",
        "        mae = mean_absolute_error(y_real, y_previsto)\n",
        "        mse = mean_squared_error(y_real, y_previsto)\n",
        "        r2 = r2_score(y_real, y_previsto)\n",
        "        print(f\"{nome} - MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "    print(\"\\n Métricas de desempenho por coluna:\")\n",
        "    calcular_metricas(df_real['abertura'], df_previsto['abertura'], \"Abertura\")\n",
        "    calcular_metricas(df_real['maximo'], df_previsto['maximo'], \"Máximo\")\n",
        "    calcular_metricas(df_real['minimo'], df_previsto['minimo'], \"Mínimo\")\n",
        "    calcular_metricas(df_real['fechamento'], df_previsto['fechamento'], \"Fechamento\")\n",
        "\n",
        "    return df_real, df_previsto, comparacao\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Importando scripts de modelo e dados...\")\n",
        "    from tensorflow.keras.models import load_model\n",
        "    from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "\n",
        "    print(\"Preparando dados para avaliação...\")\n",
        "    X_treino, X_teste, y_treino, y_teste = preparar_dados_lstm(\n",
        "        path_dados='/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv',\n",
        "        tam_seq=96,\n",
        "        tx_treino=0.8\n",
        "    )\n",
        "\n",
        "    print(\"📡 Carregando modelo salvo...\")\n",
        "    modelo_lstm_v1 = load_model('/content/Piloto_Day_Trade/models/LSTM/modelo_LSTM_v1.keras')\n",
        "\n",
        "    print(\"Avaliando modelo...\")\n",
        "    df_real, df_previsto, comparacao = avaliar_modelo_lstm(\n",
        "        modelo=modelo_lstm_v1,\n",
        "        X_teste=X_teste,\n",
        "        y_teste=y_teste\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FOx4USVH-3l",
        "outputId": "3f11c884-e9e9-46e8-d3b9-c94754882639"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Piloto_Day_Trade/scripts/modelagem_machine_learning/calcular_metricas_avaliar_modelo_LSTM.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Avaliar o modelo LSTM v1 treinado\n",
        "from scripts.modelagem_machine_learning.calcular_metricas_avaliar_modelo_LSTM import avaliar_modelo_lstm\n",
        "from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Preparar os dados\n",
        "X_treino, X_teste, y_treino, y_teste = preparar_dados_lstm(\n",
        "    path_dados='/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv',\n",
        "    tam_seq=96,\n",
        "    tx_treino=0.8\n",
        ")\n",
        "\n",
        "# Carregar o modelo salvo\n",
        "modelo_lstm_v1 = load_model('/content/Piloto_Day_Trade/models/LSTM/modelo_LSTM_v1.keras')\n",
        "\n",
        "# Avaliar o modelo\n",
        "df_real, df_previsto, comparacao = avaliar_modelo_lstm(\n",
        "    modelo=modelo_lstm_v1,\n",
        "    X_teste=X_teste,\n",
        "    y_teste=y_teste,\n",
        "    caminho_scaler='/content/Piloto_Day_Trade/models/LSTM/scalers/scaler_normalizacao_preco.pkl'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOeVxRPDI-hl",
        "outputId": "447104a0-413a-4500-f0d8-4df5d0ca3b84"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados preparados salvos em: /content/Piloto_Day_Trade/data/transformed/dados_preparados_para_modelagem.csv\n",
            "Realizando previsões...\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step\n",
            "Carregando scaler de preços para inversão...\n",
            "\n",
            "Comparação de previsões (valores reais):\n",
            "   Abertura_Real  Abertura_Prevista  Maximo_Real  Maximo_Previsto  \\\n",
            "0          11.35          11.770454        11.37        11.856690   \n",
            "1          11.36          11.607411        11.37        11.674302   \n",
            "2          11.35          11.472589        11.36        11.518727   \n",
            "3          11.36          11.380248        11.37        11.402222   \n",
            "4          11.36          11.400326        11.38        11.411223   \n",
            "5          11.34          11.424469        11.35        11.433488   \n",
            "6          11.33          11.449021        11.34        11.458927   \n",
            "7          11.33          11.460655        11.37        11.472838   \n",
            "8          11.35          11.467196        11.42        11.481082   \n",
            "9          11.34          11.471559        11.41        11.487149   \n",
            "\n",
            "   Minimo_Real  Minimo_Previsto  Fechamento_Real  Fechamento_Previsto  \n",
            "0        11.33        11.740356            11.35            11.848277  \n",
            "1        11.35        11.585472            11.35            11.671049  \n",
            "2        11.34        11.444901            11.36            11.509893  \n",
            "3        11.35        11.339261            11.36            11.380775  \n",
            "4        11.34        11.351844            11.35            11.384187  \n",
            "5        11.32        11.388720            11.33            11.419835  \n",
            "6        11.33        11.421501            11.34            11.449255  \n",
            "7        11.33        11.436099            11.35            11.469912  \n",
            "8        11.34        11.442727            11.34            11.481201  \n",
            "9        11.34        11.447119            11.39            11.488998  \n",
            "\n",
            " Métricas de desempenho por coluna:\n",
            "Abertura - MAE: 0.1377, MSE: 0.0286, R²: -0.1708\n",
            "Máximo - MAE: 0.1356, MSE: 0.0272, R²: -0.1123\n",
            "Mínimo - MAE: 0.1377, MSE: 0.0285, R²: -0.1751\n",
            "Fechamento - MAE: 0.1347, MSE: 0.0268, R²: -0.0979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Finalizando Modelo LSTM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YGdD_mkKPCK",
        "outputId": "d5bad5a5-4f4b-4056-cf1a-eace442e39f3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado inesperado do Git:\n",
            "[main 5c324f6] Finalizando Modelo LSTM\n",
            " 1 file changed, 2545 insertions(+)\n",
            " create mode 100644 data/raw/dados_brutos_teste.csv\n",
            "\n",
            "Repositório atualizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline de dados"
      ],
      "metadata": {
        "id": "TTzGcdW3uxIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def listar_arquivos_em_subpastas(pasta_base):\n",
        "    for raiz, subpastas, arquivos in os.walk(pasta_base):\n",
        "        nivel = raiz.replace(pasta_base, '').count(os.sep)\n",
        "        indent = ' ' * 4 * nivel\n",
        "        print(f\"{indent}{os.path.basename(raiz)}/\")\n",
        "        subindent = ' ' * 4 * (nivel + 1)\n",
        "        for arquivo in arquivos:\n",
        "            print(f\"{subindent}{arquivo}\")\n",
        "\n",
        "# Caminho da pasta onde estão os scripts\n",
        "pasta_scripts = \"/content/Piloto_Day_Trade/scripts\"\n",
        "listar_arquivos_em_subpastas(pasta_scripts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5_tfm2NxWQy",
        "outputId": "6a1062ac-6bb9-4082-c3b5-bce29374bdbc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scripts/\n",
            "    modelagem_machine_learning/\n",
            "        calcular_metricas_avaliar_modelo_LSTM.py\n",
            "        preparar_dados_modelagem_LSTM.py\n",
            "        criar_modelo_LSTM.py\n",
            "        modelo_LSTM_v1.py\n",
            "        __pycache__/\n",
            "            calcular_metricas_avaliar_modelo_LSTM.cpython-311.pyc\n",
            "            criar_modelo_LSTM.cpython-311.pyc\n",
            "            preparar_dados_modelagem_LSTM.cpython-311.pyc\n",
            "    .ipynb_checkpoints/\n",
            "        modelagem_dados_XGBoot.py\n",
            "    __pycache__/\n",
            "        extracao_dados_v2.cpython-311.pyc\n",
            "        limpeza_dados.cpython-311.pyc\n",
            "        limpeza_basica_dadosv2.cpython-311.pyc\n",
            "        gerar_catalogo.cpython-311.pyc\n",
            "        transformacao_dados_v2.cpython-311.pyc\n",
            "        gerar_catalogo_dados.cpython-311.pyc\n",
            "        transformacao_dados.cpython-311.pyc\n",
            "        calcular_metricas_avaliar_modelo.cpython-311.pyc\n",
            "        preparar_dados_modelagem_LSTM.cpython-311.pyc\n",
            "        extracao_dados.cpython-311.pyc\n",
            "    pipeline/\n",
            "        gerar_catalogo_dados.py\n",
            "        extracao_dados.py\n",
            "        executar_pipeline_completo.py\n",
            "        limpeza_dados.py\n",
            "        criar_banco_dimensional.py\n",
            "        transformacao_dados.py\n",
            "        carga_dados.py\n",
            "        __pycache__/\n",
            "            limpeza_dados.cpython-311.pyc\n",
            "            executar_pipeline_completo.cpython-311.pyc\n",
            "            transformacao_dados.cpython-311.pyc\n",
            "            criar_banco_dimensional.cpython-311.pyc\n",
            "            carga_dados.cpython-311.pyc\n",
            "            extracao_dados.cpython-311.pyc\n",
            "    operacional/\n",
            "        configurar_git.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/pipeline/executar_pipeline.py\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from scripts.pipeline.extracao_dados import extrair_dados\n",
        "from scripts.pipeline.limpeza_dados import limpeza_dados\n",
        "from scripts.pipeline.transformacao_dados import transformar_dados\n",
        "from scripts.pipeline.carga_dados import carregar_dados\n",
        "from scripts.pipeline.criar_banco_dimensional import criar_banco\n",
        "from scripts.modelagem_machine_learning.preparar_dados_modelagem_LSTM import preparar_dados_lstm\n",
        "\n",
        "def executar_pipeline(ticker, intervalo, dias, caminho_bruto, caminho_limpo, caminho_transformado, db_path, tam_seq, tx_treino):\n",
        "    print(\"\\nIniciando execução completa do pipeline...\")\n",
        "\n",
        "    # Etapa 1: Extração\n",
        "    print(\"Executando: Extração de dados\")\n",
        "    extrair_dados(ticker, dias, intervalo, caminho_bruto)\n",
        "\n",
        "    # Etapa 2: Limpeza\n",
        "    print(\"Executando: Limpeza de dados\")\n",
        "    df_bruto = pd.read_csv(caminho_bruto, index_col=0, parse_dates=True, dayfirst=True)\n",
        "    limpeza_dados(df_bruto, caminho_limpo)\n",
        "\n",
        "    # Etapa 3: Transformação\n",
        "    print(\"Executando: Transformação de dados\")\n",
        "    transformar_dados(caminho_limpo, caminho_transformado)\n",
        "\n",
        "    # Etapa 4: Criar banco dimensional\n",
        "    print(\"Executando: Criação do banco dimensional\")\n",
        "    criar_banco(db_path)  # Passando db_path para a função\n",
        "\n",
        "    # Etapa 5: Carga de dados\n",
        "    print(\"Executando: Carga de dados\")\n",
        "    df_transformado = pd.read_csv(caminho_transformado)\n",
        "    carregar_dados(df_transformado)\n",
        "\n",
        "    # Etapa 7: Preparação dos dados para modelagem\n",
        "    print(\"Executando: Preparação de dados para LSTM\")\n",
        "    preparar_dados_lstm(\n",
        "        path_dados=caminho_transformado,\n",
        "        tam_seq=tam_seq,\n",
        "        tx_treino=tx_treino\n",
        "    )\n",
        "\n",
        "    print(\"\\nPipeline finalizado com sucesso.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Chamada com parâmetros do projeto\n",
        "    ticker = \"BBDC4.SA\"\n",
        "    intervalo = \"5m\"\n",
        "    dias = 45\n",
        "    caminho_bruto = \"/content/Piloto_Day_Trade/data/raw/dados_brutos.csv\"\n",
        "    caminho_limpo = \"/content/Piloto_Day_Trade/data/cleaned/dados_limpos.csv\"\n",
        "    caminho_transformado = \"/content/Piloto_Day_Trade/data/transformed/dados_transformados.csv\"\n",
        "    db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional.db\"\n",
        "    tam_seq = 96\n",
        "    tx_treino = 0.8\n",
        "\n",
        "    executar_pipeline(ticker, intervalo, dias, caminho_bruto, caminho_limpo, caminho_transformado, db_path, tam_seq, tx_treino)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tHLaJNGMGuL",
        "outputId": "f2948b30-c834-4dc7-9ba9-a5b4ca4e7c36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/Piloto_Day_Trade/scripts/pipeline/executar_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Testar pipeline\n",
        "\n",
        "# Importando a função do pipeline\n",
        "from Piloto_Day_Trade.scripts.pipeline.executar_pipeline import executar_pipeline\n",
        "\n",
        "# Definindo os parâmetros a serem passados\n",
        "ticker = \"BBDC4.SA\"\n",
        "intervalo = \"5m\"\n",
        "dias = 45\n",
        "caminho_bruto = \"/content/Piloto_Day_Trade/data/raw/dados_brutos_teste.csv\"\n",
        "caminho_limpo = \"/content/Piloto_Day_Trade/data/cleaned/dados_limpos_teste.csv\"\n",
        "caminho_transformado = \"/content/Piloto_Day_Trade/data/transformed/dados_transformados_teste.csv\"\n",
        "db_path = \"/content/Piloto_Day_Trade/modelagem/database/banco_dimensional_teste.db\"\n",
        "tam_seq = 96\n",
        "tx_treino = 0.8\n",
        "\n",
        "# Executando o pipeline com os parâmetros definidos\n",
        "executar_pipeline(ticker, intervalo, dias, caminho_bruto, caminho_limpo, caminho_transformado, db_path, tam_seq, tx_treino)\n"
      ],
      "metadata": {
        "id": "7rxNatUoW_7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/.github/workflows/pipeline.yml\n",
        "\n",
        "name: Executar Pipeline Completo\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [main]\n",
        "  pull_request:\n",
        "    branches: [main]\n",
        "\n",
        "jobs:\n",
        "  pipeline:\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    steps:\n",
        "      - name: Clonar repositório\n",
        "        uses: actions/checkout@v3\n",
        "\n",
        "      - name: Configurar Python\n",
        "        uses: actions/setup-python@v4\n",
        "        with:\n",
        "          python-version: '3.11'\n",
        "\n",
        "      - name: Instalar dependências\n",
        "        run: |\n",
        "          python -m pip install --upgrade pip\n",
        "          pip install -r requirements.txt\n",
        "\n",
        "      - name: Executar pipeline\n",
        "        run: |\n",
        "          python scripts/pipeline/executar_pipeline_completo.py\n"
      ],
      "metadata": {
        "id": "j7_m_o8cXdlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/.github/workflows/workflow_doc.md\n",
        "\n",
        "#@title Definindo Documentação do workflow completo para Execução do Pipeline\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Este documento descreve o fluxo de trabalho para a execução automatizada do pipeline de dados, utilizando o GitHub Actions. O pipeline abrange desde a extração até a modelagem de dados, e foi estruturado para garantir a automação da execução do processo, facilitando atualizações contínuas e execuções programadas no repositório GitHub.\n",
        "\n",
        "## Estrutura do Pipeline\n",
        "\n",
        "O pipeline é composto por várias etapas que são executadas em sequência. Cada uma das etapas envolve um script específico para garantir a correta manipulação dos dados:\n",
        "\n",
        "1. **Extração de Dados:** Obtém os dados brutos de uma fonte externa, como o Yahoo Finance ou qualquer outra fonte configurada.\n",
        "2. **Limpeza de Dados:** Realiza a limpeza e formatação dos dados brutos.\n",
        "3. **Transformação de Dados:** Aplica transformações necessárias para deixar os dados prontos para a modelagem.\n",
        "4. **Criação de Banco Dimensional:** Estrutura os dados de maneira que sejam facilmente analisáveis.\n",
        "5. **Carga de Dados:** Carrega os dados transformados para o banco de dados.\n",
        "6. **Geração de Catálogo de Dados:** Cria um catálogo de metadados para facilitar o uso futuro dos dados.\n",
        "7. **Preparação de Dados para Modelagem LSTM:** Prepara os dados específicos para alimentar o modelo de LSTM.\n",
        "8. **Modelagem e Avaliação:** Treina e avalia o modelo LSTM.\n",
        "\n",
        "### Scripts Responsáveis por Cada Etapa\n",
        "\n",
        "1. **`extracao_dados.py`:** Extração dos dados brutos.\n",
        "2. **`limpeza_dados.py`:** Limpeza e pré-processamento dos dados brutos.\n",
        "3. **`transformacao_dados.py`:** Aplicação das transformações necessárias nos dados.\n",
        "4. **`criar_banco_dimensional.py`:** Criação do banco dimensional para armazenar os dados.\n",
        "5. **`carga_dados.py`:** Carga dos dados transformados no banco dimensional.\n",
        "6. **`gerar_catalogo_dados.py`:** Geração do catálogo de dados.\n",
        "7. **`preparar_dados_modelagem_LSTM.py`:** Preparação final dos dados para treinamento do modelo LSTM.\n",
        "\n",
        "## GitHub Actions: Workflow\n",
        "\n",
        "O objetivo é configurar um workflow automatizado no GitHub Actions para que ele execute todo o pipeline a cada novo push ou evento programado. Para isso, criamos um arquivo YAML no repositório do GitHub.\n",
        "\n",
        "### Workflow: `pipeline.yml`\n",
        "\n",
        "Este workflow será responsável por orquestrar todas as etapas de execução. Abaixo está o conteúdo do arquivo YAML:\n",
        "\n",
        "```yaml\n",
        "name: Pipeline Completo de Dados\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches:\n",
        "      - main\n",
        "  schedule:\n",
        "    - cron: '0 0 * * 1'  # Executa toda segunda-feira às 00:00 (UTC)\n",
        "\n",
        "jobs:\n",
        "  run_pipeline:\n",
        "    runs-on: ubuntu-latest\n",
        "\n",
        "    steps:\n",
        "    - name: Checkout do repositório\n",
        "      uses: actions/checkout@v2\n",
        "\n",
        "    - name: Configurar Python\n",
        "      uses: actions/setup-python@v2\n",
        "      with:\n",
        "        python-version: '3.8'\n",
        "\n",
        "    - name: Instalar dependências\n",
        "      run: |\n",
        "        python -m pip install --upgrade pip\n",
        "        pip install -r requirements.txt\n",
        "\n",
        "    - name: Executar Extração de Dados\n",
        "      run: python scripts/pipeline/extracao_dados.py\n",
        "\n",
        "    - name: Executar Limpeza de Dados\n",
        "      run: python scripts/pipeline/limpeza_dados.py\n",
        "\n",
        "    - name: Executar Transformação de Dados\n",
        "      run: python scripts/pipeline/transformacao_dados.py\n",
        "\n",
        "    - name: Criar Banco Dimensional\n",
        "      run: python scripts/pipeline/criar_banco_dimensional.py\n",
        "\n",
        "    - name: Carregar Dados\n",
        "      run: python scripts/pipeline/carga_dados.py\n",
        "\n",
        "    - name: Gerar Catálogo de Dados\n",
        "      run: python scripts/pipeline/gerar_catalogo_dados.py\n",
        "\n",
        "    - name: Preparar Dados para Modelagem LSTM\n",
        "      run: python scripts/modelagem_machine_learning/preparar_dados_modelagem_LSTM.py\n",
        "\n",
        "    - name: Avaliar Modelo LSTM\n",
        "      run: python scripts/modelagem_machine_learning/calcular_metricas_avaliar_modelo_LSTM.py\n",
        "\n",
        "\n",
        "  ### Explicação do Workflow:\n",
        "\n",
        "    - Evento de Acionamento:\n",
        "\n",
        "    O workflow é acionado por dois eventos principais:\n",
        "\n",
        "    Push para a branch main: Sempre que um novo commit for enviado para a branch main, o pipeline será executado automaticamente.\n",
        "\n",
        "    Agendamento Semanal: O pipeline é executado toda segunda-feira às 00:00 UTC, garantindo que os dados sejam atualizados regularmente.\n",
        "\n",
        "    - Jobs:\n",
        "\n",
        "    run_pipeline: Este job é o responsável por executar todas as etapas do pipeline.\n",
        "\n",
        "    Ele é executado em uma máquina virtual Ubuntu, configurada com Python 3.8.\n",
        "\n",
        "    - Etapas:\n",
        "\n",
        "    Checkout do Repositório: Faz o checkout do código do repositório.\n",
        "\n",
        "    Configuração do Python: Configura o ambiente Python necessário.\n",
        "\n",
        "    Instalação de Dependências: Instala as dependências listadas no requirements.txt.\n",
        "\n",
        "    Execução das Etapas do Pipeline: Cada etapa do pipeline é executada com o comando python apontando para o script correspondente.\n",
        "\n",
        "    - Requisitos para o Workflow:\n",
        "    requirements.txt: Um arquivo contendo todas as dependências necessárias para rodar o pipeline. Ele deve estar no repositório e ser mantido atualizado.\n",
        "\n",
        "    Acesso ao Repositório: O repositório deve conter todos os scripts e arquivos de dados necessários para a execução do pipeline.\n"
      ],
      "metadata": {
        "id": "5OPmFsn9Y4wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atualizar_repo(\"Finalizando Pipeline\")"
      ],
      "metadata": {
        "id": "xZi27YhVdwWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}