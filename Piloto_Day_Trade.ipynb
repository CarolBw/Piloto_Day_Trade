{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlFwiMPy5sk+LgCrIcWsD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarolBw/Piloto_Day_Trade/blob/main/Piloto_Day_Trade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No6u9fG0h8-A",
        "outputId": "71139b34-4ea7-4425-fe89-18c771470485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/.env\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/.env\n",
        "\n",
        "PROJECT_NAME=Piloto_Day_Trade\n",
        "\n",
        "# Vari√°veis de ambiente para o Github\n",
        "GITHUB_USERNAME=CarolBw\n",
        "GITHUB_TOKEN =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\n",
        "EMAIL=carolbrescowitt@yahoo.com.br\n",
        "\n",
        "# Vari√°veis de ambiente para o Google Colab\n",
        "db_name=piloto_database\n",
        "db_path=/content/Piloto_Day_Trade/data/piloto_database.db\n",
        "\n",
        "# Variaveis de ambientepara coleta de dados e processamento de dados\n",
        "\n",
        "dados_brutos=/content/Piloto_Day_Trade/data/dados_brutos.csv\n",
        "dados_limpos=/content/Piloto_Day_Trade/data/dados_limpos.csv\n",
        "dados_transformados=/content/Piloto_Day_Trade/data/dados_transformados.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando depend√™ncias necess√°rias\n",
        "'''\n",
        "Usamos `-q` para ocultar a sa√≠da detalhada e mostrar apenas a barra de progresso\n",
        "\n",
        "'''\n",
        "!pip install -q tensorflow > /dev/null  # Framework para redes neurais e deep learning\n",
        "!pip install -q keras > /dev/null  # Biblioteca de alto n√≠vel para redes neurais\n",
        "!pip install -q pandas > /dev/null  # Manipula√ß√£o e an√°lise de dados\n",
        "!pip install -q numpy > /dev/null  # Computa√ß√£o num√©rica eficiente\n",
        "!pip install -q matplotlib > /dev/null  # Visualiza√ß√£o de gr√°ficos e an√°lise explorat√≥ria\n",
        "!pip install -q scikit-learn > /dev/null  # Ferramentas para pr√©-processamento e m√©tricas de avalia√ß√£o\n",
        "!pip install -q gitpython > /dev/null  # Gerenciamento de reposit√≥rios Git via Python\n",
        "!pip install -q python-dotenv > /dev/null  # Manipula√ß√£o de vari√°veis de ambiente\n",
        "!pip install -q seaborn > /dev/null  # Biblioteca de visualiza√ß√£o estat√≠stica baseada no Matplotlib\n",
        "!pip install -q yfinance > /dev/null  # Coleta de dados financeiros diretamente do Yahoo Finance\n",
        "!pip install -q sqlalchemy > /dev/null  # ORM para interagir com bancos de dados relacionais\n",
        "!pip install -q dotenv > /dev/null # Manipula√ß√£o de vari√°veis de ambiente\n",
        "\n",
        "# Importa√ß√µes das bibliotecas\n",
        "import pandas as pd  # Manipula√ß√£o de DataFrames\n",
        "import numpy as np  # C√°lculos num√©ricos e matrizes\n",
        "import matplotlib.pyplot as plt  # Gera√ß√£o de gr√°ficos\n",
        "import sqlite3  # Integra√ß√£o com banco de dados SQLite\n",
        "\n",
        "# Pr√©-processamento dos dados\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler  # Normaliza√ß√£o e padroniza√ß√£o dos dados\n",
        "from sklearn.model_selection import train_test_split  # Divis√£o dos dados em treino e teste\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error  # Avalia√ß√£o do desempenho do modelo\n",
        "\n",
        "# Constru√ß√£o do modelo preditivo\n",
        "from keras.models import Sequential  # Modelo sequencial de rede neural\n",
        "from keras.layers import Dense  # Camada densa para aprendizado profundo\n",
        "\n",
        "# Controle de vers√£o e vari√°veis de ambiente\n",
        "import git  # Gerenciamento do reposit√≥rio Git\n",
        "import dotenv  # Carregamento de vari√°veis de ambiente\n",
        "\n",
        "# Carregar vari√°veis de ambiente\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Capturamdo todas as dependencias do ambiente nesta primeira etapa\n",
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "LWOFHzJ6IupA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile /content/configurar_git.py\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path='/content/.env')\n",
        "\n",
        "def git_config():\n",
        "    \"\"\"Configura o Git localmente e sincroniza com o reposit√≥rio remoto no GitHub.\"\"\"\n",
        "\n",
        "    # Carregar vari√°veis de ambiente do arquivo .env\n",
        "    load_dotenv(dotenv_path='/content/.env')\n",
        "\n",
        "    # Obter as vari√°veis de ambiente do .env para o GitHub\n",
        "    GITHUB_USERNAME = os.getenv('GITHUB_USERNAME')\n",
        "    EMAIL = os.getenv('EMAIL')\n",
        "    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
        "    PROJECT_NAME = os.getenv('PROJECT_NAME')\n",
        "    REPO_URL = f\"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{PROJECT_NAME}.git\"\n",
        "\n",
        "    # Configurar o Git localmente com as credenciais\n",
        "    os.system(f'git config --global user.name \"{GITHUB_USERNAME}\"')\n",
        "    os.system(f'git config --global user.email \"{EMAIL}\"')\n",
        "\n",
        "    # Verificar se o diret√≥rio do projeto j√° existe e se √© um reposit√≥rio Git v√°lido\n",
        "    if os.path.isdir(PROJECT_NAME):\n",
        "        print(f\"O diret√≥rio '{PROJECT_NAME}' j√° existe. Entrando no diret√≥rio e sincronizando...\")\n",
        "\n",
        "        os.chdir(PROJECT_NAME)  # Entrar na pasta do projeto\n",
        "\n",
        "        # Garantir que estamos na branch main\n",
        "        os.system(\"git branch -M main\")\n",
        "\n",
        "        # Remover qualquer configura√ß√£o errada do reposit√≥rio remoto e adicionar novamente\n",
        "        os.system(\"git remote remove origin\")\n",
        "        os.system(\"git remote add origin \" + REPO_URL)\n",
        "\n",
        "        # Puxar as √∫ltimas atualiza√ß√µes do GitHub, tratando hist√≥ricos n√£o relacionados\n",
        "        os.system(\"git pull origin main --allow-unrelated-histories --no-rebase\")\n",
        "    else:\n",
        "        print(f\"Clonando o reposit√≥rio '{PROJECT_NAME}'...\")\n",
        "\n",
        "        # Clonar o reposit√≥rio remoto\n",
        "        os.system(f\"git clone {REPO_URL}\")\n",
        "        os.chdir(PROJECT_NAME)  # Entrar no diret√≥rio ap√≥s o clone\n",
        "\n",
        "        # Inicializar o reposit√≥rio Git local (se necess√°rio) e configurar remoto\n",
        "        os.system(\"git branch -M main\")\n",
        "        os.system(\"git remote add origin \" + REPO_URL)\n",
        "\n",
        "        # Realizar o pull inicial para garantir que a branch main est√° sincronizada\n",
        "        os.system(\"git pull origin main --allow-unrelated-histories --no-rebase\")\n",
        "\n",
        "    print(f\"‚úÖ Configura√ß√£o do Git conclu√≠da e sincronizada com a branch main do reposit√≥rio{REPO_URL}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    git_config()\n",
        "\n",
        "\n",
        "'''\n",
        "# Executar a partir de arquivo .py\n",
        "from configurar_git import git_config\n",
        "python configurar_git.py\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "zPIufIwbI0JW",
        "outputId": "a41b1069-4bc8-476e-fb79-3bce33beda86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clonando o reposit√≥rio 'Piloto_Day_Trade'...\n",
            "‚úÖ Configura√ß√£o do Git conclu√≠da e sincronizada com a branch main do reposit√≥riohttps://CarolBw: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@github.com/CarolBw/Piloto_Day_Trade.git\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Executar a partir de arquivo .py\\nfrom configurar_git import git_config\\npython configurar_git.py\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a estrutura de pastas inicial do projeto\n",
        "\n",
        "'''\n",
        "Piloto_Day_Trade/\n",
        "‚îÇ‚îÄ‚îÄ data/                # Armazena os dados coletados\n",
        "‚îÇ‚îÄ‚îÄ models/              # Modelos treinados e checkpoints\n",
        "‚îÇ‚îÄ‚îÄ scripts/             # C√≥digos Python para automa√ß√£o\n",
        "‚îÇ‚îÄ‚îÄ reports/             # An√°lises, gr√°ficos e relat√≥rios\n",
        "‚îÇ‚îÄ‚îÄ .gitignore           # Arquivos a serem ignorados pelo Git\n",
        "‚îÇ‚îÄ‚îÄ README.md            # Documenta√ß√£o do projeto\n",
        "\n",
        "'''\n",
        "\n",
        "# Criar as pastas dentro do reposit√≥rio\n",
        "!mkdir -p /content/Piloto_Day_Trade/notebooks\n",
        "!mkdir -p /content/Piloto_Day_Trade/models\n",
        "!mkdir -p /content/Piloto_Day_Trade/scripts\n",
        "!mkdir -p /content/Piloto_Day_Trade/reports\n",
        "!mkdir -p /content/Piloto_Day_Trade/workflows\n",
        "!mkdir -p /content/Piloto_Day_Trade/data\n",
        "\n",
        "# Criar arquivos vazios\n",
        "!touch /content/Piloto_Day_Trade/.gitignore\n",
        "!touch /content/Piloto_Day_Trade/README.md\n",
        "\n",
        "# Mover .env para dentro do projeto\n",
        "!mv /content/.env /content/Piloto_Day_Trade/.env\n",
        "!mv /content/configurar_git.py /content/Piloto_Day_Trade/scripts/configurar_git.py\n",
        "!mv /content/requirements.txt /content/Piloto_Day_Trade/requirements.txt"
      ],
      "metadata": {
        "id": "vVkhl3kbZKbs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coleta de dados"
      ],
      "metadata": {
        "id": "BRXHqSGEMjIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile /content/Piloto_Day_Trade/scripts/extracao_dados_v2.py\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "def extrair_dados(ticker, intervalo, dias, dados_brutos):\n",
        "    \"\"\"Extrai e organiza dados do Yahoo Finance no intervalo correto.\"\"\"\n",
        "\n",
        "    df_total = pd.DataFrame()  # DataFrame para armazenar os dados\n",
        "    data_inicio = datetime.today() - timedelta(days=dias)  # Data inicial\n",
        "    data_fim = datetime.today()  # Data final (hoje)\n",
        "\n",
        "    # Verifica se o arquivo de dados brutos existe\n",
        "    if os.path.exists(dados_brutos):\n",
        "        df = pd.read_csv(dados_brutos, index_col=0, parse_dates=True, )\n",
        "\n",
        "        # Garante que a data √© v√°lida\n",
        "\n",
        "        if not df.empty:\n",
        "            # Atualiza a data de in√≠cio para a √∫ltima data dispon√≠vel nos dados brutos\n",
        "            data_inicio = pd.to_datetime(df.index.max()) + timedelta(minutes=5)\n",
        "\n",
        "    print(f\"üîÑ Extraindo dados de {data_inicio} at√© {data_fim}\")\n",
        "\n",
        "    # Extrai os dados do Yahoo Finance\n",
        "    df_novo = yf.download(ticker, start=data_inicio.strftime(\"%Y-%m-%d\"),\n",
        "                          end=data_fim.strftime(\"%Y-%m-%d\"), interval=intervalo, progress=True)\n",
        "\n",
        "    if not df_novo.empty:\n",
        "        # Ajusta o fuso hor√°rio dos dados para \"America/Sao_Paulo\"\n",
        "        df_novo.index = df_novo.index.tz_convert(\"America/Sao_Paulo\")\n",
        "\n",
        "        # Concatena os novos dados com os existentes e remove duplicatas\n",
        "        df_total = pd.concat([df_total, df_novo])\n",
        "        df_total = df_total[~df_total.index.duplicated(keep='last')].sort_index()\n",
        "\n",
        "        # Remove linhas com mais de 50% de valores nulos\n",
        "        df_total = df_total.dropna(thresh=df_total.shape[1] * 0.5)\n",
        "\n",
        "        # Salva os dados atualizados no arquivo CSV\n",
        "        df_total.to_csv(dados_brutos)\n",
        "        print(\"‚úÖ Dados salvos com sucesso.\")\n",
        "\n",
        "    # Filtra os dados para o hor√°rio entre 10:00 e 18:00\n",
        "    df_filtrado = df_total.between_time(\"10:00\", \"18:00\")\n",
        "\n",
        "    # Exibe os 10 primeiros e os 10 √∫ltimos registros\n",
        "    print(\"√öltimos 10 dados filtrados:\")\n",
        "    print(df_filtrado.tail(10))\n",
        "    print(\"Primeiros 10 dados filtrados:\")\n",
        "    print(df_filtrado.head(10))\n",
        "\n",
        "    return df_filtrado\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ticker = \"BBDC4.SA\"  # Ticker da a√ß√£o\n",
        "    intervalo = \"5m\"  # Intervalo de tempo (5 minutos)\n",
        "    dias = 45  # N√∫mero de dias a partir de hoje para buscar os dados\n",
        "    dados_brutos = \"/content/Piloto_Day_Trade/data/dados_brutos2.csv\"  # Caminho do arquivo de dados brutos\n",
        "    df = extrair_dados(ticker, intervalo, dias, dados_brutos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3JYaCsNw5fQ",
        "outputId": "7b908aa3-ac60-486c-fca9-b91c79f56a1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Extraindo dados de 2025-02-12 18:54:47.644004 at√© 2025-03-29 18:54:47.644024\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dados salvos com sucesso.\n",
            "√öltimos 10 dados filtrados:\n",
            "Price                        Close     High      Low     Open   Volume\n",
            "Ticker                    BBDC4.SA BBDC4.SA BBDC4.SA BBDC4.SA BBDC4.SA\n",
            "Datetime                                                              \n",
            "2025-03-28 16:05:00-03:00    12.86    12.86    12.84    12.85   220800\n",
            "2025-03-28 16:10:00-03:00    12.85    12.86    12.85    12.86    88000\n",
            "2025-03-28 16:15:00-03:00    12.84    12.86    12.83    12.86   444200\n",
            "2025-03-28 16:20:00-03:00    12.84    12.86    12.83    12.84   146900\n",
            "2025-03-28 16:25:00-03:00    12.86    12.87    12.84    12.85   307100\n",
            "2025-03-28 16:30:00-03:00    12.87    12.87    12.84    12.86   311800\n",
            "2025-03-28 16:35:00-03:00    12.88    12.89    12.87    12.87   307600\n",
            "2025-03-28 16:40:00-03:00    12.88    12.90    12.88    12.89   382800\n",
            "2025-03-28 16:45:00-03:00    12.88    12.90    12.88    12.89   617700\n",
            "2025-03-28 16:50:00-03:00    12.89    12.89    12.87    12.88   494000\n",
            "Primeiros 10 dados filtrados:\n",
            "Price                        Close     High      Low     Open   Volume\n",
            "Ticker                    BBDC4.SA BBDC4.SA BBDC4.SA BBDC4.SA BBDC4.SA\n",
            "Datetime                                                              \n",
            "2025-02-12 10:05:00-03:00    11.97    12.05    11.92    12.05   272800\n",
            "2025-02-12 10:10:00-03:00    11.90    11.98    11.87    11.98  2343400\n",
            "2025-02-12 10:15:00-03:00    11.90    11.91    11.86    11.90  1737500\n",
            "2025-02-12 10:20:00-03:00    11.89    11.94    11.88    11.91   932000\n",
            "2025-02-12 10:25:00-03:00    11.85    11.91    11.85    11.91  1095900\n",
            "2025-02-12 10:30:00-03:00    11.78    11.85    11.74    11.85  2582200\n",
            "2025-02-12 10:35:00-03:00    11.76    11.78    11.72    11.77  1733700\n",
            "2025-02-12 10:40:00-03:00    11.74    11.76    11.72    11.74  1404100\n",
            "2025-02-12 10:45:00-03:00    11.76    11.81    11.73    11.74  2520000\n",
            "2025-02-12 10:50:00-03:00    11.76    11.78    11.74    11.76   987200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpeza de dados\n"
      ],
      "metadata": {
        "id": "dornsMZJT65c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile /content/Piloto_Day_Trade/scripts/limpeza_basica_dadosv2.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def limpeza_basica_dados(df):\n",
        "    # Verificar se os dados est√£o corretos\n",
        "    print(\"Dados originais:\")\n",
        "    print(df.head())\n",
        "    print(df.info())\n",
        "\n",
        "    # Remover as primeiras duas linhas (com 'Ticker' e 'Datetime')\n",
        "    df = df.iloc[2:].copy()\n",
        "\n",
        "    # Verificar ap√≥s a remo√ß√£o das linhas iniciais\n",
        "    print(\"Ap√≥s remo√ß√£o das duas primeiras linhas:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Garantir que o √≠ndice esteja no formato de data e hora (timezone UTC)\n",
        "    df.index = pd.to_datetime(df.index, utc=True)\n",
        "\n",
        "    # Definir o fuso hor√°rio como \"America/Sao_Paulo\"\n",
        "    df.index = df.index.tz_convert(\"America/Sao_Paulo\")\n",
        "\n",
        "    # Remover a refer√™ncia de fuso hor√°rio (deixar o hor√°rio local sem informa√ß√£o de timezone)\n",
        "    df.index = df.index.tz_localize(None)\n",
        "\n",
        "    # Criar a coluna 'hora' com base no √≠ndice\n",
        "    df['hora'] = df.index.strftime('%H:%M:%S')\n",
        "\n",
        "    # Renomear o √≠ndice para 'data'\n",
        "    df.index.name = 'data'\n",
        "\n",
        "    # Resetar o √≠ndice para transformar o Datetime em uma coluna normal\n",
        "    df = df.reset_index()\n",
        "\n",
        "    # Verificar ap√≥s a transforma√ß√£o do √≠ndice\n",
        "    print(\"Ap√≥s convers√£o de √≠ndice:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Remover o hor√°rio da coluna 'data', mantendo apenas a data\n",
        "    df['data'] = df['data'].dt.date\n",
        "\n",
        "    # Mapeamento das colunas para nomes padronizados\n",
        "    mapeamento_colunas = {\n",
        "        'Close': 'fechamento',\n",
        "        'High': 'maximo',\n",
        "        'Low': 'minimo',\n",
        "        'Open': 'abertura',\n",
        "        'Volume': 'volume'\n",
        "    }\n",
        "\n",
        "    # Renomear as colunas\n",
        "    df.rename(columns=mapeamento_colunas, inplace=True)\n",
        "\n",
        "    # Converte e arredonda as colunas num√©ricas\n",
        "    for col in ['abertura', 'minimo', 'maximo', 'fechamento']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce').round(2)\n",
        "\n",
        "    # Converte a coluna 'volume' para n√∫mero inteiro\n",
        "    df['volume'] = pd.to_numeric(df['volume'], errors='coerce', downcast='integer')\n",
        "\n",
        "    # Reorganiza as colunas na ordem desejada\n",
        "    df = df[['data', 'hora', 'abertura', 'minimo', 'maximo', 'fechamento', 'volume']]\n",
        "\n",
        "    # Verificar ap√≥s reorganizar as colunas\n",
        "    print(\"Ap√≥s reorganizar as colunas:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Verificar e remover duplicatas mantendo a primeira ocorr√™ncia\n",
        "    df = df.drop_duplicates(keep='first')\n",
        "\n",
        "    # Remover as linhas com 50% ou mais de valores nulos\n",
        "    df = df.dropna(thresh=df.shape[1] * 0.5)\n",
        "\n",
        "    # Verificar ap√≥s remo√ß√£o de duplicatas e nulos\n",
        "    print(\"Ap√≥s remover duplicatas e nulos:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Garantir que 'data' e 'hora' estejam no formato datetime\n",
        "    df['data'] = pd.to_datetime(df['data'], format='%Y-%m-%d')\n",
        "    df['hora'] = pd.to_datetime(df['hora'], format='%H:%M:%S').dt.time\n",
        "\n",
        "    # Filtra apenas os dias √∫teis (segunda a sexta)\n",
        "    df = df[df['data'].dt.weekday < 5]\n",
        "\n",
        "    # Verificar ap√≥s filtrar dias √∫teis\n",
        "    print(\"Ap√≥s filtrar apenas os dias √∫teis:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Filtra apenas hor√°rios entre 09:55 e 18:05\n",
        "    df = df[(df['hora'] >= pd.to_datetime('09:55:00').time()) &\n",
        "            (df['hora'] <= pd.to_datetime('18:05:00').time())]\n",
        "\n",
        "    # Verificar ap√≥s filtrar o intervalo de hor√°rio\n",
        "    print(\"Ap√≥s filtrar o intervalo de hor√°rio (09:55-18:05):\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Caso o DataFrame fique vazio, informar o motivo\n",
        "    if df.empty:\n",
        "        print(\"O DataFrame ficou vazio ap√≥s o filtro de hor√°rio. Verifique se os dados est√£o dentro do intervalo de 09:55-18:05.\")\n",
        "    else:\n",
        "        print(\"Limpeza de dados conclu√≠da com sucesso.\")\n",
        "\n",
        "        # Ordenar os dados\n",
        "    df = df.sort_values([\"data\", \"hora\"], ascending=[False, True])\n",
        "    print(\"Dados limpos e ordenados\\n:\")\n",
        "    print(df.head(10))\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ler os dados brutos\n",
        "    df = pd.read_csv(\"/content/Piloto_Day_Trade/data/dados_brutos2.csv\", index_col=0, parse_dates=True, dayfirst=True)\n",
        "    # Aplicar limpeza nos dados\n",
        "    df_limpo = limpeza_basica_dados(df)\n",
        "    # Salva os dados limpos em CSV\n",
        "    df.to_csv(f\"/content/Piloto_Day_Trade/data/dados_limpos.csv\", index=False)\n",
        "    print(f\"Dados limpos salvos em dados_limpos.csv\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIHkNWR1-QS4",
        "outputId": "a65c0283-a91c-4ec3-b0b9-17cd7f1191d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados originais:\n",
            "                                        Close                High  \\\n",
            "Price                                                               \n",
            "Ticker                               BBDC4.SA            BBDC4.SA   \n",
            "Datetime                                  NaN                 NaN   \n",
            "2025-02-12 10:05:00-03:00  11.970000267028809  12.050000190734863   \n",
            "2025-02-12 10:10:00-03:00  11.899999618530273  11.979999542236328   \n",
            "2025-02-12 10:15:00-03:00  11.899999618530273   11.90999984741211   \n",
            "\n",
            "                                          Low                Open    Volume  \n",
            "Price                                                                        \n",
            "Ticker                               BBDC4.SA            BBDC4.SA  BBDC4.SA  \n",
            "Datetime                                  NaN                 NaN       NaN  \n",
            "2025-02-12 10:05:00-03:00  11.920000076293945  12.050000190734863    272800  \n",
            "2025-02-12 10:10:00-03:00  11.869999885559082  11.979999542236328   2343400  \n",
            "2025-02-12 10:15:00-03:00  11.859999656677246  11.899999618530273   1737500  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2557 entries, Ticker to 2025-03-28 16:50:00-03:00\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Close   2556 non-null   object\n",
            " 1   High    2556 non-null   object\n",
            " 2   Low     2556 non-null   object\n",
            " 3   Open    2556 non-null   object\n",
            " 4   Volume  2556 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 119.9+ KB\n",
            "None\n",
            "Ap√≥s remo√ß√£o das duas primeiras linhas:\n",
            "                                        Close                High  \\\n",
            "Price                                                               \n",
            "2025-02-12 10:05:00-03:00  11.970000267028809  12.050000190734863   \n",
            "2025-02-12 10:10:00-03:00  11.899999618530273  11.979999542236328   \n",
            "2025-02-12 10:15:00-03:00  11.899999618530273   11.90999984741211   \n",
            "2025-02-12 10:20:00-03:00  11.890000343322754    11.9399995803833   \n",
            "2025-02-12 10:25:00-03:00  11.850000381469727   11.90999984741211   \n",
            "\n",
            "                                          Low                Open   Volume  \n",
            "Price                                                                       \n",
            "2025-02-12 10:05:00-03:00  11.920000076293945  12.050000190734863   272800  \n",
            "2025-02-12 10:10:00-03:00  11.869999885559082  11.979999542236328  2343400  \n",
            "2025-02-12 10:15:00-03:00  11.859999656677246  11.899999618530273  1737500  \n",
            "2025-02-12 10:20:00-03:00  11.880000114440918   11.90999984741211   932000  \n",
            "2025-02-12 10:25:00-03:00  11.850000381469727   11.90999984741211  1095900  \n",
            "Ap√≥s convers√£o de √≠ndice:\n",
            "                 data               Close                High  \\\n",
            "0 2025-02-12 10:05:00  11.970000267028809  12.050000190734863   \n",
            "1 2025-02-12 10:10:00  11.899999618530273  11.979999542236328   \n",
            "2 2025-02-12 10:15:00  11.899999618530273   11.90999984741211   \n",
            "3 2025-02-12 10:20:00  11.890000343322754    11.9399995803833   \n",
            "4 2025-02-12 10:25:00  11.850000381469727   11.90999984741211   \n",
            "\n",
            "                  Low                Open   Volume      hora  \n",
            "0  11.920000076293945  12.050000190734863   272800  10:05:00  \n",
            "1  11.869999885559082  11.979999542236328  2343400  10:10:00  \n",
            "2  11.859999656677246  11.899999618530273  1737500  10:15:00  \n",
            "3  11.880000114440918   11.90999984741211   932000  10:20:00  \n",
            "4  11.850000381469727   11.90999984741211  1095900  10:25:00  \n",
            "Ap√≥s reorganizar as colunas:\n",
            "         data      hora  abertura  minimo  maximo  fechamento   volume\n",
            "0  2025-02-12  10:05:00     12.05   11.92   12.05       11.97   272800\n",
            "1  2025-02-12  10:10:00     11.98   11.87   11.98       11.90  2343400\n",
            "2  2025-02-12  10:15:00     11.90   11.86   11.91       11.90  1737500\n",
            "3  2025-02-12  10:20:00     11.91   11.88   11.94       11.89   932000\n",
            "4  2025-02-12  10:25:00     11.91   11.85   11.91       11.85  1095900\n",
            "Ap√≥s remover duplicatas e nulos:\n",
            "         data      hora  abertura  minimo  maximo  fechamento   volume\n",
            "0  2025-02-12  10:05:00     12.05   11.92   12.05       11.97   272800\n",
            "1  2025-02-12  10:10:00     11.98   11.87   11.98       11.90  2343400\n",
            "2  2025-02-12  10:15:00     11.90   11.86   11.91       11.90  1737500\n",
            "3  2025-02-12  10:20:00     11.91   11.88   11.94       11.89   932000\n",
            "4  2025-02-12  10:25:00     11.91   11.85   11.91       11.85  1095900\n",
            "Ap√≥s filtrar apenas os dias √∫teis:\n",
            "        data      hora  abertura  minimo  maximo  fechamento   volume\n",
            "0 2025-02-12  10:05:00     12.05   11.92   12.05       11.97   272800\n",
            "1 2025-02-12  10:10:00     11.98   11.87   11.98       11.90  2343400\n",
            "2 2025-02-12  10:15:00     11.90   11.86   11.91       11.90  1737500\n",
            "3 2025-02-12  10:20:00     11.91   11.88   11.94       11.89   932000\n",
            "4 2025-02-12  10:25:00     11.91   11.85   11.91       11.85  1095900\n",
            "Ap√≥s filtrar o intervalo de hor√°rio (09:55-18:05):\n",
            "        data      hora  abertura  minimo  maximo  fechamento   volume\n",
            "0 2025-02-12  10:05:00     12.05   11.92   12.05       11.97   272800\n",
            "1 2025-02-12  10:10:00     11.98   11.87   11.98       11.90  2343400\n",
            "2 2025-02-12  10:15:00     11.90   11.86   11.91       11.90  1737500\n",
            "3 2025-02-12  10:20:00     11.91   11.88   11.94       11.89   932000\n",
            "4 2025-02-12  10:25:00     11.91   11.85   11.91       11.85  1095900\n",
            "Limpeza de dados conclu√≠da com sucesso.\n",
            "Dados limpos e ordenados\n",
            ":\n",
            "           data      hora  abertura  minimo  maximo  fechamento  volume\n",
            "2472 2025-03-28  10:00:00     13.00   12.98   13.01       12.99       0\n",
            "2473 2025-03-28  10:05:00     12.98   12.96   13.00       13.00   82400\n",
            "2474 2025-03-28  10:10:00     13.00   13.00   13.03       13.00  119400\n",
            "2475 2025-03-28  10:15:00     13.00   12.96   13.00       12.98  160200\n",
            "2476 2025-03-28  10:20:00     12.97   12.96   12.98       12.97   58000\n",
            "2477 2025-03-28  10:25:00     12.98   12.98   13.02       13.01   88600\n",
            "2478 2025-03-28  10:30:00     13.01   12.99   13.04       13.00  156100\n",
            "2479 2025-03-28  10:35:00     13.00   12.99   13.03       13.01  155200\n",
            "2480 2025-03-28  10:40:00     13.00   12.97   13.01       12.97  127500\n",
            "2481 2025-03-28  10:45:00     12.97   12.97   13.01       12.98  151700\n",
            "Dados limpos salvos em dados_limpos.csv\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-9872576797ff>:117: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(\"/content/Piloto_Day_Trade/data/dados_brutos2.csv\", index_col=0, parse_dates=True, dayfirst=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforma√ß√£o de dados"
      ],
      "metadata": {
        "id": "aPwYXx-8x7g9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Fun√ß√£o de Transforma√ß√£o de Dados para Modelagem Preditiva\n",
        "\n",
        "Este script processa e transforma os dados de movimenta√ß√£o de ativos para an√°lise e previs√£o, gerando um conjunto\n",
        "de caracter√≠sticas (features) que podem ser utilizadas no treinamento de modelos como XGBoost, LSTM e Transformer.\n",
        "\n",
        "Objetivos:\n",
        "- Criar um dataset amplo e rico em vari√°veis relevantes para an√°lise preditiva.\n",
        "- Incluir indicadores t√©cnicos, estat√≠sticas de volatilidade, m√©dias m√≥veis e outras features derivadas.\n",
        "- Permitir a experimenta√ß√£o com diferentes combina√ß√µes de features ao longo dos testes e otimiza√ß√µes.\n",
        "- Evitar a inclus√£o simult√¢nea de vari√°veis normalizadas e seus equivalentes reais, para n√£o confundir o modelo.\n",
        "\n",
        "Estrat√©gia:\n",
        "- O dataset conter√° diversas colunas transformadas e calculadas para maximizar o potencial do modelo.\n",
        "- Durante os testes de parametriza√ß√£o e treinamento, ser√£o geradas diferentes vers√µes do dataset, refinando a\n",
        "  sele√ß√£o de features conforme necess√°rio.\n",
        "- A abordagem visa garantir que o modelo receba apenas informa√ß√µes √∫teis, reduzindo redund√¢ncias e colinearidades.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "1eO0DX7V2rQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/transformacao_dados_v2.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Carregar vari√°veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "def carregar_dados(caminho):\n",
        "    \"\"\"Carrega um CSV e retorna um DataFrame ou None se n√£o existir.\"\"\"\n",
        "    try:\n",
        "        if os.path.exists(caminho):\n",
        "            df = pd.read_csv(caminho, parse_dates=[\"data\"])\n",
        "            return df if not df.empty else pd.DataFrame()\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Arquivo de dados n√£o encontrado.\")\n",
        "            return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao carregar {caminho}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def obter_ultima_data(df):\n",
        "    \"\"\"Retorna a √∫ltima data dispon√≠vel nos dados.\"\"\"\n",
        "    return df[\"data\"].max() if not df.empty and \"data\" in df.columns else None\n",
        "\n",
        "def filtrar_novos_dados(df, ultima_data):\n",
        "    \"\"\"Filtra os dados para incluir apenas os novos registros.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"‚ö†Ô∏è Nenhum dado limpo dispon√≠vel.\")\n",
        "        return pd.DataFrame()\n",
        "    return df[df[\"data\"] > ultima_data] if ultima_data else df\n",
        "\n",
        "def calcular_lags(df, colunas, lags=3):\n",
        "    \"\"\"Gera vari√°veis defasadas (lags) para as colunas especificadas.\"\"\"\n",
        "    for coluna in colunas:\n",
        "        for lag in range(1, lags + 1):\n",
        "            df[f\"{coluna}_lag{lag}\"] = df[coluna].shift(lag)\n",
        "    return df\n",
        "\n",
        "def calcular_indicadores(df):\n",
        "    \"\"\"Calcula indicadores t√©cnicos para an√°lise financeira.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"‚ö†Ô∏è Nenhum dado para calcular indicadores.\")\n",
        "        return df\n",
        "\n",
        "    df = df.sort_values(\"data\", ascending=False)\n",
        "    df['retorno'] = df['fechamento'].pct_change().fillna(0)\n",
        "    df['volatilidade'] = df['retorno'].rolling(20).std().fillna(0)\n",
        "\n",
        "    df['SMA_10'] = df['fechamento'].rolling(10).mean().fillna(0)\n",
        "    df['EMA_10'] = df['fechamento'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    ganho = df['retorno'].clip(lower=0)\n",
        "    perda = -df['retorno'].clip(upper=0)\n",
        "    df['rsi'] = 100 - (100 / (1 + (ganho.ewm(span=14).mean() / (perda.ewm(span=14).mean() + 1e-10))))\n",
        "\n",
        "    df['SMA_20'] = df['fechamento'].rolling(20).mean()\n",
        "    df['std_dev'] = df['fechamento'].rolling(20).std()\n",
        "    df['upper_band'] = df['SMA_20'] + 2 * df['std_dev']\n",
        "    df['lower_band'] = df['SMA_20'] - 2 * df['std_dev']\n",
        "\n",
        "    df['MACD'] = df['fechamento'].ewm(span=12).mean() - df['fechamento'].ewm(span=26).mean()\n",
        "    df['Signal_Line'] = df['MACD'].ewm(span=9).mean()\n",
        "\n",
        "    df['OBV'] = (df['volume'] * np.sign(df['fechamento'].diff())).fillna(0).cumsum()\n",
        "\n",
        "    # Criar vari√°veis de defasagem (lags)\n",
        "    df = calcular_lags(df, ['fechamento', 'retorno', 'volume'], lags=3)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df[['fechamento_normalizado', 'volume_normalizado']] = scaler.fit_transform(df[['fechamento', 'volume']])\n",
        "\n",
        "    std_scaler = StandardScaler()\n",
        "    df[['rsi_padronizado', 'macd_padronizado']] = std_scaler.fit_transform(df[['rsi', 'MACD']].fillna(0))\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def processar_transformacao(dados_limpos, dados_transformados):\n",
        "    \"\"\"Executa o processo de transforma√ß√£o dos dados.\"\"\"\n",
        "\n",
        "    df_transformado = carregar_dados(dados_transformados)\n",
        "    df_limpo = carregar_dados(dados_limpos)\n",
        "\n",
        "    if df_transformado is None or df_transformado.empty:\n",
        "        df_transformado = pd.DataFrame()\n",
        "\n",
        "    ultima_data = obter_ultima_data(df_transformado)\n",
        "    novos_dados = filtrar_novos_dados(df_limpo, ultima_data)\n",
        "\n",
        "    if not novos_dados.empty:\n",
        "        novos_dados = calcular_indicadores(novos_dados)\n",
        "        df_final = pd.concat([df_transformado, novos_dados], ignore_index=True) if not df_transformado.empty else novos_dados\n",
        "        return df_final\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è Nenhum novo dado para processar.\")\n",
        "        return df_transformado\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Referencia o caminho para os dados que ser√£o transformados\n",
        "    dados_limpos = '/content/Piloto_Day_Trade/data/dados_limpos.csv'\n",
        "    dados_transformados = '/content/Piloto_Day_Trade/data/dados_transformados.csv'\n",
        "\n",
        "    # Aplica a transforma√ß√£o\n",
        "    df_transformado = processar_transformacao(dados_limpos, dados_transformados)\n",
        "\n",
        "    # Salva os dados transformados em arquivo .csv, se houver novos dados\n",
        "    if not df_transformado.empty:\n",
        "        print(\"‚úÖ Os dados foram transformados com sucesso.\")\n",
        "        print(\"\\nAmostra dos dados transformados:\\n\", df_transformado.head())\n",
        "\n",
        "        df_transformado.to_csv(dados_transformados, index=False)\n",
        "        print(f\"‚úÖ Dados transformados salvos em {dados_transformados}.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Nenhum dado transformado para salvar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uillK5_03Pcv",
        "outputId": "be3ad6ea-811b-4daf-9f45-f02d3cf15d2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/Piloto_Day_Trade/scripts/transformacao_dados_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/Piloto_Day_Trade/scripts/transformacao_dados_v1.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Carregar vari√°veis de ambiente\n",
        "load_dotenv()\n",
        "\n",
        "def carregar_dados(caminho):\n",
        "    \"\"\"Carrega um CSV e retorna um DataFrame ou None se n√£o existir.\"\"\"\n",
        "    try:\n",
        "        if os.path.exists(caminho):\n",
        "            df = pd.read_csv(caminho, parse_dates=[\"data\"])\n",
        "            return df if not df.empty else pd.DataFrame()\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Arquivo de dados n√£o encontrado.\")\n",
        "            return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao carregar {caminho}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def obter_ultima_data(df):\n",
        "    \"\"\"Retorna a √∫ltima data dispon√≠vel nos dados.\"\"\"\n",
        "    return df[\"data\"].max() if not df.empty and \"data\" in df.columns else None\n",
        "\n",
        "def filtrar_novos_dados(df, ultima_data):\n",
        "    \"\"\"Filtra os dados para incluir apenas os novos registros.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"‚ö†Ô∏è Nenhum dado limpo dispon√≠vel.\")\n",
        "        return pd.DataFrame()\n",
        "    return df[df[\"data\"] > ultima_data] if ultima_data else df\n",
        "\n",
        "def calcular_indicadores(df):\n",
        "    \"\"\"Calcula indicadores t√©cnicos para an√°lise financeira.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"‚ö†Ô∏è Nenhum dado para calcular indicadores.\")\n",
        "        return df\n",
        "\n",
        "    df = df.sort_values(\"data\", ascending=False)\n",
        "    df['retorno'] = df['fechamento'].pct_change().fillna(0)\n",
        "    df['volatilidade'] = df['retorno'].rolling(20).std().fillna(0)\n",
        "\n",
        "    df['SMA_10'] = df['fechamento'].rolling(10).mean().fillna(0)\n",
        "    df['EMA_10'] = df['fechamento'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    ganho = df['retorno'].clip(lower=0)\n",
        "    perda = -df['retorno'].clip(upper=0)\n",
        "    df['rsi'] = 100 - (100 / (1 + (ganho.ewm(span=14).mean() / (perda.ewm(span=14).mean() + 1e-10))))\n",
        "\n",
        "    df['SMA_20'] = df['fechamento'].rolling(20).mean()\n",
        "    df['std_dev'] = df['fechamento'].rolling(20).std()\n",
        "    df['upper_band'] = df['SMA_20'] + 2 * df['std_dev']\n",
        "    df['lower_band'] = df['SMA_20'] - 2 * df['std_dev']\n",
        "\n",
        "    df['MACD'] = df['fechamento'].ewm(span=12).mean() - df['fechamento'].ewm(span=26).mean()\n",
        "    df['Signal_Line'] = df['MACD'].ewm(span=9).mean()\n",
        "\n",
        "    df['OBV'] = (df['volume'] * np.sign(df['fechamento'].diff())).fillna(0).cumsum()\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df[['fechamento_normalizado', 'volume_normalizado']] = scaler.fit_transform(df[['fechamento', 'volume']])\n",
        "\n",
        "    std_scaler = StandardScaler()\n",
        "    df[['rsi_padronizado', 'macd_padronizado']] = std_scaler.fit_transform(df[['rsi', 'MACD']].fillna(0))\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df  # Agora retorna corretamente os dados transformados\n",
        "\n",
        "def processar_transformacao(dados_limpos, dados_transformados):\n",
        "    \"\"\"Executa o processo de transforma√ß√£o dos dados.\"\"\"\n",
        "\n",
        "    df_transformado = carregar_dados(dados_transformados)\n",
        "    df_limpo = carregar_dados(dados_limpos)\n",
        "\n",
        "    if df_transformado is None or df_transformado.empty:\n",
        "        df_transformado = pd.DataFrame()  # Garante que n√£o √© None\n",
        "\n",
        "    ultima_data = obter_ultima_data(df_transformado)\n",
        "    novos_dados = filtrar_novos_dados(df_limpo, ultima_data)\n",
        "\n",
        "    if not novos_dados.empty:\n",
        "        novos_dados = calcular_indicadores(novos_dados)\n",
        "\n",
        "        # Se df_transformado estiver vazio, apenas mant√©m novos_dados\n",
        "        df_final = pd.concat([df_transformado, novos_dados], ignore_index=True) if not df_transformado.empty else novos_dados\n",
        "\n",
        "        return df_final\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è Nenhum novo dado para processar.\")\n",
        "        return df_transformado  # Retorna o DataFrame existente caso n√£o haja novos dados\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Referencia o caminho para os dados que ser√£o transformados\n",
        "    dados_limpos = '/content/Piloto_Day_Trade/data/dados_limpos.csv'\n",
        "    dados_transformados = '/content/Piloto_Day_Trade/data/dados_transformados.csv'\n",
        "\n",
        "    # Aplica a transforma√ß√£o\n",
        "    df_transformado = processar_transformacao(dados_limpos, dados_transformados)\n",
        "\n",
        "    # Salva os dados transformados em arquivo .csv, se houver novos dados\n",
        "    if not df_transformado.empty:\n",
        "        print(\"‚úÖ Dados transformados com sucesso.\")\n",
        "        print(\"\\nAmostra dos dados transformados:\\n\", df_transformado.head())\n",
        "\n",
        "        df_transformado.to_csv(dados_transformados, index=False)\n",
        "        print(f\"‚úÖ Dados transformados salvos em {dados_transformados}.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Nenhum dado transformado para salvar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4IdEaygx7Hc",
        "outputId": "73626e3e-8d2f-4290-e26d-5b627134185f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Piloto_Day_Trade/scripts/transformacao_dados_v1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando fun√ß√£o para atualizar o repositorio remoto\n",
        "\n",
        "def atualizar_repositorio(commit_message):\n",
        "    \"\"\"Atualiza o reposit√≥rio remoto no GitHub.\"\"\"\n",
        "    !git add .\n",
        "    !git commit -m commit_message\n",
        "    !git push origin main\n",
        "    print(\"‚úÖ Atualiza√ß√£o do reposit√≥rio conclu√≠da!\")\n",
        "\n",
        "atualizar_repositorio('Atualizando Scripts de extra√ß√£o, limpeza e transforma√ß√£o')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGqg5UeqMKbl",
        "outputId": "052a90af-d924-4118-b1f0-cf5a272c6a62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 451478d] commit_message\n",
            " 5 files changed, 5242 insertions(+), 1857 deletions(-)\n",
            " create mode 100644 data/dados_brutos2.csv\n",
            " rewrite data/dados_limpos.csv (97%)\n",
            " create mode 100644 scripts/transformacao_dados_v2.py\n",
            "Enumerating objects: 15, done.\n",
            "Counting objects: 100% (15/15), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 52.51 KiB | 2.50 MiB/s, done.\n",
            "Total 9 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
            "To https://github.com/CarolBw/Piloto_Day_Trade.git\n",
            "   8a5cb90..451478d  main -> main\n",
            "‚úÖ Atualiza√ß√£o do reposit√≥rio conclu√≠da!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de dados"
      ],
      "metadata": {
        "id": "LQ3IJIpH521c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "injpAahyCCxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelagem de dados"
      ],
      "metadata": {
        "id": "ai-nFsCPCE4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Carregar dados\n",
        "caminho_dados = '/content/Piloto_Day_Trade/data/dados_transformados.csv'\n",
        "df = pd.read_csv(caminho_dados, parse_dates=['data'])\n",
        "\n",
        "# Filtrar os dados para considerar apenas os √∫ltimos 20 dias √∫teis\n",
        "df = df.sort_values(by=['data', 'hora'], ascending=True)\n",
        "df = df[df['data'] >= df['data'].max() - pd.Timedelta(days=20)]  # Considera os √∫ltimos 20 dias √∫teis\n",
        "\n",
        "# Criar features e targets\n",
        "df.loc[:, 'target_min'] = df['minimo'].shift(-1)  # Previs√£o da m√≠nima do pr√≥ximo dia\n",
        "df.loc[:, 'target_max'] = df['maximo'].shift(-1)  # Previs√£o da m√°xima do pr√≥ximo dia\n",
        "df.loc[:, 'target_fechamento'] = df['fechamento'].shift(-1)  # Previs√£o do fechamento do pr√≥ximo dia\n",
        "\n",
        "df.dropna(inplace=True)  # Remover valores nulos\n",
        "\n",
        "# Definindo as features\n",
        "features = ['abertura', 'minimo', 'maximo', 'fechamento',\n",
        "            'volume']\n",
        "X = df[features]\n",
        "\n",
        "# Fun√ß√£o para treinar, avaliar e mostrar a compara√ß√£o de previs√µes\n",
        "def treinar_e_avaliar(target_name):\n",
        "    y = df[target_name]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Criar modelo XGBoost com ajustes de par√¢metros\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        n_estimators=200,  # N√∫mero de √°rvores\n",
        "        learning_rate=0.05,  # Taxa de aprendizado\n",
        "        max_depth=5,         # Profundidade das √°rvores\n",
        "        min_child_weight=1,  # Peso m√≠nimo das folhas\n",
        "        subsample=0.8,       # Subamostragem\n",
        "        colsample_bytree=0.8,  # Subamostragem das colunas\n",
        "        early_stopping_rounds=50  # Early stopping\n",
        "    )\n",
        "\n",
        "    # Treinar o modelo\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_test, y_test)],\n",
        "              verbose=False)  # Usar early stopping\n",
        "\n",
        "    # Realizar previs√µes\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Corrigir a associa√ß√£o de data e hora com as previs√µes\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Data': df.loc[X_test.index, 'data'],\n",
        "        'Hora': df.loc[X_test.index, 'hora'],  # Incluir hora\n",
        "        'Real': y_test,\n",
        "        'Previsao': y_pred\n",
        "    })\n",
        "\n",
        "    print(f\"\\nAmostra de Compara√ß√£o para {target_name}:\")\n",
        "    print(comparison_df.head(10))  # Exibir as primeiras 10 linhas para visualiza√ß√£o\n",
        "\n",
        "    # Avalia√ß√£o\n",
        "    print(f\"\\nAvalia√ß√£o para {target_name}:\")\n",
        "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
        "    print(f\"R¬≤: {r2_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "    # Salvar o modelo treinado\n",
        "    model.save_model(f'/content/Piloto_Day_Trade/models/xgboost_model_{target_name}.json')\n",
        "    print(f\"‚úÖ Modelo XGBoost para {target_name} salvo com sucesso!\")\n",
        "\n",
        "# Treinar e avaliar para cada target\n",
        "treinar_e_avaliar('target_min')\n",
        "treinar_e_avaliar('target_max')\n",
        "treinar_e_avaliar('target_fechamento')\n"
      ],
      "metadata": {
        "id": "eud4XU96L7Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Carregar dados\n",
        "caminho_dados = '/content/Piloto_Day_Trade/data/dados_transformados.csv'\n",
        "df = pd.read_csv(caminho_dados, parse_dates=['data'])\n",
        "\n",
        "# Filtrar os dados para considerar os √∫ltimos 20 dias √∫teis\n",
        "df = df.sort_values(by=['data', 'hora'], ascending=True)\n",
        "df = df[df['data'] >= df['data'].max() - pd.Timedelta(days=20)]  # Considera os √∫ltimos 20 dias √∫teis\n",
        "\n",
        "# Criar features e targets\n",
        "df['target_min'] = df['minimo'].shift(-1)  # Previs√£o da m√≠nima do pr√≥ximo dia\n",
        "df['target_max'] = df['maximo'].shift(-1)  # Previs√£o da m√°xima do pr√≥ximo dia\n",
        "df['target_fechamento'] = df['fechamento'].shift(-1)  # Previs√£o do fechamento do pr√≥ximo dia\n",
        "\n",
        "df.dropna(inplace=True)  # Remover valores nulos\n",
        "\n",
        "# Sele√ß√£o de features\n",
        "features = ['abertura', 'minimo', 'maximo', 'fechamento',\n",
        "            'volume', 'SMA_10', 'EMA_10', 'rsi', 'MACD', 'Signal_Line', 'volatilidade']\n",
        "X = df[features]\n",
        "\n",
        "# Fun√ß√£o para treinar, avaliar e mostrar a compara√ß√£o de previs√µes\n",
        "def treinar_e_avaliar(target_name):\n",
        "    y = df[target_name]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Criar modelo XGBoost\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        n_estimators=200,  # N√∫mero reduzido de √°rvores\n",
        "        learning_rate=0.05,  # Taxa de aprendizado mais baixa\n",
        "        max_depth=5,         # Profundidade menor para evitar overfitting\n",
        "        min_child_weight=1,  # Ajuste no peso m√≠nimo das folhas\n",
        "        subsample=0.8,       # Subamostragem\n",
        "        colsample_bytree=0.8,  # Subamostragem das colunas\n",
        "        early_stopping_rounds=50  # Early stopping\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_test, y_test)],\n",
        "              verbose=False)  # Usar early stopping\n",
        "\n",
        "    # Realizar previs√µes\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Corrigir a associa√ß√£o de data com as previs√µes\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Data': df.loc[X_test.index, 'data'],  # Usando as datas corretas\n",
        "        'Real': y_test,\n",
        "        'Previsao': y_pred\n",
        "    })\n",
        "\n",
        "    print(f\"\\nAmostra de Compara√ß√£o para {target_name}:\")\n",
        "    print(comparison_df.head(10))  # Exibir as primeiras 10 linhas para visualiza√ß√£o\n",
        "\n",
        "    # Avalia√ß√£o\n",
        "    print(f\"\\nAvalia√ß√£o para {target_name}:\")\n",
        "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
        "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
        "    print(f\"R¬≤: {r2_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "    # Salvar o modelo treinado\n",
        "    model.save_model(f'/content/Piloto_Day_Trade/models/xgboost_model_{target_name}.json')\n",
        "    print(f\"‚úÖ Modelo XGBoost para {target_name} salvo com sucesso!\")\n",
        "\n",
        "# Treinar e avaliar para cada target\n",
        "treinar_e_avaliar('target_min')\n",
        "treinar_e_avaliar('target_max')\n",
        "treinar_e_avaliar('target_fechamento')\n"
      ],
      "metadata": {
        "id": "s3W6gicdLAqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}